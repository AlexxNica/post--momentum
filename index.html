<!doctype html>
<meta charset="utf-8">
<script src="assets/lib/template.js"></script>
<script type="text/front-matter">
  title: Why Momentum Works.
  description: And why gradient descent fails.
  authors:
    - Gabriel Goh: http://gabgoh.github.io
  affiliations:
    - UC Davis: http://math.ucdavis.edu
</script>

<script src="assets/lib/auto-render.min.js"></script>
<script src="assets/lib/katex.min.js"></script>
<link   rel="stylesheet" href="assets/lib/katex.min.css">
<link   rel="stylesheet" type="text/css" href="assets/widgets.css">
<script src="assets/lib/d3.v4.min.js"></script>
<script src="assets/lib/numeric-1.2.6.min.js"></script>
<script src="assets/utils.js"></script>
<script src="assets/flow.js"></script>
<script src="assets/momentum.js"></script>
<script src="assets/poly.js"></script>
<script src="assets/data/Sigma.json"></script>
<script src="assets/data/matrix.json"></script>
<script src="assets/data/Uval.json"></script>
<script src="assets/flow.js"></script>
<script src="assets/milestones.js"></script>
<script src="assets/lib/contour_plot.js"></script>
<script src="assets/lib/tooltip.js"></script>
<script src="assets/iterates.js"></script>
<script src="assets/data/Urosen.json"></script>
<script src="assets/phasediagram.js"></script>
<script src="assets/eigensum.js"></script>
<script src="assets/graph.js"></script>
<script src="assets/lib/swoopy-drag.js"></script>


<link rel="stylesheet" type="text/css" href="assets/widgets.css">

<svg style="display: none;">
  <g id="pointerThingy">
    <circle fill="none" stroke="#FF6C00" stroke-linecap="round" cx="0" cy="0" r="14"/>
    <circle fill="#FF6C00" cx="0" cy="0" r="11"/>
    <path id="XMLID_173_" fill="#FFFFFF" d="M-3.2-1.3c0-0.1,0-0.2,0-0.3c0-0.1,0-0.2,0-0.3c-0.6,0-1.2,0-1.8,0c0,0.6,0,1.2,0,1.8
      c0.2,0,0.4,0,0.6,0c0-0.4,0-0.8,0-1.2c0,0,0.1,0,0.1,0c0.3,0,0.5,0,0.8,0C-3.4-1.3-3.3-1.3-3.2-1.3c0,0.2,0,0.4,0,0.6
      c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0,0,0,0-0.1c0-1.6,0-3.2,0-4.8c0-0.6,0-1.2,0-1.8c0,0,0,0,0.1,0
      c0.3,0,0.7,0,1,0c0.1,0,0.1,0,0.2,0c0-0.2,0-0.4,0-0.6c-0.4,0-0.8,0-1.2,0C-2-7.2-2-7-2-6.8c0,0,0,0-0.1,0c-0.2,0-0.3,0-0.5,0
      c0,0,0,0-0.1,0c0,1.8,0,3.6,0,5.5c-0.2,0-0.3,0-0.4,0C-3.1-1.3-3.2-1.3-3.2-1.3z M1.1-3.7C1-3.8,1-3.8,1.1-3.7C1-4,1-4.1,1-4.3
      c0,0,0,0,0-0.1c-0.4,0-0.8,0-1.2,0c0-0.8,0-1.6,0-2.4c-0.2,0-0.4,0-0.6,0c0,1.8,0,3.6,0,5.5c0.2,0,0.4,0,0.6,0c0-0.8,0-1.6,0-2.4
      c0,0,0.1,0,0.1,0C0.3-3.7,0.6-3.7,1.1-3.7C1-3.7,1-3.7,1.1-3.7C1.1-3.7,1-3.7,1.1-3.7c0,0.8,0,1.6,0,2.3c0,0,0,0.1,0,0.1
      c0.2,0,0.4,0,0.6,0c0-0.6,0-1.2,0-1.8c0.4,0,0.8,0,1.2,0c0,0.8,0,1.6,0,2.4c0.2,0,0.4,0,0.6,0c0-0.6,0-1.2,0-1.8c0.2,0,0.4,0,0.6,0
      c0,0,0,0,0,0.1c0,0.1,0,0.3,0,0.4c0,0,0,0.1,0,0.1c0.2,0,0.4,0,0.5,0c0,0,0.1,0,0.1,0.1c0,0.2,0,0.5,0,0.7c0,1.1,0,2.3,0,3.4
      c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0c0,0,0,0,0,0c0,0.6,0,1.1,0,1.7c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0c0,0.4,0,0.8,0,1.2
      c-1.6,0-3.2,0-4.9,0c0-0.4,0-0.8,0-1.2c-0.2,0-0.4,0-0.6,0C-2,3.8-2,3.4-2,3c-0.2,0-0.4,0-0.6,0c0,0.4,0,0.8,0,1.2
      c0.2,0,0.4,0,0.6,0C-2,4.8-2,5.4-2,6c2,0,4.1,0,6.1,0c0-0.1,0-0.2,0-0.3c0-0.5,0-0.9,0-1.4c0-0.1,0-0.1,0-0.2c0.2,0,0.4,0,0.5,0
      c0.1,0,0.1,0,0.1-0.1c0-0.4,0-0.9,0-1.3c0-0.1,0-0.3,0-0.4c0.1,0,0.2,0,0.3,0c0.1,0,0.2,0,0.3,0c0-1.4,0-2.8,0-4.3
      c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c-0.4,0-0.8,0-1.2,0c0-0.2,0-0.4,0-0.6
      c-0.1,0-0.2,0-0.3,0c-0.4,0-0.9,0-1.3,0C1.2-3.7,1.1-3.7,1.1-3.7z M-3.2,1.8c0,0.4,0,0.8,0,1.2c0.2,0,0.4,0,0.5,0
      c0.1,0,0.1,0,0.1-0.1c0-0.3,0-0.6,0-1c0-0.1,0-0.1,0-0.2C-2.8,1.8-3,1.8-3.2,1.8c0-0.4,0-0.8,0-1.2c-0.2,0-0.4,0-0.6,0
      c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0,0,0,0,0.1c0,0.1,0,0.3,0,0.4c0,0.2,0,0.5,0,0.7
      c0,0,0,0.1,0.1,0.1c0.1,0,0.2,0,0.3,0C-3.4,1.8-3.3,1.8-3.2,1.8z"/>
    <path id="XMLID_172_" fill="#FFFFFF" d="M4.1,4.2C4.1,4.2,4.1,4.2,4.1,4.2c0-0.6,0-1.2,0-1.8c0,0,0,0,0,0c0.2,0,0.4,0,0.6,0
      c0,0,0-0.1,0-0.1c0-1.1,0-2.3,0-3.4c0-0.2,0-0.5,0-0.7c0,0,0-0.1-0.1-0.1c-0.2,0-0.4,0-0.5,0c0,0,0-0.1,0-0.1c0-0.1,0-0.3,0-0.4
      c0,0,0-0.1,0-0.1c-0.2,0-0.4,0-0.6,0c0,0.6,0,1.2,0,1.8c-0.2,0-0.4,0-0.6,0c0-0.8,0-1.6,0-2.4c-0.4,0-0.8,0-1.2,0
      c0,0.6,0,1.2,0,1.8c-0.2,0-0.4,0-0.6,0c0,0,0-0.1,0-0.1c0-0.7,0-1.5,0-2.2c0,0,0-0.1,0-0.1l0,0c0.1,0,0.2,0,0.2,0
      c0.4,0,0.9,0,1.3,0c0.1,0,0.2,0,0.3,0c0,0.2,0,0.4,0,0.6c0.4,0,0.8,0,1.2,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6
      c0.2,0,0.4,0,0.6,0c0,1.4,0,2.8,0,4.3c-0.1,0-0.2,0-0.3,0c-0.1,0-0.2,0-0.3,0c0,0.1,0,0.3,0,0.4c0,0.4,0,0.9,0,1.3
      c0,0.1,0,0.1-0.1,0.1C4.5,4.2,4.3,4.2,4.1,4.2L4.1,4.2z"/>
    <path id="XMLID_171_" fill="#FFFFFF" d="M4.1,4.2c0,0.1,0,0.1,0,0.2c0,0.5,0,0.9,0,1.4c0,0.1,0,0.2,0,0.3C2.1,6,0,6-2,6
      c0-0.6,0-1.2,0-1.8c-0.2,0-0.4,0-0.6,0c0-0.4,0-0.8,0-1.2C-2.4,3-2.2,3-2,3c0,0.4,0,0.8,0,1.2c0.2,0,0.4,0,0.6,0c0,0.4,0,0.8,0,1.2
      c1.6,0,3.2,0,4.9,0c0-0.4,0-0.8,0-1.2C3.7,4.2,3.9,4.2,4.1,4.2L4.1,4.2z"/>
    <path id="XMLID_170_" fill="#FFFFFF" d="M-2-6.8c0,0.6,0,1.2,0,1.8c0,1.6,0,3.2,0,4.8c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0
      c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6l0,0c0.1,0,0.1,0,0.2,0c0.1,0,0.3,0,0.4,0c0-1.8,0-3.6,0-5.5
      c0,0,0.1,0,0.1,0C-2.4-6.8-2.2-6.8-2-6.8C-2.1-6.8-2-6.8-2-6.8L-2-6.8z"/>
    <path id="XMLID_169_" fill="#FFFFFF" d="M1.1-3.7C1-3.7,1-3.7,1.1-3.7c-0.4,0-0.8,0-1.2,0c0,0,0,0-0.1,0c0,0.8,0,1.6,0,2.4
      c-0.2,0-0.4,0-0.6,0c0-1.8,0-3.6,0-5.5c0.2,0,0.4,0,0.6,0c0,0.8,0,1.6,0,2.4c0.4,0,0.8,0,1.2,0c0,0,0,0.1,0,0.1C1-4.1,1-4,1.1-3.7
      C1-3.8,1-3.8,1.1-3.7L1.1-3.7z"/>
    <path id="XMLID_168_" fill="#FFFFFF" d="M-3.2,1.8c-0.1,0-0.2,0-0.3,0c-0.1,0-0.2,0-0.3,0c0,0-0.1,0-0.1-0.1c0-0.2,0-0.5,0-0.7
      c0-0.1,0-0.3,0-0.4c0,0,0,0,0-0.1c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0
      C-3.2,0.9-3.2,1.3-3.2,1.8c0.2,0,0.4,0,0.6,0c0,0.1,0,0.1,0,0.2c0,0.3,0,0.6,0,1C-2.6,3-2.7,3-2.7,3c-0.2,0-0.3,0-0.5,0
      C-3.2,2.6-3.2,2.2-3.2,1.8z"/>
    <path id="XMLID_167_" fill="#FFFFFF" d="M-3.2-1.3c-0.1,0-0.2,0-0.3,0c-0.3,0-0.5,0-0.8,0c0,0,0,0-0.1,0c0,0.4,0,0.8,0,1.2
      c-0.2,0-0.4,0-0.6,0c0-0.6,0-1.2,0-1.8c0.6,0,1.2,0,1.8,0c0,0.1,0,0.2,0,0.3C-3.2-1.5-3.2-1.4-3.2-1.3L-3.2-1.3z"/>
    <path id="XMLID_166_" fill="#FFFFFF" d="M-2-6.8C-2-7-2-7.2-2-7.4c0.4,0,0.8,0,1.2,0c0,0.2,0,0.4,0,0.6c-0.1,0-0.1,0-0.2,0
      C-1.3-6.8-1.6-6.8-2-6.8C-2-6.8-2-6.8-2-6.8L-2-6.8z"/>
  </g>
</svg>

<dt-article class="centered">
  <h1>Why Momentum Works [DRAFT] </h1>
  <h2>And why gradient descent fails.</h2>

  <figure style = "position:relative; width:920px; height:400px;">
    <div id="banana" style="position:relative; border: 1px solid rgba(0, 0, 0, 0.2);"></div>
    <div id="sliderAlpha" style="position:absolute; width:300px; height: 50px; left:0px; top: 320px;">
      <text class="figtext" style="top: -5px; left: 20px; position: relative;">Step size α = 0.02</text>
    </div>
    <div id="sliderBeta" style="position:absolute; width: 300px; height: 50px; left: 240px; top: 320px;;">
      <text class="figtext" style="top: -5px; left: 20px; position: relative;">Momentum β = 0.99</text>
    </div>
    <figcaption id="Bananacaption" style="position:absolute; width: 360px; height: 50px; left: 520px; top: 320px;">
      Find something to put here. Find something to put here. Gradient Descent (β = 0), Momentum (β = 0.99). Find something to put here. Find something to put here.
    </figcaption>
  </figure>

  <dt-byline class="l-page"></dt-byline>

  <script>

  // Render Foreground
  var iterControl = genIterDiagram(bananaf, [1,1/3], [[-2,2],[2/3 + 0.4,-2/3 + 0.4]])
                    .alpha(0.003)
                    .beta(0)
                    (d3.select("#banana").style("position","relative"))

  var iterChange = iterControl.control
  var getw0 = iterControl.w0

  var StepRange = d3.scaleLinear().domain([0,100]).range([0,0.0062])
  var MomentumRange = d3.scaleLinear().domain([0,100]).range([0,0.98])

  var update = function (i,j) { iterChange(i, 0, getw0()) }

  var slidera = sliderGen([230, 40])
              .ticks([0,0.0015,0.003])
              .change( function (i) {
                d3.select("#sliderAlpha").selectAll(".figtext").html("Step size α = " + getalpha().toPrecision(2) )
                iterChange(getalpha(), getbeta(), getw0() )
              } )
              .startxval(0.003)
              .cRadius(7)
              .shifty(-12)
              .margins(20,20)

  var sliderb = sliderGen([230, 40])
              .ticks([0,0.5,0.99])
              .change( function (i) {
                d3.select("#sliderBeta").selectAll(".figtext").html("Momentum β = " + getbeta().toPrecision(2) )
                iterChange(getalpha(), getbeta(), getw0() )
              } )
              .cRadius(7)
              .shifty(-12)
              .startxval(0.74)
              .margins(20,20)

  var getalpha = slidera( d3.select("#sliderAlpha")).xval
  var getbeta  = sliderb( d3.select("#sliderBeta")).xval

  iterChange(getalpha(), getbeta(), getw0() )

  </script><!-- 
  <p>
  If you've spent enough some time optimizing smooth functions, you may have met this optimizers' old nemesis. The condition has many names, but it always borrows the language of pathology: Ill-conditioning, pathological curvature, dead gradients. You're performing gradient descent. Your choice of step-size, seems correct. The gradients don't blow up. There is no division by zero, no NaNs, no square rooting of minus one. In fact, things often begin quite well - with an impressive, almost immediate decrease in the loss. But as the iterations progress, you start to get a nagging feeling you're not making as much progress as you should be. You're iterating hard, but the loss isn't getting smaller. Should you keep iterating, and hope for the best?
  </p>

  <p>
  The problem could be pathological curvature. The landscapes are often described as a valleys, trenches, canals, ravines. In these steep valleys, gradient descent fumbles. All progress along certain directions grind close to a halt. The iterates only approaches the optimum in small, tedious steps, or ossilate between two valleys. These are all classical symptoms of pathological curvature.
  </p> -->
  <p>
  Gradient descent has many virtues, but speed is not one of them. When optimizing a smooth function $f$, gradient descent 

  $$w^{k+1} = w^k-\alpha\nabla f(w^k).$$

  Your choice of step-size, $\alpha$ seems correct. The gradients don't blow up. There is no division by zero, no NaNs, no square rooting of minus one. In fact, things begin quite well - with an impressive, almost immediate decrease in the loss. But after a time, you notice something amiss. The improvements begin to slow. The loss isn't getting smaller. And convergence is nowhere in sight. Something is wrong, but what? -->
  </p>

  <p>
  You may have encountered the optimizers old nemesis - pathological curvature. Pathological curvature is, simply put, regions of $f$ which aren't scaled properly. The landscapes are often described as a valleys, trenches, canals, ravines. The iterates either jump between valleys, or approach the optimum in small, timid steps. Progress along certain directions grind to a halt. In these unfortunate regions, gradient descent fumbles.  </p>
  <p>
  But there is a mysterious tweak to gradient descent which improves things significantly. The change is innocent, and costs almost nothing - all we do is give gradient descent a short term memory:

  $$
  \begin{aligned}
  z^{k+1}&=\beta z^{k}+\nabla f(w^{k})\\
  w^{k+1}&=w^{k}-\alpha z^{k+1},
  \end{aligned}
  $$

  Instead of taking taking a step in the gradient, we move in a geometrically decaying superposition all the previous gradients. When $ \beta = 0 $ , we recover gradient descent. But for $ \beta = 0.99 $ (sometimes $ 0.999$, if things are really bad), this appears to be the boost we need. Our iterations regain that speed and boldness it lost, speeding to the optimum with a renewed energy.

  <!-- there is a simple tweak to gradient descent which makes things work much better. Add an auxiliary sequence, $z^k$, and an extra parameter $\beta$, the <a href = "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc#L129">momentum</a> parameter, -->
<!--
  $$
  \begin{aligned}
  z^{k+1}&=\beta z^{k}+\nabla f(w^{k})\\
  w^{k+1}&=w^{k}-\alpha z^{k+1}.
  \end{aligned}
  $$

  When $ \beta = 0 $ , we recover gradient descent. But for $ \beta = 0.99 $ (sometimes $ 0.999$, if things are really bad), the situation improves quite dramatically. -->
  </p>
  <p>
  Optimizers call this minor miracle "acceleration". 
  </p>

<!--   <p>
  Now, If you've heard about momentum before, you might now be anticipating something along the lines of "Momentum smoothens out oscillations within gradient descent, allowing the iterates to move in the direction of the true optimum". This little bit of mathematical folklore is unfortunately a half truth. As the above demo clearly shows, momentum can help even when there are no oscillations, sometimes even creating its own oscillations when pushed too far. The smoothing effect of momentum, as far as I can tell, is not the only source of its power.
  </p> -->

  <p>
  The new iteration, known best to machine learners as Momentum<dt-fn> also known by it's older names, the Heavy Ball Method or the Chebychev Method </dt-fn> <dt-cite key="sutskever2013importance,polyak1964some,rutishauser1959theory"></dt-cite>, may seem like a cheap hack. A simple trick to get around gradient descent's more aberrant behavior - a smoother for oscillations between steep canyons. But the truth, if anything, is the other way round. It is gradient descent which is the hack. First, momentum gives a guaranteed quadratic speedup on convex functions. This is no small matter - this is the speedup you get from the Fast Fourier Transform, and Grover's Algorithm. When the universe speeds things up for you quadratically, you should start to pay attention.
  </p>

  <p>
  But there's more. A lower bound, courtesy of Nesterov <dt-cite key="nesterov2013introductory"></dt-cite>, states that momentum is in a certain technical sense optimal. Now this doesn't mean it is the best algorithm under any circumstances for all functions. But it does mean it satisfies some curiously beautiful mathematical properties which scratches a very human itch for perfection and closure. But more on that later. Let's say this for now - momentum is an algorithm for the book.
  </p>

<hr>
  <h2>First Steps: Gradient Descent</h2>
  <p>
  We begin by studying gradient descent on simplest model possible which isn't trivial. I choose the convex quadratic,

  $$f(w) = \tfrac{1}{2}w^TAw - b^Tw, \qquad w \in \mathbf{R}^n. $$

  Simple as this model may be, it is rich enough to approximate of many functions (think of $A$ as your favorite model of curvature - the Hessian, Fisher Information Matrix <dt-cite key="amari1998natural"></dt-cite>, etc) and captures all the key features of pathological curvature. And more importantly, we can write a formula for gradient descent iterations here in closed form, with no approximating bounds or inequalities.
  </p>

  <p>
  This is how it goes. Since $\nabla f(w)=Aw - b$, the iterates are

  $$
  w^{k+1}=w^{k}- \alpha (Aw^{k} - b)
  $$

  Here's the trick. There is a very natural space to view gradient descent where the iterates all act independently - the eigenvalues of $A$.
  </p>
  <div style = "width:750px; height:340px; display:block; margin-left:auto; margin-right:auto; position:relative">
  <div id = "mom1" style="width:400px; position:absolute; left:0px; top:0px"></div>
  <div id = "mom2" style="width:400px; position:absolute; left:400px; top:0px"></div>
  </div>
  <script>

  (function() { var U = givens(Math.PI/4)
  var Ut = numeric.transpose(U)
  // Render Foreground
  var left = d3.select("#mom1").style("border", "1px solid rgba(0, 0, 0, 0.2)")

  var c1 = genIterDiagram(quadf,  [0,0], [[-3,3],[-3,3]])
            .width(340)
            .height(340)
            .iters(300)
            .alpha(0.018)
            .showSolution(false)
            .pathWidth(1)
            .circleRadius(1.5)
            .pointerScale(0.5)
            .showStartingPoint(false)           
            .drag(function() {
              c2.control(c1.alpha(),
                         c1.beta(),
                         numeric.dot(U,c1.w0())) })
            (left)

  // var s = 150
  // left.select("svg")
  //     .append("line").attr("x1", s).attr("x2",340-s).attr("y1", s).attr("y2",340-s)
  //     .style("stroke", "red")
  //     .style("stroke-width", 1)

  // s = 100
  // left.select("svg")
  //     .append("line").attr("x1", 340-s).attr("y1", s).attr("x2",s).attr("y2",340-s)
  //     .style("stroke", "red")
  //     .style("stroke-width", 1)

  // s = 100
  // left.select("svg")
  //     .append("text").attr("x", 135).attr("y", 145)
  //     .style("font-size", "13px")
  //     .text("q₁")

  // left.select("svg")
  //     .append("text").attr("x", 240).attr("y", 95)
  //     .style("font-size", "13px")
  //     .text("q₂")

  var right = d3.select("#mom2").style("border", "1px solid rgba(0, 0, 0, 0.2)")
  var c2 = genIterDiagram(eyef,  [0,0], [[-3,3],[-3,3]])
            .width(340)
            .height(340)
            .iters(300)
            .alpha(0.018)
            .showSolution(false)
            .pathWidth(1)
            .circleRadius(1.5) 
            .pointerScale(0.5)
            .showStartingPoint(false)           
            .drag(function() {
              c1.control(c2.alpha(),
                         c2.beta(),
                         numeric.dot(Ut,c2.w0())) })
            (right)

// Initalize
c2.control(0.018,0,[-2.5,1])
c1.control(0.018,0,numeric.dot(Ut,[-2.5,1])) })()

</script>
<p>
  Every symmetric matrix $A$ has an eigenvalue decomposition

  $$
  A=Q \text{diag}(\lambda_{1},\ldots,\lambda_{n})Q^{T},\qquad Q = [q_1,\ldots,q_n],
  $$

  and as per convention, we will assume that the $\lambda_i$'s are sorted, from smallest $\lambda_1$ to biggest $\lambda_n$. Perform a change of variables, $x^{k} = Q^T(w^{k} - w^\star)$. And the iterations break apart, becoming

  $$
  x_{i}^{k+1}=x_{i}^{k}-\alpha \lambda_ix_{i}^{k} = (1-\alpha\lambda_i)x^k_i=(1-\alpha \lambda_i)^kx^0_i
  $$

  Moving back to our original space $w$, we can see that

  $$
  w^k - w^\star = Qx^k=\sum_i^n x^0_i(1-\alpha\lambda_i)^k q_i
  $$

  and there we have it - gradient descent in closed form.
  </p>
  </p>
  <h3>Decomposing the Error</h3>
  <p>
  The above equation admits a simple interpretation. $x^0$ is the component of the error in the initial guess in the space of $Q$. There are $n$ such errors, and each of these errors follow their own, solitary path to the minimum, at a rate of $1-\alpha\lambda_i$. The closer that number is to $1$, the slower it converges.
  </p>
  <p>
  For most step-sizes, the eigenvectors with largest eigenvectors converge the fastest. This triggers an explosion of progress in the first few iterations, before things slow down as the smaller eigenvector's struggles are revealed. By writing the contributions of each eigenspaces's sub-optimality to the loss
  $$
  f(w^{k})-f(w^{\star})=\sum(1-\alpha\lambda_{i})^{2k}\lambda_{i}[x_{i}^{0}]^2
  $$
  we can visualize the contributions of each eigenspace to the loss $f(w^k)$.
  </p>
  <figure style="position:relative; width:920px; height:350px">
  <figcaption style="position:absolute; text-align:left; left:140px; width:350px; height:80px">Optimization can be seen as combination of several component problems, shown here as <svg style="position:relative; top:2px; width:3px; height:14px; background:#fde0dd"></svg> 1 <svg style="position:relative; top:2px; width:3px; height:14px; background:#fa9fb5"></svg> 2 <svg style="position:relative; top:2px; width:3px; height:14px; background:#c51b8a"></svg> 3 with eigenvalues <svg style="position:relative; top:2px; width:3px; height:14px; background:#fde0dd"></svg> $\lambda_1=1$, <svg style="position:relative; top:2px; width:3px; height:14px; background:#fa9fb5"></svg> $\lambda_2=10$, and <svg style="position:relative; top:2px; width:3px; height:14px; background:#c51b8a"></svg> $\lambda_3=100$ respectively. </figcaption>

<!-- ["#fde0dd", "#fa9fb5", "#c51b8a"]
 -->  
  <div id = "sliderStep" style="position:absolute; left:550px; width:250px; height:100px">
    <div id="stepSizeMilestones" class="figtext" style="position:absolute; left:15px; top:15px">Step Size </div>
    <div class="figtext2" style="position:absolute; font-size:11px; left:152px; top:18px">Optimal Step Size </div>
    <svg style="position:absolute; font-size:10px; left:224px; top:34px">
<line marker-end="url(#arrowhead)" style="stroke: black; stroke-width: 1.5; visibility: visible;" x2="5" y2="10" x1="5" y1="0"></line>
    </svg>

  </div>
  <div id = "obj"></div>  
  </figure>  
  <script>
  var graphDiv = d3.select("#obj")
                   .style("width",  920 + "px")
                   .style("height", 300 + "px")
                   .style("top", "90px")
                   .style("position", "relative")
                   .style("margin-left", "auto")
                   .style("margin-right", "auto")
                   .attr("width", 920)
                   .attr("height", 500)
  
  var svg = graphDiv.append("svg").attr("width", 920).attr("height", 500)

  var updateSliderGD = renderMilestones(svg, function() {})

  var slidera = sliderGen([250, 80])
              .ticks([0,1,200/(101),2])
              .change( function (i) {
                var html = katex.renderToString("\\alpha = " + i.toPrecision(4) )
                d3.select("#stepSizeMilestones")
                  .html("Stepsize " + html )
                updateSliderGD(i,0.000)
              } )
              .ticktitles(function(d,i) { return [0,1,"",2][i] })
              .startxval(200/(106))
              .cRadius(7)
              .shifty(-12)
              .shifty(10)
              .margins(20,20)(d3.select("#sliderStep"))


  renderDraggable(svg, [133.5, 23], [114.5, 90], 2, " ").attr("opacity", 0.4)
  renderDraggable(svg, [133.5, 88], [115.5, 95], 2, " ").attr("opacity", 0.4)
  renderDraggable(svg, [132.5, 154], [114.5, 100], 2, " ").attr("opacity", 0.4)


  svg.append("text")
    .attr("class", "katex morsd mathit")
    .style("font-size", "19px")
    .style("font-family","KaTeX_Math")
    .attr("x", 105)
    .attr("y", 50)
    .attr("text-anchor", "end")
    .attr("fill", "gray")
    .html("f(w<tspan baseline-shift = \"super\" font-size = \"15\">k</tspan>) - f(w<tspan baseline-shift = \"super\" font-size = \"15\">*</tspan>)")



  svg.append("text")
    .style("font-size", "13px")
    .attr("x", 0)
    .attr("y", 80)
    .attr("dy", 0)
    .attr("transform", "translate(110,0)")
    .attr("class", "caption")
    .attr("text-anchor", "end")
    .attr("fill", "gray")
    .text("At the initial point, the error in each component is equal.")

  svg.selectAll(".caption").call(wrap, 100)


  svg.append("text")
    .style("font-size", "13px")
    .attr("x", 420)
    .attr("y", 270)
    .attr("dy", 0)
    .attr("text-anchor", "end")
    .attr("fill", "gray")
    .text("At the initial point, the error in each component is equal.")

  </script>
  <p>
<!--   The loss, $f(w^k)$ is a superposition of exponentially decaying curves. For the quadratic function $f$ with $A = \text{diag}([1,10,100])$ and $b = [1,1/\sqrt{10}, 1/\sqrt{100}]$, we see three curves, each with its own rate of decay, depending on the step-length $\alpha$. The timeline beneath the plot visualizes the milestones of descent - with a tick every time error in one eigenspace as it dips below a certain tolerance. Our iterates thus move through 3 stages of convergence, and when the $k$ passes the final milestone, convergence is achieved. The rate at which the slowest error goes down is referred to as the convergence rate.
 -->  
 <h3>Choosing A Step-size</h3>
  <p>
  The above analysis gives us immediate guidance as to how to set a step-size $\alpha$. In order to converge, each $|1-\alpha \lambda_i|$ must be strictly less than 1. <dt-fn>All workable step-sizes, therefore, fall in the interval $0<\alpha<\frac{2}{\lambda_n}$</dt-fn> The overall convergence rate is determined by the slowest error component, which must be either $\lambda_0$ or $\lambda_n$:
  $$\begin{aligned}\text{rate}(\alpha) & ~=~ \max_{i}\left|1-\alpha\lambda_{i}\right|\\[0.9em] & ~=~ \max\left(|1-\alpha\lambda_{0}|,~ |1-\alpha\lambda_{n}|\right) \end{aligned}$$
  </p>
  <p>
  This overall rate is minimized when the rates for $\lambda_0$ and $\lambda_n$ are the same -- this mirrors our informal observation in the previous section that the optimal step size causes the first and last eigenvector to converge at the same time. If we work this through we get:

  $$
  \begin{aligned}
  \text{optimal }\alpha ~=~{\mathop{\text{argmin}}\limits_\alpha} ~\text{rate}(\alpha) & ~=~\frac{2}{\lambda_{1}+\lambda_{n}}\\[1.4em]
  \text{optimal rate} ~=~{\min_\alpha} ~\text{rate}(\alpha) & ~=~\frac{\lambda_{n}/\lambda_{1}-1}{\lambda_{n}/\lambda_{1}+1}
  \end{aligned}
  $$
<!--   For ill conditioned problems, the optimal $\alpha$ gets uncomfortably close to limit of divergence. This, however, justifies the heuristic of choosing the largest stepsize you can get away with.
 -->  </p>
  <p>
  Notice the ratio $\lambda_n/\lambda_1$ determines the convergence rate of the problem. In fact, this ratio appears often enough that we give it a name, and a symbol - the condition number.
  $$
  \text{condition number} := \kappa :=\frac{\lambda_n}{\lambda_1}
  $$
  The condition number means many things. It is a measure of how singular a matrix is. It is a measure of how robust $A^{-1}b$ is to perturbations in $b$. And in this context - the condition number gives us a measure of how poorly gradient descent will perform. A ratio of $1$ is ideal, giving convergence in one step (of course, the function is trivial). And larger the ratio, the slower gradient descent will be. The condition number is therefore a direct measure pathological curvature.
  </p>

<hr>
  <h2>
  The Path of Descent
  </h2>

  <p>
  The above analysis reveals an interesting insight - all errors are not made equal. Indeed, there are different kinds of errors, $n$, to be exact, one for each of the eigenvectors of $A$. And gradient descent is better at correcting some kinds of errors better than others. <!-- The pathological directions, the eigenvectors with smallest eigenvalues, are what causes gradient descent to slow down.  -->Lets take apply this understanding on a few, simple examples.
  </p>

  <h3>Linear Regression</h3>
  <p>
  Linear Regression, conveniently a quadratic objective, provides a cartoon we need for understanding pathological curvature in deep learning. Let's do a quick refresher. In Linear Regression we fit $n$ features (the components of $z_i$) to observations $d_i$. Assume there are $m$ data points, stacked to form matrix $Z$. Then the optimization problem we're solving is

  $$
  \text{minimize}\qquad \frac{1}{2}\sum_i^m (z_i^Tw - d_i)^2 = \tfrac{1}{2}\|Zw-d\|^2
  $$

  which is equivalent to
  $$
  \text{minimize}\qquad\tfrac{1}{2}w^{T}Z^{T}Zw-(Zd)^{T}w
  $$
  a quadratic function. To understand gradient descent on linear regression, we must understand the eigenvectors of $Z^TZ$.
<!--   Now notice a surprising connection to unsupervised learning. The directions $q_i$ are exactly principle components of $Z$! And thus we arrive at a first conclusion
   </p>
   <p><i>
  Gradient descent makes good progress in directions corresponding to the strongest principle components, and bad progress in its weakest. </i>
  </p>
  <p>
  This makes intuitive sense. The principle components of $X$ measure the quality of data in the corresponding directions. And thus, the strongest principle components also give the strongest error correcting signals in the gradient! Let's refine this insight with a few examples.
  </p> -->
  <h3>Bad Scaling</h3>

  <p>
  One simple culprit of pathological curvature is a bad scaling of the data. Even if the data were perfectly uncorrelated with mean 0, i.e. $Z^TZ$ was diagonal, the condition number of the matrix still depends on the variance of the individual features (the values of the diagonal). If the ratio of the largest and smallest variances are large, the matrix $Z^TZ$ becomes ill conditioned.<dt-fn>This holds even if the matrix is correlated, see <dt-cite key="wiesler2011convergence"></dt-cite></dt-fn>
  </p>

  <p>
  Fortunately, this kind of pathological curvature is relatively easily fixed. It displays a clear signature - the value of the gradients in the "pathological coordinates" are abnormally small. And they can thus be weighted upwards. This is the principle behind Adagrad <dt-cite key="duchi2011adaptive"></dt-cite> and ADAM <dt-cite key="kingma2014adam"></dt-cite>. And in the training of deep neural networks, we can modify our objective so that the data is normalized before moving to the next layer - this is batch normalization <dt-cite key="ioffe2015batch"></dt-cite>, a surprisingly powerful heuristic. </p>

<!--   The problem gets more complex, however, when the directions of pathology are not axis aligned. Lets say you've been given some data, scaled properly, and you discover $Z^TZ$ is ill-conditioned. What do the pathological directions mean? As we shall see, we can interpret these directions as blind spots in our data. And somewhat surprisingly, pathological curvature, our old nemesis turns into a friend.  --><!-- If all our data lies close to a linear manifold, then we would have no information on the direction orthogonal to that, and that results in a pathological direction.  -->

<!--   <h3>Feature Coadaptiation</h3>

  <p>
  A more insidious example of pathological curvature comes when the directions of pathology are not axis aligned. In an extreme example, two features (say $i$ and $j$) are completely identical. Say, $i$ corresponds to height in feet, and $j$ corresponds to height in meters. Let $e_i$ be the $i^{th}$ unit vector. Then the matrix $Z^TZ$ is singular, as

  $$
  Ze_i = Ze_j \Rightarrow Z^TZ(e_i-e_j) = 0
  $$

  and the condition number is infinite. If the two columns were close, but not equal (height and weight, say), the matrix would be close to singular, but ill conditioned. In general, if any feature can be written as a linear combination of other features, the direction orthogonal to that, the data's "blind spot" will be pathological. And as the number of features get higher, we introduce subtle correlations everywhere, making the question of where the pathological curvature lies murkier. But as we shall see,  the eigenvectors of $Q$ give us a powerful tool to peel these correlations apart. And we arrive at a surprising result - the directions of pathological curvature are exactly those directions which overfit the data.
  </p> -->

  <h3>Understanding Early Stopping</h3>
<!-- 
  <p>
  Pathological curvature gets more complex, however, when the directions of pathology are not axis aligned. This isn't necessarily a bad thing, however. By understanding what these directions of pathological curvature mean, we can shed light on an interesting heuristic used in neural network training - early stopping.
  </p> -->
<!--  As we shall see, the smallest eigenvectors of $Z^T Z$ correspond often to blind spots in our data. And deliberately decreasing the error in these eigenspaces can lead to overfitting. -->  
  <p>
  The situation where the directions of pathology aren't axis aligned gets more complicated. But by studying this carefully, we can precisely understand and characterize the path the iterates take to the optimum, gain a few insights into a common heuristic in the optimization of neural networks - Early Stopping. 
  </p>

  <p>
  It has been noted by many practitioners that there exists a turning point sometime in the middle of optimization where the model's generalization capacity peaks. The observation is as follows. Early in the optimization, when we are far from the optimum, both the training and test error goes down - all is well. But somewhere in the middle, we hit a point of negative returns, so to speak. The test error stops going down, and in fact, starts going up! Each iterate now hurts our model, in a classical case of over-fitting. This is a strikingly counterintuitive fact. Too much optimality, it seems, is a bad thing! But how can this be?
  </p>

  <h4> Example: Polynomial Regression</h4>

  <p>
  Lets understand this phenomena in the quadratic model. Consider polynomial regression. Given 1D data, $\xi_i$, we wish to fit the model

<!--   $$
  \xi \mapsto w_0 + w_1\xi + w_2\xi^2 + \cdots + w_n\xi^{n-1} 
  $$
 -->
<!--   To the human eye, these monomials look very much alike. $\xi^2$ resembles $\xi^4$, and $\xi^3$ resembles $\xi^5$. In fact, from a visual inspection of these curves, one might venture that beyond the fourth or so power, the polynomials cannot furnish any new information.  -->

<!--   Observe that our model is linear in the weights. That is to say, after training, we take our $w$, and apply it to whatever data is out there via a linear combination of monomials,
 -->
  $$
\text{model}(\xi)=w_{1}p_{1}(\xi)+\cdots+w_{n}p_{n}(\xi)\qquad p_{i}=\xi\mapsto\xi^{i-1}
  $$
  
  to our data. This model, though nonlinear in the input ($\xi$) is linear in the weights, i.e. we can write the model as a linear combination of monomials, like:
  </p>

  <figure id = "polydisplay" style="height:165px"></figure>

  <script>
  (function() { 

  // Preprocess x, get eigendecomposition, etc
  var x = [-0.6, -0.55,-0.5,-0.45,-0.4,0.4,0.45,0.5,0.55,0.6]
  var D = vandermonde(x, 5)
  var Eigs = eigSym(numeric.dot(numeric.transpose(D),D))
  var U = Eigs.U
  var lambda = Eigs.lambda

  // Preprocess y
  var b = [-3/2,-4/2,-5/2,-3/2,-2/2,1/2,2/2,3/2,2/2,1/2]
  var Dtb = numeric.dot(b,D)
  var sol = numeric.mul(numeric.dot(U, Dtb), lambda.map(inv))

  var step = 1.8/lambda[0]
  var iter = geniter(U, lambda, Dtb, step)

  var eigensum = d3.select("#polydisplay")

  var wi = [2,2,2,2,2,2]

  function refit(b) {
    var Dtb = numeric.dot(b,D)
    var sol = numeric.mul(numeric.dot(U, Dtb), lambda.map(inv))
    var Utsol = numeric.dot(sol,U)
    eigenControl.updateweights(Utsol)
  }

  var eigenControl = renderEigenPanel(eigensum, numeric.identity(6), x, b, wi, refit)
  })()

  </script>
  <p>

  Because of the linearity, we can fit this model at our observed data $\xi_i$ by using linear regression on the model mismatch

  $$
  \text{minimize}_w \qquad\frac{1}{2}\sum_i (\text{model}(\xi_{i})-d_{i})^{2} = \tfrac{1}{2}\|Zw - d\|^2
  $$
  where
  $$
  Z=\left(\begin{array}{ccccc}
  1 & \xi_{1} & \xi_{1}^{2} & \ldots & \xi_{1}^{n-1}\\
  1 & \xi_{2} & \xi_{2}^{2} & \ldots & \xi_{2}^{n-1}\\
  \vdots & \vdots & \vdots & \ddots & \vdots\\
  1 & \xi_{m} & \xi_{m}^{2} & \ldots & \xi_{m}^{n-1}
  \end{array}\right).
  $$
  </p>

  <p>
  The path of convergence, as we know, is elucidated when we view the iterates in the space of $Q$. So let's recast our regression problem in $Q$ space. First, we do a change of variables, by rotating $w$ into $Qw$, and counter-rotating our feature maps $p$ into eigenspace, $\bar{p}$. We can now conceptualize the same regression as one over a different polynomial basis, with the model

  $$
  \text{model}(\xi)=x_{1}\bar{p}_{1}(\xi)+\cdots+x_{n}\bar{p}_{n}(\xi)\qquad p_{i}=\sum q_{ij}p_j.
  $$

  This model is identical to the old one. But these new features $\bar{p}$ (which I call eigenfeatures) and weights have the pleasing property that each coordinate acts independently of the other. Now our optimization problem breaks down, really, into $n$ small 1D optimization problems. And each coordinate can be optimized greedily and independently, one at a time in any order, to produce the final, global, optimum. These independent features, which I call the eigenfeatures, are also much more informative,
  </p>


  <figure id = "eigendisplay" style="height:285px"></figure>

  <script>
  (function() { 
  var inv = function(lambda) { return 1/lambda }
  var scal = function(lambda) { return lambda < 1e-10 ? -100 : 1.5/Math.sqrt(lambda) }

  // Preprocess x, get eigendecomposition, etc
  var x = [-0.6, -0.55,-0.5,-0.45,-0.4,0.4,0.45,0.5,0.55,0.6]
  var D = vandermonde(x, 5)
  var Eigs = eigSym(numeric.dot(numeric.transpose(D),D))
  var U = Eigs.U
  var lambda = Eigs.lambda

  // Preprocess y
  var b = [-3/2,-4/2,-5/2,-3/2,-2/2,1/2,2/2,3/2,2/2,1/2]
  var Dtb = numeric.dot(b,D)
  var sol = numeric.mul(numeric.dot(U, Dtb), lambda.map(inv))

  var step = 1.8/lambda[0]
  var iter = geniter(U, lambda, Dtb, step)

  var eigensum = d3.select("#eigendisplay")

  var wi = lambda.slice(0).map(scal)

  function refit(b) {
    // var Dtb = numeric.dot(b,D)
    // iter = geniter(U, lambda, Dtb, step)
    // updateEverything(slidera.xval())
    var Dtb = numeric.dot(b,D)
    var sol = numeric.mul(numeric.dot(U, Dtb), lambda.map(inv))
    var Utsol = numeric.dot(sol,U)
    eigenControl.updateweights(sol)
  }

  var eigenControl = renderEigenPanel(eigensum, U, x, b, wi, refit, true)

  var annotate = eigensum

  annotate.append("figcaption")
  .style("width", 200 + "px")
  .style("height", 150 + "px")
  .style("left", "30px")
  .style("position", "absolute")
  .style("border-top", "1px solid black")  
    .style("padding", "10px")    
  .html("The first 2 eigenfeatures, the largest components, captures variations between the clusters. ")

  annotate.append("figcaption")
  .style("width", 200 + "px")
  .style("height", 150 + "px")
  .style("left", "300px")
  .style("position", "absolute")
  .style("border-top", "1px solid black")  
  .style("padding", "10px")  
  .html("Next there are smooth variations within clusters, peaks within clusters,")

  annotate.append("figcaption")
  .style("width", 200 + "px")
  .style("height", 150 + "px")
  .style("left", 300+270+"px")
  .style("position", "absolute")
  .style("border-top", "1px solid black")
  .style("padding", "10px")    
  .html("Finally, jagged polynomials which differ wildly on neighboring points. ")})()


  var figwidth = d3.select("#eigendisplay").style("width")
  var figheight = d3.select("#eigendisplay").style("height")
  var svg = d3.select("#eigendisplay")
                      .append("svg")
                      .style("width", figwidth)
                      .style("height", figheight)
                      .style("position", "absolute")
                      .style("top","0px")
                      .style("left","0px")
                      //.style("pointer-events","none")

  // Swoopy Annotator
  var annotations = [
  {
    "x": 0,
    "y": 0,
    "path": "M 807,198 A 26.661 26.661 0 0 1 838,159",
    "text": "drag points to fit data",
    "textOffset": [
      799,
      214
    ]
  }
  ]

  var swoopy = d3.swoopyDrag()
    .x(function(d){ return (d.x) })
    .y(function(d){ return (d.y) })
      .draggable(false)
      .annotations(annotations)

  var svg = d3.select("#eigendisplay").selectAll("svg")
  var swoopySel = svg.append("g").attr("class", "figtext").call(swoopy)

  svg.append('marker')
      .attr('id', 'arrow')
      .attr('viewBox', '-10 -10 20 20')
      .attr('markerWidth', 20)
      .attr('markerHeight', 20)
      .attr('orient', 'auto')
    .append('path')
      .attr('d', 'M-6.75,-6.75 L 0,0 L -6.75,6.75')
      .attr("transform", "scale(0.5)")

  swoopySel.selectAll('path').attr('marker-end', 'url(#arrow)')


  </script>
  <p>
  The observations in the above diagram can be justified mathematically. From a statistical point of view, we would like a model which is in some sense, robust to noise. Our model cannot possibly be meaningful if the slightest perturbation to the observations changes the entire model dramatically. And thus, it would be useful if there were a decomposition of our model into the parts which fit the data, and the parts which fit the noise. The eigenfeatures are in a sense are exactly such a decomposition. In the eigenfeature basis, the most robust components appear in the front (with the largest eigenvalues), and the most sensitive components in the back (with the smallest eigenvalues). And with a small perturbation to the observations, it is the smallest eigenvalues which jump around the most, while the largest eigenvalues remain stable.
  </p>

  <p>
  Now this measure of robustness, by a rather convenient coincidence, is also a measure of how easily an eigenspace converges. And thus, we arrive at a surprising result. The "pathological directions" - the eigenspaces which converge the slowest are also those which are most sensitive to noise! This property is easily exploited. Start at a simple initial point like 0 (call this a prior, if you like), and track the iterates till a desired level of complexity is reached. Lets see how this plays out in gradient descent.
  <!--  This qualitative observation can be justified by thinking in terms of perturbation theory. Indeed, a simple derivation <dt-fn>

   we can justify this by thinking of what happens when we inject our observations with a bit of isotropic, $N(0,I)$ noise $\nu$. If we view our solution, $w^\star$ in the space of $U$.

  $$
  x^{\star} = Qw^{\star}=Q(X^{T}X)^{-1}(d+[Q^{T}\nu])=Qw^{\star}+\left(\begin{array}{c}
\lambda_{1}^{-1}\nu\\
\vdots\\
\lambda_{n}^{-1}\nu
\end{array}\right)
$$
  
  </dt-fn> shows that an interesting.  pathological directions are those most sensitive to tiny changes to the observations. In other words, their purpose of existence is to fit the the smallest changes to $d$, the noise.  is a somewhat surprising result. What this says is that : in linear regression, the hardest directions to optimize are also those which fit the noise. And this property is easy to exploit - just stop optimizing before those errors go down. This is early stopping. Lets see how this plays out in gradient descent.
 -->  <!-- Pathological curvature here is a feature, not a bug. The hardest directions to move along are those we dont want to fit anyway. And if we set our initial point to something simple, like $0$, then we can exploit the first phases of the optimization to get a structurally simple result. Lets see how this plays out on a more complex example. -->
  </p>

  <figure id = "eigensum2" style="height:360px"></figure>

  <script>
  (function() { var inv = function(lambda) { return 1/lambda }
  var scal = function(lambda) { return lambda < 1e-10 ? -100 : 1.5/Math.sqrt(lambda) }

  // Preprocess x, get eigendecomposition, etc
  var x = [-0.6, -0.55,-0.5,-0.45,-0.4,0.4,0.45,0.5,0.55,0.6]
  var b = [-3/2,-4/2,-5/2,-3/2,-2/2,1/2,2/2,3/2,2/2,1/2]

  var D = vandermonde(x, 5)
  var Eigs = eigSym(numeric.dot(numeric.transpose(D),D))
  var U = Eigs.U
  var lambda = Eigs.lambda

  // Preprocess y
  var Dtb = numeric.dot(b,D)
  var sol = numeric.mul(numeric.dot(U, Dtb), lambda.map(inv))

  var step = 1.8/lambda[0]
  var iter = geniter(U, lambda, Dtb, step)

  var eigensum = d3.select("#eigensum2")

  var wi = lambda.slice(0).map(scal)

  function refit(b) {
    var Dtb = numeric.dot(b,D)
    iter = geniter(U, lambda, Dtb, step)
    updateEverything(slidera.xval())
  }

  var eigenControl = renderEigenPanel(eigensum, U, x, b, wi, refit, true)

  var barlengths = getStepsConvergence(lambda, step).map(Math.log)

  var onChange = function(i) {
    eigenControl.updateweights(numeric.dot(U,iter(Math.floor(Math.exp(i-0.1)) ))) 
  }
  var sliderControl = sliderBarGen(barlengths).update(onChange)(d3.select("#eigensum2"))

  d3.select("#eigensum2").append("figcaption")
        .style("width", "120px")
        .style("position", "absolute")
        .style("left", "820px")
        .style("top","200px")
        .html("When an eigenspace has converged to three significant digits, the bar greys out. Drag the observations to change fit.")

  sliderControl.slidera.init()

  var figwidth = d3.select("#eigensum2").style("width")
  var figheight = d3.select("#eigensum2").style("height")
  var svgannotate = d3.select("#eigensum2")
                      .append("svg")
                      .style("width", figwidth)
                      .style("height", figheight)
                      .style("position", "absolute")
                      .style("top","0px")
                      .style("left","0px")
                      .style("pointer-events","none")

  renderDraggable(svgannotate, [139.88888549804688, 243.77951049804688], [121.88888549804688, 200.77951049804688], 5, "We begin at x=w=0")})()

  </script>
<!--   <p>
  The behavior is exactly as you might expect. Gradient descent produces a decent result just under 50 iterations, and starts overfitting data only at the $k=100$ mark. Things then slow down for a rather long time, only reaching final convergence in about about $k=1241709$. And the iterates are exactly as we expect it to be - we see structurally simple iterates first, and complex ones only later.
  </p> -->

  <p>
  The effect of early stopping is very similar to that of more conventional methods of regularization, such as Tikhonov Regression. Both methods try to suppress the components of the smallest eigenvalues, though both through a employ a different method of spectral decay <dt-fn>In Tikhonov Regression we add a quadratic penalty to the regression, minimizing
$$
\text{minimize}\qquad\tfrac{1}{2}\|Zw-d\|^{2}+\eta\|w\|^{2}=\tfrac{1}{2}w^{T}(Z^{T}Z+\eta I)w-(Zd)^{T}w
$$
Recall that $Z^{T}Z=Q\text{diag}(\Lambda_{1},\ldots,\Lambda_{n})Q^T$. The solution to Tikhonov Regression is therefore
$$
(Z^{T}Z+\eta I)^{-1}(Zd)=Q\text{diag}\left(\frac{1}{\lambda_{1}+\eta},\cdots,\frac{1}{\lambda_{n}+\eta}\right)Q^T(Zd)
$$
Rewrite the eigenvalues of $(Z^{T}Z+\eta I)^{-1}$ as 
$$
\frac{1}{\lambda_{i}+\eta}=\frac{1}{\lambda_{i}}\left(1-\left(1+\lambda_{i}/\eta\right)^{-1}\right).
$$
Now the term $1-\left(1+\lambda_{i}/\eta\right)^{-1}$ can be seen as a multiplicative decay which hits the smallest eigenvalues (those closest to $0$) the hardest. Gradient descent can be seen as employing a similar decay, but with the decay rate $1-\left(1-\alpha\lambda_{i}\right)^{k}$ instead. Note that this decay is dependent on the stepsize. For stepsizes
close to the upper limit of convergence, the smallest eigenvalue may converge faster (or at the same rate) as the largest. These stepsizes,
however, occupy a minuscule fraction of the set of acceptable stepsizes, and are almost never encountered in practice. 
</dt-fn>. But early stopping has a distinct advantage. Once the stepsize is chosen, there are no regularization parameters to fiddle with. Indeed, in the course of a single optimization, we have the entire family of models, from underfitted to overfitted, at our disposal. This gift, it seems, doesn't seem to come at a price. A beautiful free lunch <dt-cite key="hintonNIPS"></dt-cite> indeed.
  
<!--   The behavior is exactly as you might expect. Our model has more than enough expressive power to overfit any dataset. And yet, for all reasonable runtimes, it does not. Gradient descent produces a decent result in a few minutes, and starts overfitting data only at the $k=10^8$ mark (a year and a half of runtime). And the whole thing reaches final convergence in about about 10 billion years, assuming your computer's still running then! That seems like a lot of work for polynomial regression in 25 variables. But the directions of slowest progress are also the directions which overfit the data. And thus pathological curvature here, is a surprising friend, putting into check our model's worst instincts. -->
  </p>

  <h3>Laplacian Systems</h3>
  <p>

  A completely different source of pathological curvature comes from a quadratics defined on graphs - the graph Laplacian. Imagine a drop of ink, diffusing through water. Movement through equilibrium is made only through local corrections - and hence left undisturbed, its march towards equilibrium is slow and laborious. This is too, a manifestation of pathological curvature, and can be seen vividly in Laplacian Systems.
  </p>
  <p>
  Given a graph $G=(V,E)$, the quadratic form induced by the graph is.
  $$
  f(w) =  \frac{1}{2} \sum_{i,j\in E} (w_i - w_j)^2 = \frac{1}{2}w^T L_G w.
  $$
  The matrix $L_G$ is the Laplacian matrix, and can be written explicitly as
  $$
  [L_{G}]_{ij}=\begin{cases} \text{degree of vertex }i & i=j\\ -1 & i\neq j,(i,j)\text{ or }(j,i)\in E\\ 0 & \text{otherwise} \end{cases}
  $$
  The study of Laplacians and their quadratic forms form a rich field of mathematics, which relate the linear algebraic properties of the matrix $L_G$ and the graph $G$. There is one pertinent fact relevant to this article. The conditioning of $L_G$ is directly connected to the connectivity of the graph. 
  </p>
  <figure style="width:900px; height:250px">
  <svg width="900" height="300" id="graph"></svg>
  <figcaption id = "expander" style="position:absolute; left:70px; top:210px; width:200px">Small world graphs, like expanders and dense graphs, have excellent conditioning</figcaption>

  <figcaption id = "expander" style="position:absolute; left:370px; top:210px; width:200px">The conditioning of grids improves with its dimensionality.</figcaption>

  <figcaption id = "expander" style="position:absolute; left:670px; top:210px; width:200px">And long, wiry graphs, like paths, condition poorly. </figcaption>

  </figure>
  <script>
  var g1 = d3.select("#graph").append("g")
  genExpander(g1)

  var g2 = d3.select("#graph").append("g").attr("transform", "translate(300,0)")
  genGrid(g2)

  var g3 = d3.select("#graph").append("g").attr("transform", "translate(600,0)")
  genPath(g3)
  </script>
  <p>

  The reason for this will be clear in a moment.
  </p>
  <h4>The Colorization Problem</h4>
  <p>
  Let us consider a toy example. On a grid of pixels let $G$ be the graph with vertices as pixels and edges connecting neighbouring pixels. Let $D$ be a set of a few distinguished vertices. Then we try to minimize a sum of two sums, 
  </p>
<figure style="width:530px; height:95px; display:block; margin-left:auto; margin-right:auto; position:relative">
  <div style="position:relative; top:-40px">
  <span style="position:absolute; top:10px">
  $$\text{minimize} $$
  </span>

  <span style="position:absolute; left:90px">
  $$\qquad  \frac{1}{2} \sum_{i\in D} (w_i - 1)^2 $$
  </span>

  <figcaption style="position:absolute; left:140px; top:100px; width:130px">
  The <b>colorizer</b> pulls distinguished pixels towards 1
  </figcaption>

  <span style="position:absolute; left:310px; top:10px">
  $$+$$ 
  </span>

  <span style="position:absolute; left:350px">
  $$\frac{1}{2} \sum_{i,j\in E} (w_i - w_j)^2.$$
  </span>


  <figcaption style="position:absolute; left:353px; top:100px; width:150px">
  The <b>smoother</b> spreads out the color
  </figcaption>  
  </div>
  </figure>
  <p>
  This is the colorization problem. It is clear the optimal solution is the vector of all ones, and a simple inspection of the gradient iteration reveals we take a long time to get there. The gradient step, for each component, is some form of weighted average of the current value and its neighbors:

  $$
w_{i}^{k+1}=w_{i}^{k}-\alpha\sum_{j\in N}(w_{i}^{k}-w_{j}^{k})-\begin{cases}
\alpha(w_{i}^{k}-1) & i\in D\\
0 & i\notin D
\end{cases}
  $$

  This kind of local averaging is effective at smoothing out local variations in the function value, but poor at taking advantage of global structure.
  </p>
  <figure id = "flow"></figure>
  <script>renderFlowWidget(d3.select("#flow"))</script>
  <p>
  This observation can be made more precise by looking at the eigenvectors of $L_G$. The eigenvectors of a Laplacian, shown here, form a generalized Fourier basis for $R^n$. Eigenvectors with high frequencies have large eigenvalues, and the smallest eigenvalues are smooth. And thus we arrive at the conclusion

  </p>
  <p>
  <i>
  In graph Laplacians, gradient descent makes good progress in directions corresponding to high frequency errors, and bad progress low frequency errors.
  </i>
  </p>
  <p>
  This fact is not a mere mathematical curiosity - it is the basis of the celebrated multigrid <dt-cite key="briggs2000multigrid"></dt-cite> algorithm. The multigrid algorithms begins by noticing that low frequency errors in the full quadratic correspond to high frequency errors in a coarsened grid. Therefore, by doing a few steps of gradient descent on a hierarchy of such models, we hit the errors at all the right places, allow information to move quickly across the graph.
  </p>

<hr>
  <h2>The Dynamics of Momentum</h2>

  <p>
  Let's turn our attention back to momentum. Recall the momentum update is

  $$
  \begin{aligned}
  z^{k+1}&=\beta z^{k}+\nabla f(w^{k})\\
  w^{k+1}&=w^{k}-\alpha z^{k+1}.
  \end{aligned}
  $$

  Since $\nabla f(w^k) = Ax - b$, the update on the quadratic is

  $$
  \begin{aligned}
  z^{k+1}&=\beta z^{k}+ (Aw^{k}-b)\\
  w^{k+1}&=w^{k}-\alpha z^{k+1}
  \end{aligned}
  $$

  following <dt-cite key="o2015adaptive"></dt-cite>, we go through the same motions, with the change of variables $
  x^{k} = Q(w^{k} - w^\star)$ and $ y^{k} = Qz^{k}$ to yield the update rule

  $$
  \begin{aligned}
  y_{i}^{k+1}&=\beta y_{i}^{k}+\lambda_{i}x_{i}^{k}\\
  x_{i}^{k+1}&=x_{i}^{k}-\alpha y_{i}^{k+1}.
  \end{aligned}
  $$

  in which each of the components of $x$ and $y$ act independently of the other coordinates (though $x_i$ and $y_i$ are coupled). And therefore, we can rewrite our iterates as

  <dt-fn>
  This is true as we can write updates in matrix form as
  $$
  \left(\!\!\begin{array}{cc}
  1 & 0\\
  \alpha & 1
  \end{array}\!\!\right)\Bigg(\!\!\begin{array}{c}
  y_{i}^{k+1}\\
  x_{i}^{k+1}
  \end{array}\!\!\Bigg)=\left(\!\!\begin{array}{cc}
  \beta & \lambda_{i}\\
  0 & 1
  \end{array}\!\!\right)\left(\!\!\begin{array}{c}
  y_{i}^{k}\\
  x_{i}^{k}
  \end{array}\!\!\right)

  $$
  which implies, by inverting the matrix on the left,
  $$
  \Bigg(\!\!\begin{array}{c}
  y_{i}^{k+1}\\
  x_{i}^{k+1}
  \end{array}\!\!\Bigg)=\left(\!\!\begin{array}{cc}
  \beta & \lambda_{i}\\
  -\alpha\beta & 1-\alpha\lambda_{i}
  \end{array}\!\!\right)\left(\!\!\begin{array}{c}
  y_{i}^{k}\\
  x_{i}^{k}
  \end{array}\!\!\right)=R^{k+1}\left(\!\!\begin{array}{c}
  x_{i}^{0}\\
  y_{i}^{0}
  \end{array}\!\!\right)
  $$

  </dt-fn>
  $$
  \left(\!\!\begin{array}{c}
  y_{i}^{k}\\
  x_{i}^{k}
  \end{array}\!\!\right)=R^k\left(\!\!\begin{array}{c}
  y_{i}^{0}\\
  x_{i}^{0}
  \end{array}\!\!\right)
  \qquad
  R = \left(\!\!\begin{array}{cc}
  \beta & \lambda_{i}\\
  -\alpha\beta & 1-\alpha\lambda_{i}
  \end{array}\!\!\right).
  $$
<!--
  Where

  $$
  R = \left(\!\!\begin{array}{cc}
  \beta & \lambda_{i}\\
  -\alpha\beta & 1-\alpha\lambda_{i}
  \end{array}\!\!\right)
  $$
 -->
  We are almost there. We need a second trick. There are many ways of taking a matrix to the $k^{th}$ power. But for the $2 \times 2$ case there is an elegant and little known formula <dt-cite key="williamsnthpower"></dt-cite> in terms of the eigenvectors of $R$, $\sigma_1$ and $\sigma_2$.

  $$
\color{#AAA}{\color{black}{R^{k}}=\begin{cases}
\color{black}{\sigma_{1}^{k}}R_{1}-\color{black}{\sigma_{2}^{k}}R_{2} & \sigma_{1}\neq\sigma_{2}\\
\sigma_{1}^{k}(kR/\sigma_1-(k-1)I) & \sigma_{1}=\sigma_{2}
\end{cases},\qquad R_{j}=\frac{R-\sigma_{j}I}{\sigma_{1}-\sigma_{2}}}
  $$

   This formula may be a lot to take in, but I've highlighted the important bits. This formula plays the exact same role the individual convergence rates, $1-\alpha\lambda_i$ do in gradient descent. But instead of one geometric series, we have two, which may have real or complex values. And since we need both $\sigma_1$ and $\sigma_2$ to converge, our convergence criteria is now

   $$\max \{|\sigma_1|, |\sigma_2|\} < 1.$$

   The exact values of $\alpha$ and $\beta$ needed to achieve this is complicated. But by plotting $\max \{|\sigma_1|, |\sigma_2|\}$ <dt-cite key="flammarion2015averaging"></dt-cite>, we see that there are distinct regions of the parameter space which reveal a rich taxonomy of convergence behavior
  </p>

  <figure id = "momentum2D" style="width:984px; height:540px">
    <div class = "l-body" style="display:block">
      <div id = "momentumCanvas" style="position:absolute; left:45px"></div>
      <div id = "momentumAnnotation" style="position:absolute; width: 204px; height: 80px; left: 630px; top: 20px;"></div>
      <div style="position:absolute; width: 204px; height: 80px; left: 643px; top: 10px;" class ="figtext" >
        Convergence Rate

      </div>

      <figcaption style="position:absolute; width: 204px; height: 80px; left: 645px; top: 86px;">
      A plot of $\max{\sigma_1, \sigma_2}$ reveals distinct regions, each with its own style of convergence.
      </figcaption>

    </div>
    <div id = "taxonomy"></div>
    <svg id="momentumOverlay" style="position:absolute; width:984px; height:540px; z-index:4; pointer-events:none"></svg>
  </figure>  
  <script>

  var defaults = [[0.0015, 0.9],
                  [0.0015, 0.125],
                  [0.01, 0.00001],
                  [0.02, 0.05   ],
                  [0.025, 0.235 ]]

  coor = render2DSliderGen(
    function(a,b,bold) { 
      var xy = coor(a,b)
      updatePaths[0](xy[0], xy[1],bold)
      updateStemGraphs[0](a,b)
    }, 
    function(a,b,bold) { 
      var xy = coor(a,b)
      updatePaths[1](xy[0], xy[1],bold)
      updateStemGraphs[1](a,b)
    },  
    function(a,b,bold) { 
      var xy = coor(a,b)
      updatePaths[2](xy[0], xy[1],bold)
      updateStemGraphs[2](a,b)
    }, 
    function(a,b,bold) { 
      var xy = coor(a,b)
      updatePaths[3](xy[0], xy[1],bold)
      updateStemGraphs[3](a,b)
    },
    function(a,b,bold) { 
      var xy = coor(a,b)
      updatePaths[4](xy[0], xy[1],bold)
      updateStemGraphs[4](a,b)
    }, defaults)(d3.select("#momentumCanvas"))

  var tax = renderTaxonomy(d3.select("#momentum2D"))

  var updatePaths = renderOverlay(d3.select("#momentumOverlay"), tax.div)
  var updateStemGraphs = tax.update

  colorMap(
  d3.select("#momentumAnnotation"),
  180,
  d3.scaleLinear().domain([0,0.3,0.5,0.7,1,1.01]).range(colorbrewer.Spectral[5].concat(["black"])),
  d3.scaleLinear().domain([0,1.2001]).range([0, 180])
  )

  var up = function (i, alpha, beta) {
             var xy = coor(alpha, beta)
             updatePaths[i](xy[0], xy[1], true)
             updateStemGraphs[i](alpha,beta)
           }

  for (var i = 0; i<5; i++) {
    up(i,defaults[i][0], defaults[i][1])
  }
  </script> 
  <p>

  Notice an immediate boon we get. Momentum allows us to use a crank up the step-size up by a factor of 2 before diverging. But the true magic happens when we find the sweet spot of $\alpha$ and $\beta$.
  </p>
  <h3>The Critical Damping Coefficient</h3>

  <p>
  Let us take a brief detour. When $\alpha$ becomes small (e.g. in problems with pathological curvature), Momentum admits an interesting physical interpretation <dt-cite key="qian1999momentum"></dt-cite>. Consider a physical simulation operating, like in a video game, in discrete time. 
  </p>


  <figure style="width:530px; height:120px; display:block; margin-left:auto; margin-right:auto; position:relative">

  <div style="position:relative; top:-20px">

  <span style="position:absolute; left:40px; top:20px">
  $$y_{i}^{k+1}=\beta y_{i}^{k}+\lambda_{i}x_{i}^{k}$$
  </span>

  <figcaption style="position:absolute;left:270px;top: 37px;width:300px;">
  Interpreit $y_i^k$ as <b>velocity</b>, which is dampened at each step, and perturbed by an external force field.

  </figcaption>

  <span style="position:absolute;left: 40px;top: 75px;">
  $$x_i^{k+1} = x_i^k - \alpha y_i^{k+1}$$
  </span>

  <figcaption style="position:absolute;left: 270px;top: 92px;width:300px;">
  Interpreit $x^k_i$ as the <b>position</b> of a particle in 1D at time $k$, now updated by a small step in velocity $y$.
  </figcaption>

  </div>
  <svg id="position_velocity" style="width:530px; height:180px; position:absolute; top:-25px; left:0px"></svg>

  </figure>
  <script>
  // Swoopy Annotator
  var annotations = [
  {
    "x": 0,
    "y": 0,
    "path": "M 22,27 A 18.336 18.336 0 0 0 34,58",
    "text": "velocity (new)",
    "textOffset": [
      6,
      20
    ]
  },
  {
    "x": 0,
    "y": 0,
    "path": "M 119,25 A 24.331 24.331 0 0 0 124,48",
    "text": "dampening coefficient",
    "textOffset": [
      103,
      20
    ]
  },
  {
    "x": 0,
    "y": 0,
    "path": "M 257,31 A 35.485 35.485 0 0 1 229,60",
    "text": "external force field depending on x",
    "textOffset": [
      243,
      21
    ]
  },
  {
    "x": 0,
    "y": 0,
    "path": "M 20,146 A 25.871 25.871 0 0 1 36,120",
    "text": "position (new)",
    "textOffset": [
      4,
      159
    ]
  }
]
  var swoopy = d3.swoopyDrag()
    .x(function(d){ return (d.x) })
    .y(function(d){ return (d.y) })
      .draggable(false)
      .annotations(annotations)

  var svg = d3.select("#position_velocity")
  var swoopySel = svg.append("g").attr("class", "figtext").call(swoopy)

  </script> 
  <p>
  Since $\alpha$ is small, we can think of $x_i$ as position of a particle (in 1D), and $y_i$ as its velocity (since at each iteration, we move in a small step along $y$). The update on $y_i$, our velocity, is

  $$y_{i}^{k+1}=\beta y_{i}^{k}+\lambda_{i}x_{i}^{k}.$$

  When $\beta=1$ and $\lambda_i = 0$, the update rule looks like

  $$y_i^{k+1} = y_i^k$$

  This is Netwon's First Law - the particle's velocity remains constant; An object in motion stays in motion. Introduce $\beta < 1$ and the equation becomes

  $$y_i^{k+1} = \beta y_i^k.$$

  The particle decelerates. $\beta$ can be thought of as a damping coefficient - the amount of kinetic energy the particle loses at each tick (perhaps due to friction). Finally, the velocity is modified by $\lambda_{i}x_{i}^{k}$ - an external force field. This external force varies in proportion to the particle's position, $x^k_i$, exerting no force when $x^k_i = 0$. This should remind you of something familiar. The damped harmonic oscillator!
  </p>

  <p>
  If you don't recall your high school physics, the damped harmonic oscillator is best imagined as a weight suspended on a spring. We pull the weight down by one unit, and we study the path it goes as it returns to equilibrium. In the analogy, the spring is the source of our external force $\lambda_ix^k_i$, and equilibrium is the state when both $x^k_i$ and $y^k_i$ are 0. We can understand this visually in a phase portrait.
  </p>
  <figure id = "phasediagram" style="position:relative; width:648px; height:490px; margin-left: auto; margin-right: auto"></figure>
  </div>
  <script>
  phaseDiagram(d3.select("#phasediagram"))
  </script>
  <p>

  </p>
  <p>
  The critical $\beta = (1 - \sqrt{\alpha \lambda_i})^2$ gives us a convergence rate (in eigenspace $i$) of $1 - \sqrt{\alpha\lambda_i}.$ A square root improvement over gradient descent, $1-\alpha\lambda_i$! This is our first quadratic speedup. Alas, this only applies to the error in the $i^{th}$ eigenspace.
  </p>

  <p>
  To get a global convergence rate, we must optimize over both $\alpha$ and $\beta$. This is a more complicated affair, <dt-fn> and involves an optimization problem over
  $$
  \min_{\alpha,\beta}\max\left\{ \bigg\| \! \left(\begin{array}{cc}
  \beta & \lambda_{i}\\
  -\alpha\beta & 1-\lambda_{i}
  \end{array}\right) \! \bigg\|,\ldots,\bigg\| \! \left(\begin{array}{cc}
  \beta & \lambda_{n}\\
  -\alpha\beta & 1-\lambda_{n}
  \end{array}\right)\! \bigg\|\right\}.
  $$
  ($\|\cdot \|$ here denotes the magnitude of the maximum eigenvalue). The optimum can be found with nothing more than high school algebra - and it occurs when the roots of the characteristic polynomial are repeated for the matrices corresponding to the extremal eigenvalues. </dt-fn> which I will for you as an exercise. Let's take it as a given that the critical point is
  $$
  \alpha = \left(\frac{2}{\sqrt{\lambda_{1}}+\sqrt{\lambda_{n}}}\right)^{2}  \quad \beta = \left(\frac{\sqrt{\lambda_{n}}-\sqrt{\lambda_{1}}}{\sqrt{\lambda_{n}}+\sqrt{\lambda_{1}}}\right)^{2}
  $$
  Plug this in, and you get
  </p>
  <figure style="width:530px; height:19px; display:block; margin-left:auto; margin-right:auto; position:relative">
  <div style="position:relative; top:-40px">

  <span style="position:absolute; left:16px">
  $$\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}$$
  </span>

  <figcaption style="position:absolute;left: 116px;top: 29px;width:130px;">
  Convergence rate, <b>Gradient Descent</b>
  </figcaption>

  <span style="position:absolute; left:313px">
  $$ \frac{\kappa-1}{\kappa+1}$$
  </span>


  <figcaption style="position:absolute;left: 393px;top: 29px;width:150px;">
  Convergence rate, <b>Momentum</b>
  </figcaption>
  </div>
  </figure> 
  <p> 
  With barely a modicum of extra effort, we have essentially square rooted the condition number! These gains, in principle, require explicit knowledge of $\lambda_1$ and $\lambda_n$. But the formulas reveal a simple guideline. For ill conditioned problems, set $\beta$ as close to $1$ as you can, and make the $\alpha$ as high as possible. Being at the knife's edge of divergence, like in gradient descent, is a good place to be.
  </p>

  <figure style="position:relative; width:920px; height:460px">
  <figcaption style="position:absolute; text-align:left; top:30px; left:140px; width:280px; height:120px">We can do the same decomposition here with momentum, with eigenvalues <svg style="position:relative; top:2px; width:3px; height:14px; background:#fde0dd"></svg> $\lambda_1=1$, <svg style="position:relative; top:2px; width:3px; height:14px; background:#fa9fb5"></svg> $\lambda_2=10$, and <svg style="position:relative; top:2px; width:3px; height:14px; background:#c51b8a"></svg>. Though the decrease is no longer monotonic, but significantly faster. </figcaption>
  <figcaption style="position:absolute; text-align:left; top:210px; left:5px; width:280px; height:120px; font-size:15px"> $f(w) - f(w^\star)$</figcaption>
  <figcaption style="position:absolute; text-align:left; top:430px; left:120px; width:780px; height:120px">The time to convergence is not proportional to the convergence rate due to a small multiplicative constant.</figcaption>

  <div class="figtext" style="position:absolute; left:705px; top:11px">Step size α = </div>
  <div class="figtext" style="position:absolute; left:464px; top:30px">Momentum β = </div>

  <div id = "sliderStep2D" style="position:absolute; left:550px; width:250px; height:100px; top:10px"></div>
  <div id = "milestonesMomentum"></div>  
  </figure>
  <script>
  (function() {var graphDiv = d3.select("#milestonesMomentum")
                   .style("width",  920 + "px")
                   .style("height", 300 + "px")
                   .style("top", "170px")
                   .style("position", "relative")
                   .style("margin-left", "auto")
                   .style("margin-right", "auto")
                   .attr("width", 920)
                   .attr("height", 500)
  
  var svg = graphDiv.append("svg").attr("width", 940).attr("height", 500)

  var update = renderMilestones(svg, function() {})

  var slidera = slider2D(d3.select("#sliderStep2D"), function(x,y) { update(x,y) })

  // Swoopy Annotator
  var annotations = [
    {
      "x": 0,
      "y": 0,
      "path": "M 250,96 A 32.227 32.227 0 0 0 206,62",
      "text": "Optimal parameters",
      "textOffset": [
        188,
        109
      ]
    }
  ]

  var swoopy = d3.swoopyDrag()
    .x(function(d){ return (d.x) })
    .y(function(d){ return (d.y) })
      .draggable(false)
      .annotations(annotations)

  var svg = d3.select("#sliderStep2D").selectAll("svg")
  var swoopySel = svg.append("g").attr("class", "figtext").call(swoopy)

  svg.append('marker')
      .attr('id', 'arrow')
      .attr('viewBox', '-10 -10 20 20')
      .attr('markerWidth', 20)
      .attr('markerHeight', 20)
      .attr('orient', 'auto')
    .append('path')
      .attr('d', 'M-6.75,-6.75 L 0,0 L -6.75,6.75')
      .attr("transform", "scale(0.5)")

  swoopySel.selectAll('path').attr('marker-end', 'url(#arrow)')

  }

  )()

  </script>
  <p>
  While loss function of gradient descent had a graceful, monotonic curve, optimization with momentum displays clear oscillations. These ripples are not restricted to quadratics, and occur in all kinds of functions in practice. They are not cause for alarm, but are an indication that extra tuning of the hyperparameters are required.
  </p>

<hr>
  <h2>
  The Resisting Oracle
  </h2>
  <p>
  If a single auxiliary sequence provides a quadratic speedup, what would two sequences give? Could one perhaps choose the alphas and betas intelligently and adaptively to do even better? It is tempting to ride this wave of optimism - to the cube root and beyond! Now, improvements to the momentum algorithm do exist - but they all run up to a certain, critical, almost inescapable lower bound.
  </p>
  <h3>Adventures in Algorthmic Space</h3>
  <p>
  To understand the limits of what we can do, we must first formally define the algorithmic space in which we are searching. Here's one possible definition. The observation we will make is that both Gradient Descent and momentum can be "unrolled". Indeed, since
  $$
  \begin{array}{lll}
    w^{1} & \!= & \!w^{0} ~-~ \alpha\nabla f(w^{0})\\[0.35em]
    w^{2} & \!= & \!w^{1} ~-~ \alpha\nabla f(w^{1})\\[0.35em]
          & \!= & \!w^{0} ~-~ \alpha\nabla f(w^{0}) ~-~ \alpha\nabla f(w^{1})\\[0.35em]
     & ~ \!\vdots \\

   w^{k} & \!= & \!w^{0} ~-~ \alpha\nabla f(w^{0}) ~-~~~~ \cdots\cdots ~~~~-~ \alpha\nabla f(w^{k})
  \end{array}
  $$
  we can write gradient descent as:

  $$
  w^{k} ~~=~~ w^{0} ~-~ \alpha\sum_{i}\nabla f(w^{i})
  $$

  A similar trick can be done with momentum,

  $$
  w^{k} ~~=~~ w^{0} ~+~ \alpha\sum\frac{(1-\beta^{k})}{1-\beta}\nabla f(w^{k})
  $$

  In fact, all manner of first order algorithms, including the Conjugate Gradient algorithm, AdaMax, Averaged Gradient and more can be written in (though not quite so neatly) this unrolled form. Therefore the class of algorithms for which

  $$
  w^{k} ~~=~~ w^{0} ~+~ \sum_{i}^{k}\gamma_{i}^{k}\nabla f(w^{i}) \qquad \text{ for some } \gamma_{i}^{k}
  $$

  contains momentum, gradient descent and a whole bunch of other iterations you might dream up. This is what is assumed in Assumption 2.1.4 <dt-cite key="nesterov2013introductory"></dt-cite> of Nesterov. But let's push this even further, and expand this class to allow different step sizes for different directions.

  $$
  w^{k} ~~=~~ w^{0} ~+~ \sum_{i}^{k}\Gamma_{i}^{k}\nabla f(w^{i}) \qquad \text{ for some diagonal matrix } \Gamma_{i}^{k} .
  $$

  This class of methods has now covers most of the popular algorithms for training neural networks, including ADAM and Adagrad. We shall refer to this class of methods as "Linear First Order Methods". Now comes the Achille's heel of all methods of this kind.

  </p>
  <h3>The Worst Function in the World</h3>
  <p>
  Here it is.

  $$
  f^n(w) ~~=~~ \frac{(\kappa-1)}{8}\left(\left(w_{1}-1\right)^{2}+\sum_{i=1}^{n}(w_{i}-w_{i+1})^{2}\right)~+~\frac{1}{2}\|w\|^{2}
  $$

  The function looks rather innocent. It is a quadratic. It is simple to express. And it is surprisingly explicable. In fact, we've seen it once before already. This is just a variation on the colorization problem, one with a small regularization term which determines how well behaved it is. It contains strong couplings between the variables (proportional to $\kappa$), and a long path from $w_1$ to $w_n$ - as discussed earlier, a sign of a trench of bad curvature. Let's write out some facts about this function.
  </p>

  <p>
  The optimal solution of this problem is

  $$
  w_{i}^{\star}=\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^{i}
  $$

  and the condtion number of the problem $f^n$ approaches $\kappa$ as $n$ goes to infinity. Now observe the behavior of the momentum algorithm on this function, starting from $w^0 = 0$.
  </p>

  <figure style="width:950px; height:770px">
    <div id = "sliderz" style="position:absolute;top: 14px;left:575px;"></div>    
    <div class="figtext" style="position:absolute; pointer-events:none; top:14px; width:300px; left:729px; height:100px">Step size α = </div>
    <div class="figtext" style="position:absolute; pointer-events:none; top:35px; width:488px; left:488px; height:100px">Momentum β = </div>

    <figcaption style="width:270px; position:absolute; left:180px; top:50px">Here we see the first 70 iterates of momentum on the Convex Rosenbrock for $n=25$. The behavior here is similar to that of any Linear First Order Algorithm. </figcaption>

    <div style="position:absolute; top:150px; left:-80px">
    <figcaption style="width:120px; position:absolute; left:670px; top:90px">Notice that this triangle is a "dead zone" of our iterates. The iterates are always 0, no matter what the parameters. </figcaption>
    <figcaption style="width:120px; position:absolute; left:670px; top:360px">The remaining expanding space is the "light cone" of our iterate's influence. Momentum does very well here with the optimal parameters. </figcaption>
    <div id = "iterates" style="position:absolute; top:60px; left:0"></div>
    <div class="figtext" style="position:absolute; top:5px; width:300px; left:460px; height:100px">Error</div>
    <div id = "rosen_colorbar1" style="position:absolute; top:5px; width:300px; left:448px; height:100px"></div>
    <div class="figtext" style="position:absolute; top:5px; width:300px; left:240px; height:100px">Weights</div>
    <div id = "rosen_colorbar2" style="position:absolute; top:5px; width:300px; left:228px; height:100px"></div>
    </div>
  </figure>
  <script>

  (function() { var RQ = [[0.033430859446525574,0.06637421995401382,0.09834969788789749,0.1288910210132599,0.1575528085231781,0.18391713500022888,0.20759953558444977,0.22825466096401215,0.24558131396770477,0.25932684540748596,0.2692908048629761,0.2753278911113739,0.2773500978946686,0.2753278911113739,0.2692908048629761,0.25932684540748596,0.24558131396770477,0.22825466096401215,0.20759953558444977,0.18391713500022888,0.1575528085231781,0.1288910210132599,0.09834969788789749,0.06637421995401382,0.033430859446525574],[-0.06637421995401382,-0.1288910210132599,-0.18391713500022888,-0.22825466096401215,-0.25932684540748596,-0.2753278911113739,-0.2753278911113739,-0.25932684540748596,-0.22825466096401215,-0.18391713500022888,-0.1288910210132599,-0.06637421995401382,-1.917815576756455e-14,0.06637421995401382,0.1288910210132599,0.18391713500022888,0.22825466096401215,0.25932684540748596,0.2753278911113739,0.2753278911113739,0.25932684540748596,0.22825466096401215,0.18391713500022888,0.1288910210132599,0.06637421995401382],[0.09834969788789749,0.18391713500022888,0.24558131396770477,0.2753278911113739,0.2692908048629761,0.22825466096401215,0.1575528085231781,0.06637421995401382,-0.033430859446525574,-0.1288910210132599,-0.20759953558444977,-0.25932684540748596,-0.2773500978946686,-0.25932684540748596,-0.20759953558444977,-0.1288910210132599,-0.033430859446525574,0.06637421995401382,0.1575528085231781,0.22825466096401215,0.2692908048629761,0.2753278911113739,0.24558131396770477,0.18391713500022888,0.09834969788789749],[0.1288910210132599,0.22825466096401215,0.2753278911113739,0.25932684540748596,0.18391713500022888,0.06637421995401382,-0.06637421995401382,-0.18391713500022888,-0.25932684540748596,-0.2753278911113739,-0.22825466096401215,-0.1288910210132599,-1.2834394835694717e-14,0.1288910210132599,0.22825466096401215,0.2753278911113739,0.25932684540748596,0.18391713500022888,0.06637421995401382,-0.06637421995401382,-0.18391713500022888,-0.25932684540748596,-0.2753278911113739,-0.22825466096401215,-0.1288910210132599],[-0.1575528085231781,-0.25932684540748596,-0.2692908048629761,-0.18391713500022888,-0.033430859446525574,0.1288910210132599,0.24558131396770477,0.2753278911113739,0.20759953558444977,0.06637421995401382,-0.09834969788789749,-0.22825466096401215,-0.2773500978946686,-0.22825466096401215,-0.09834969788789749,0.06637421995401382,0.20759953558444977,0.2753278911113739,0.24558131396770477,0.1288910210132599,-0.033430859446525574,-0.18391713500022888,-0.2692908048629761,-0.25932684540748596,-0.1575528085231781],[-0.18391713500022888,-0.2753278911113739,-0.22825466096401215,-0.06637421995401382,0.1288910210132599,0.25932684540748596,0.25932684540748596,0.1288910210132599,-0.06637421995401382,-0.22825466096401215,-0.2753278911113739,-0.18391713500022888,-4.583645525716709e-15,0.18391713500022888,0.2753278911113739,0.22825466096401215,0.06637421995401382,-0.1288910210132599,-0.25932684540748596,-0.25932684540748596,-0.1288910210132599,0.06637421995401382,0.22825466096401215,0.2753278911113739,0.18391713500022888],[-0.20759953558444977,-0.2753278911113739,-0.1575528085231781,0.06637421995401382,0.24558131396770477,0.25932684540748596,0.09834969788789749,-0.1288910210132599,-0.2692908048629761,-0.22825466096401215,-0.033430859446525574,0.18391713500022888,0.2773500978946686,0.18391713500022888,-0.033430859446525574,-0.22825466096401215,-0.2692908048629761,-0.1288910210132599,0.09834969788789749,0.25932684540748596,0.24558131396770477,0.06637421995401382,-0.1575528085231781,-0.2753278911113739,-0.20759953558444977],[-0.22825466096401215,-0.25932684540748596,-0.06637421995401382,0.18391713500022888,0.2753278911113739,0.1288910210132599,-0.1288910210132599,-0.2753278911113739,-0.18391713500022888,0.06637421995401382,0.25932684540748596,0.22825466096401215,4.337912797388764e-15,-0.22825466096401215,-0.25932684540748596,-0.06637421995401382,0.18391713500022888,0.2753278911113739,0.1288910210132599,-0.1288910210132599,-0.2753278911113739,-0.18391713500022888,0.06637421995401382,0.25932684540748596,0.22825466096401215],[-0.24558131396770477,-0.22825466096401215,0.033430859446525574,0.25932684540748596,0.20759953558444977,-0.06637421995401382,-0.2692908048629761,-0.18391713500022888,0.09834969788789749,0.2753278911113739,0.1575528085231781,-0.1288910210132599,-0.2773500978946686,-0.1288910210132599,0.1575528085231781,0.2753278911113739,0.09834969788789749,-0.18391713500022888,-0.2692908048629761,-0.06637421995401382,0.20759953558444977,0.25932684540748596,0.033430859446525574,-0.22825466096401215,-0.24558131396770477],[-0.25932684540748596,-0.18391713500022888,0.1288910210132599,0.2753278911113739,0.06637421995401382,-0.22825466096401215,-0.22825466096401215,0.06637421995401382,0.2753278911113739,0.1288910210132599,-0.18391713500022888,-0.25932684540748596,-2.6265288907570752e-15,0.25932684540748596,0.18391713500022888,-0.1288910210132599,-0.2753278911113739,-0.06637421995401382,0.22825466096401215,0.22825466096401215,-0.06637421995401382,-0.2753278911113739,-0.1288910210132599,0.18391713500022888,0.25932684540748596],[0.2692908048629761,0.1288910210132599,-0.20759953558444977,-0.22825466096401215,0.09834969788789749,0.2753278911113739,0.033430859446525574,-0.25932684540748596,-0.1575528085231781,0.18391713500022888,0.24558131396770477,-0.06637421995401382,-0.2773500978946686,-0.06637421995401382,0.24558131396770477,0.18391713500022888,-0.1575528085231781,-0.25932684540748596,0.033430859446525574,0.2753278911113739,0.09834969788789749,-0.22825466096401215,-0.20759953558444977,0.1288910210132599,0.2692908048629761],[0.2753278911113739,0.06637421995401382,-0.25932684540748596,-0.1288910210132599,0.22825466096401215,0.18391713500022888,-0.18391713500022888,-0.22825466096401215,0.1288910210132599,0.25932684540748596,-0.06637421995401382,-0.2753278911113739,-2.224606886845463e-15,0.2753278911113739,0.06637421995401382,-0.25932684540748596,-0.1288910210132599,0.22825466096401215,0.18391713500022888,-0.18391713500022888,-0.22825466096401215,0.1288910210132599,0.25932684540748596,-0.06637421995401382,-0.2753278911113739],[0.2773500978946686,1.2625047254649028e-16,-0.2773500978946686,-6.312523627324514e-16,0.2773500978946686,8.83753307825432e-16,-0.2773500978946686,-8.83753307825432e-16,0.2773500978946686,1.5781309068311285e-15,-0.2773500978946686,-4.734392720493385e-16,0.2773500978946686,2.0200075607438445e-15,-0.2773500978946686,-1.7990691808479273e-15,0.2773500978946686,1.6728188141805554e-15,-0.2773500978946686,-1.4518804342846382e-15,0.2773500978946686,-3.4718879950284827e-16,-0.2773500978946686,1.2625047254649028e-15,0.2773500978946686],[-0.2753278911113739,0.06637421995401382,0.25932684540748596,-0.1288910210132599,-0.22825466096401215,0.18391713500022888,0.18391713500022888,-0.22825466096401215,-0.1288910210132599,0.25932684540748596,0.06637421995401382,-0.2753278911113739,-1.0026396759284933e-15,0.2753278911113739,-0.06637421995401382,-0.25932684540748596,0.1288910210132599,0.22825466096401215,-0.18391713500022888,-0.18391713500022888,0.22825466096401215,0.1288910210132599,-0.25932684540748596,-0.06637421995401382,0.2753278911113739],[0.2692908048629761,-0.1288910210132599,-0.20759953558444977,0.22825466096401215,0.09834969788789749,-0.2753278911113739,0.033430859446525574,0.25932684540748596,-0.1575528085231781,-0.18391713500022888,0.24558131396770477,0.06637421995401382,-0.2773500978946686,0.06637421995401382,0.24558131396770477,-0.18391713500022888,-0.1575528085231781,0.25932684540748596,0.033430859446525574,-0.2753278911113739,0.09834969788789749,0.22825466096401215,-0.20759953558444977,-0.1288910210132599,0.2692908048629761],[-0.25932684540748596,0.18391713500022888,0.1288910210132599,-0.2753278911113739,0.06637421995401382,0.22825466096401215,-0.22825466096401215,-0.06637421995401382,0.2753278911113739,-0.1288910210132599,-0.18391713500022888,0.25932684540748596,1.770693656524247e-16,-0.25932684540748596,0.18391713500022888,0.1288910210132599,-0.2753278911113739,0.06637421995401382,0.22825466096401215,-0.22825466096401215,-0.06637421995401382,0.2753278911113739,-0.1288910210132599,-0.18391713500022888,0.25932684540748596],[0.24558131396770477,-0.22825466096401215,-0.033430859446525574,0.25932684540748596,-0.20759953558444977,-0.06637421995401382,0.2692908048629761,-0.18391713500022888,-0.09834969788789749,0.2753278911113739,-0.1575528085231781,-0.1288910210132599,0.2773500978946686,-0.1288910210132599,-0.1575528085231781,0.2753278911113739,-0.09834969788789749,-0.18391713500022888,0.2692908048629761,-0.06637421995401382,-0.20759953558444977,0.25932684540748596,-0.033430859446525574,-0.22825466096401215,0.24558131396770477],[0.22825466096401215,-0.25932684540748596,0.06637421995401382,0.18391713500022888,-0.2753278911113739,0.1288910210132599,0.1288910210132599,-0.2753278911113739,0.18391713500022888,0.06637421995401382,-0.25932684540748596,0.22825466096401215,9.870699398085384e-16,-0.22825466096401215,0.25932684540748596,-0.06637421995401382,-0.18391713500022888,0.2753278911113739,-0.1288910210132599,-0.1288910210132599,0.2753278911113739,-0.18391713500022888,-0.06637421995401382,0.25932684540748596,-0.22825466096401215],[-0.20759953558444977,0.2753278911113739,-0.1575528085231781,-0.06637421995401382,0.24558131396770477,-0.25932684540748596,0.09834969788789749,0.1288910210132599,-0.2692908048629761,0.22825466096401215,-0.033430859446525574,-0.18391713500022888,0.2773500978946686,-0.18391713500022888,-0.033430859446525574,0.22825466096401215,-0.2692908048629761,0.1288910210132599,0.09834969788789749,-0.25932684540748596,0.24558131396770477,-0.06637421995401382,-0.1575528085231781,0.2753278911113739,-0.20759953558444977],[-0.18391713500022888,0.2753278911113739,-0.22825466096401215,0.06637421995401382,0.1288910210132599,-0.25932684540748596,0.25932684540748596,-0.1288910210132599,-0.06637421995401382,0.22825466096401215,-0.2753278911113739,0.18391713500022888,8.371955066048889e-16,-0.18391713500022888,0.2753278911113739,-0.22825466096401215,0.06637421995401382,0.1288910210132599,-0.25932684540748596,0.25932684540748596,-0.1288910210132599,-0.06637421995401382,0.22825466096401215,-0.2753278911113739,0.18391713500022888],[-0.1575528085231781,0.25932684540748596,-0.2692908048629761,0.18391713500022888,-0.033430859446525574,-0.1288910210132599,0.24558131396770477,-0.2753278911113739,0.20759953558444977,-0.06637421995401382,-0.09834969788789749,0.22825466096401215,-0.2773500978946686,0.22825466096401215,-0.09834969788789749,-0.06637421995401382,0.20759953558444977,-0.2753278911113739,0.24558131396770477,-0.1288910210132599,-0.033430859446525574,0.18391713500022888,-0.2692908048629761,0.25932684540748596,-0.1575528085231781],[-0.1288910210132599,0.22825466096401215,-0.2753278911113739,0.25932684540748596,-0.18391713500022888,0.06637421995401382,0.06637421995401382,-0.18391713500022888,0.25932684540748596,-0.2753278911113739,0.22825466096401215,-0.1288910210132599,-3.0802549003271684e-16,0.1288910210132599,-0.22825466096401215,0.2753278911113739,-0.25932684540748596,0.18391713500022888,-0.06637421995401382,-0.06637421995401382,0.18391713500022888,-0.25932684540748596,0.2753278911113739,-0.22825466096401215,0.1288910210132599],[-0.09834969788789749,0.18391713500022888,-0.24558131396770477,0.2753278911113739,-0.2692908048629761,0.22825466096401215,-0.1575528085231781,0.06637421995401382,0.033430859446525574,-0.1288910210132599,0.20759953558444977,-0.25932684540748596,0.2773500978946686,-0.25932684540748596,0.20759953558444977,-0.1288910210132599,0.033430859446525574,0.06637421995401382,-0.1575528085231781,0.22825466096401215,-0.2692908048629761,0.2753278911113739,-0.24558131396770477,0.18391713500022888,-0.09834969788789749],[-0.06637421995401382,0.1288910210132599,-0.18391713500022888,0.22825466096401215,-0.25932684540748596,0.2753278911113739,-0.2753278911113739,0.25932684540748596,-0.22825466096401215,0.18391713500022888,-0.1288910210132599,0.06637421995401382,6.798085926082532e-17,-0.06637421995401382,0.1288910210132599,-0.18391713500022888,0.22825466096401215,-0.25932684540748596,0.2753278911113739,-0.2753278911113739,0.25932684540748596,-0.22825466096401215,0.18391713500022888,-0.1288910210132599,0.06637421995401382],[0.033430859446525574,-0.06637421995401382,0.09834969788789749,-0.1288910210132599,0.1575528085231781,-0.18391713500022888,0.20759953558444977,-0.22825466096401215,0.24558131396770477,-0.25932684540748596,0.2692908048629761,-0.2753278911113739,0.2773500978946686,-0.2753278911113739,0.2692908048629761,-0.25932684540748596,0.24558131396770477,-0.22825466096401215,0.20759953558444977,-0.18391713500022888,0.1575528085231781,-0.1288910210132599,0.09834969788789749,-0.06637421995401382,0.033430859446525574]]

  RLambda = [4.6419172286987305,15.514562606811523,33.45938491821289,58.21471405029297,89.4195556640625,126.61888122558594,169.27023315429688,216.7516632080078,268.37078857421875,323.3748474121094,380.9618225097656,440.29193115234375,500.5,560.7080688476562,620.0381469726562,677.6251220703125,732.6292114257812,784.2483520507812,831.7297973632812,874.381103515625,911.5804443359375,942.7852783203125,967.5405883789062,985.4854125976562,996.3580932617188]

  var alpha = 0.003
  var beta = 0.8
  var b = zeros(25); b[0] =  249.75
  var iter = geniterMomentum(RQ, RLambda, b, alpha, beta) 
  var res = function(i) { return iter(i)[0] }

  var sampleSVG = d3.select("#iterates")
    .style("display", "block")
      .append("svg")
      .attr("width", 770)
      .attr("height", 700)   

  var rosen = d3.scaleLinear().domain([0,0.2,0.5,1.4,5]).range(colorbrewer.RdPu[5]);
  var jet = d3.scaleLinear().domain([-0.5,0,0.2,0.5,1.0,5]).range(colorbrewer.Spectral[6]);
  var contrast = d3.scaleLinear().domain([-12,0,12]).range(colorbrewer.RdBu[3]);

  var xstar = iter(10000000)
  var numIters = 72


  var Disps3 = []
  var errorPlot = sampleSVG.append("g").attr("transform", "translate(" + 440 + ",0)")
  for (var j = 0; j < numIters; j++) {
    var disp = errorPlot.append("g")

    var denter = disp.selectAll("rect")
        .data(iter(j)[1])
        .enter()

    denter.append("rect")
        .style("fill", function(d,i) { return rosen(d) })
        .attr("height", 7)
        .attr("width", 7.7)
        .attr("x", function(d, i){return i*8+ 10})
        .attr("y", function(d, i){return j*8 })  
        // .attr("stroke", "black")
        // .attr("stroke-width", function(d,i) { return (i == j) ? 1.5 : 0 })
        // .attr("stroke-dasharray", "14.8, 14")

    // if ((j % 10 == 0) || j == 0) {
    // disp.append("text")
    //     .attr("class", "figtext")
    //     .attr("text-anchor", "end")
    //     .attr("x", 0)
    //     .attr("y", function(d, i){return (j*8 + 10) })  
    //     .html( (j == 0) ? "Iteration 0" : "" + j) 
    // }

    Disps3.push(disp)
  } 



  var Disps4 = []
  var errorPlot = sampleSVG.append("g").attr("transform", "translate(" + 220 + ",0)")
  for (var j = 0; j < numIters; j++) {
    var disp = errorPlot.append("g")

    var denter = disp.selectAll("rect")
        .data(iter(j)[1])
        .enter()

    denter.append("rect")
        .style("fill", function(d,i) { return jet(d) })
        .attr("height", 7)
        .attr("width", 7.7)
        .attr("x", function(d, i){return i*8+ 10})
        .attr("y", function(d, i){return j*8 })  
        // .attr("stroke", "black")
        // .attr("stroke-width", function(d,i) { return (i == j) ? 1.5 : 0 })
        // .attr("stroke-dasharray", "14.8, 14")

    if ((j % 10 == 0) || j == 0) {
    disp.append("text")
        .attr("class", "figtext")
        .attr("text-anchor", "end")
        .attr("x", 0)
        .attr("y", function(d, i){return (j*8 + 10) })  
        .html( (j == 0) ? "Iteration 0" : "" + j) 
    }

    Disps4.push(disp)
  } 
  function update(alpha, beta) {
    var iter = geniterMomentum(RQ, RLambda, b, alpha, beta) 
    for (var j = 0; j < numIters; j++) {
      var iterj = iter(j)
      Disps3[j].selectAll("rect").data(iterj[1]).merge(Disps3[j]).style("fill", function(d,i) { return rosen(Math.abs(d - xstar[1][i]) ) })
      Disps4[j].selectAll("rect").data(iterj[1]).merge(Disps4[j]).style("fill", function(d,i) { return jet(d) })

    }
  }

  var slidera = slider2D(d3.select("#sliderz"), function(x,y) { update(x/RLambda[RLambda.length - 1],y) })

  update(0.0035,0.95)
  
  colorMap( d3.select("#rosen_colorbar1"),
            180,
            rosen,
            d3.scaleLinear().domain([0,1.5]).range([0, 180]) )

  colorMap( d3.select("#rosen_colorbar2"),
            180,
            jet,
            d3.scaleLinear().domain([0.2,1.5]).range([0, 180]) )})()

  </script> 
  <p>
  The observations made in the above diagram are true for any Linear First Order algorithm. Let us prove this. First observe that each component of the gradient depends only on the values directly before and after it:

  $$
\nabla f(x)_{i}=\frac{\kappa-1}{4}(2w_{i}-w_{i-1}-w_{i+1})+w_{i}, \qquad i \neq 1.
  $$

  Therefore the fact we start at 0 guarantees that that component must remain stoically there till an element either before and after it turns nonzero. And therefore, by induction, for any linear first order algorithm,
   
  $$
  \begin{array}{lllllllll}
    w^{0} & = & [~~0, & 0, & 0, & \ldots & 0, & 0, & \ldots & 0~]\\[0.35em]
    w^{1} & = & [~w_{1}^{1}, & 0, & 0, & \ldots & 0, & 0, & \ldots & 0~]\\[0.35em]
    w^{2} & = & [~w_{1}^{2}, & w_{2}^{2}, & 0, & \ldots & 0, & 0, & \ldots & 0~]\\[0.35em]
     & ~ \vdots \\

   w^{k} & = & [~w_{1}^{k}, & w_{2}^{k}, & w_{3}^{k}, & \ldots &  w_{k}^{k}, & 0, & \ldots & 0~]\\
  \end{array}
  $$

  Think of this restriction as a "speed of light" of information transfer. Error signals will take at least $k$ steps to move from $w_0$ can propagate to $w_k$. We can therefore sum up the errors which can not have changed yet

  $$
\begin{aligned}
\|w^{k}-w^{\star}\| & \geq\sum_{i=k+1}^{n}\!\!w_{i}^{\star}\\[1.7em]
 & =\sum_{i=k+1}^{n}\!\!\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^{i}\\[1.7em]
 & =\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^{k}\|w^{k}-w^{0}\|+\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^{n}
\end{aligned}
  $$

  With the final term going to 0 as $n$ gets large. And there we have it - the lower bound, which no linear first order algorithm, not ADAM or AdaGrad or Conjugate Gradients, can do better than.
  </p>
  <p>
  Like many such lower bounds, results like this must not be taken literally, but spiritually. Some of our favourite methods, including BFGS, and more, do not fall into the class of linear first order methods. But it is a surprising and satisfying coincidence that this lower bound is exactly that achieved by gradient descent with momentum.
  </p>

<hr>
  <h2>Onwards and Downwards</h2>
  <p>
  The study of acceleration is seeing a small revival within the optimization community. If the ideas in this article excite you, you may wish to read <dt-cite key="su2014differential"></dt-cite>, which fully explores the idea of momentum as the discretization of a certain differential equation. But other, less physical interpretations exist. There an algebraic interpretation of momentum in terms of approximating polynomials <dt-cite key="rutishauser1959theory,hardtzen"></dt-cite>. Geometric interpretations are emerging <dt-cite key="bubeck2015geometric,drusvyatskiy2016optimal"></dt-cite>, connecting momentum to older methods, like the Ellipsoid method. And finally, there are interpretations relating momentum to duality <dt-cite key="allen2014linear"></dt-cite>, perhaps providing a clue as how to accelerate second order methods and Quasi Newton (for a first step, see <dt-cite key="nesterov2008accelerating"></dt-cite>). But Like the proverbial blind men feeling elephant, momentum seems like something bigger than the sum of its parts. One day, hopefully soon, the many perspectives will unify into a satisfying whole.
  </p>
</dt-article>

<!--   The invention of acceleration, which we date almost 30 years ago, is old enough to be called "classical". But even in this day it is not fully understood. Its intuitions remain tricky  to grasp. Not for a lack of trying. Nemerovski called it an "analytical trick". And in the last 2 years there has been a small boom of papers on acceleration, probing it's deeper connections to physics, quasi-newton, ordinary differential equations, classical algorithms like conjugate gradient, and more. In this blog post, I hope to give you a taste of why momentum works. And how these insights can be brought into everyday practice.
 -->

<!--    The slider visualizes the eigenvalues on a logramtic scale - and the price paid for pathological curvature here is, well, shall I say arbitrarily bad. On a 2014 Macbook Pro, this would take a grand total of 299 years to converge, just in time perhaps for the heat death of the universe. All this work for polynomial regression with 25 variables! Of course, there are far way more numerically stable ways to solve this system (like the QR factorization). But that is not the point of this post. -->

<!--<p>
Geoff Hinton calls this phenomena, in a rather raunchy biological analogy, "coadaptation". I'll leave you to read the details of this paper on your own - but here's the PG-13 version. A deep neural net generates features within its layers. To exploit the full modeling power of the network, they should be in some sense independent - information about a certain feature cannot be derived from information in other features. In linear regression, this independence takes on a stronger meaning. For the problem to be well conditioned, the features need to be linearly independent. And the condition number is precisely a measure of how well these features correlate.
</p>-->

<!--To do this, we diagonalize $R$,  $U^{-1} \Sigma U = R$ (A technical note: When the roots are repeated, diagonalization fails. Let's sweep this under the rug.). And thus

  $$
  R^{k} = (U^{-1}\Sigma U)^{k} =U^{-1}\Sigma^{k}U
  $$

  </p>

  <p>-->


<!--
<hr>
  <h2>
  Thinking Outside the Box
  </h2>
  <p>


  How can this be true? The answer is simple. These are not pure gradient methods. Indeed, by carefully examining the structure of these problems, it is possible to find a basis where, and perform optimization on that space. This is method of preconditioning.


  As we brush up against the limits of first order methods, a whole new vista emerges. For starters, second order methods are plentiful. Newton's Method, the Natural Gradient methods. In a sense, the study of optimization is the study of problem structure. It is the detailed analysis of a problem class, it's symmmetries, its bottlenecks and its statistical', from which new and beautiful algorithms emerge.
  </p>-->

<!--
  <dt-fn>
    The demo above suggests that $\beta$ is optimized when the eigenvalues of $R$,

    $$
  \begin{aligned}
  \sigma_{1} & =\tfrac{1}{2}\big(1+\beta-\alpha\lambda_{i}+\sqrt{(1+\beta+\alpha\lambda_{i})^{2}-4\beta}\big)\\
  \sigma_{2} & =\tfrac{1}{2}\big(1+\beta-\alpha\lambda_{i}-\sqrt{(1+\beta+\alpha\lambda_{i})^{2}-4\beta}\big)
  \end{aligned}
    $$

    are repeated, $\sigma_{1} = \sigma_{2}$. So setting the discriminant to 0, we get, for any choice of stepsize $\alpha$,

    $$
    (1+\beta+\alpha\lambda_i)^{2}-4\beta=0 \iff \beta = 1-\alpha\lambda_i-\sqrt{\alpha\lambda_i}
    $$

    Now we optimize for $\alpha$ the way we did before, to get
    $$
  \begin{aligned}
  \text{ }\underset{\alpha}{\text{argmin}}\{\max_{i}\{1-\sqrt{\alpha_{i}\lambda_{i}}\}\} & =\left(\frac{2}{\sqrt{\lambda_{1}}+\sqrt{\lambda_{n}}}\right)^{2}.\\
  \underset{\alpha}{\text{min}}\{\max_{i}\{1-\sqrt{\alpha_{i}\lambda_{i}}\}\} & =\frac{\sqrt{\lambda_{n}/\lambda_{1}}-1}{\sqrt{\lambda_{n}/\lambda_{1}}+1}
  \end{aligned}
    $$
  </dt-fn>-->

<!--
  $$
  R = \left(\begin{array}{cc}
  \beta & \lambda_{i}\\
  -\alpha\beta & 1-\alpha\lambda_{i}
  \end{array}\right)
  $$

  Recall that the eigenvalues are the roots of the characteristic polynomial

  $$
  \gamma \mapsto\det \left(\begin{array}{cc}
  \beta -\gamma& \lambda_{i}\\
  -\alpha\beta & 1-\alpha\lambda_{i} - \gamma
  \end{array}\right)
  $$-->

<!--     You probably now have in your mind the image of a spring bouncing up and down, with oscillations of decreasing amplitude. But the rate in which these movements decay depends strongly the amount it is damped. Is the spring immersed in water? Or is it in a frictionless vacuum? In our analogy, the damping coefficient is controlled by our momentum parameter, $\beta$. Gradient descent ($\beta = 0$) corresponds to over-damping. There's no oscillation, but the spring is immersed in a viscous fluid which is restricting its movement just a little too much. When $\beta = 1$, we're under-damping. Here's there's no resistance at all, and spring oscillates up and down forever, missing the optimal value over and over. The best value of $\beta$ must lie somewhere in the middle. This sweet spot, the "critical damping rate", happens exactly when the eigenvalues of $R$ are repeated, which happens when (work this out!)
  <dt-fn> Hint : the eigenvalues are $\frac{1}{2} \left(-\sqrt{(-\alpha  \lambda +\beta +1)^2-4 \beta }-\alpha
   \lambda +\beta +1\right)$</dt-fn> -->
<p>
<!-- 
  To understand why these features are meaningful, lets imagine that for some $n$-vector $q$, 

  $$
  q_{1}\bar{p}_1 + \cdots + q_{n}\bar{p}_n =\bar{p}_n(\xi_i) = 0 \qquad \text{for all }i, \qquad \|q\| = 1
  $$

  If such a function were an eigenfeature, then its weight is meaningless. It can be made arbitrarily large or small, and will not affect the fit one iota! This is, so to speak, a blind spot in the data - a manifold of points for which we have no information on. If such a linear combination exists, then $q$ will be an eigenvector of $Q$ with eigenvalue $0$. And even if this extreme does not hold, the eigenvalues $Q$ will effectively partition or features such that each $x_i$ acts independently. And eigenfeatures with large eigenvalues will represent polynomials which vary robustly across the data, and eigenfeautrees with small eigenvalues those which are extremely sensitive to changes in $d$. Lets visualize these below. -->
  <!-- The eigenvectors, $Q$, if you will recall represent a rotation of $w$ space into $x$ space, which has the very special property that gradient descent acts independently on the features there. But $Q$ can also be interpreted as a data preprocessing operator. $Q$ decorrelates the data, since $(ZQ)^T(ZQ)$ is diagonal. Therefore it is seems sensible to visualize this rotation by performing the same transformation on the feature map, giving our data decorrelated features,

  $$
  \xi\mapsto Q[\xi^{0},\xi^{1}\ldots,\xi^{n}].
  $$

  This eigenfeaturemap is visualized below. -->
  </p>

<!-- 
<div id = "poly2"></div>

<div id="eigenFeatures" style="display:block; margin-left:auto; margin-right:auto; width:920px; position:relative; top:20px; height:250px">
  <svg>
  </svg>
</div>

<script>
var control = renderEigenFeatures(d3.select("#poly2"))
renderNN(function(i) {control.eigen(25-i)}, function(i) {control.poly(25-i)})
</script> -->

<dt-appendix class="centered"></dt-appendix>
<script type="text/bibliography">
@article{o2015adaptive,
  title={Adaptive restart for accelerated gradient schemes},
  author={O’Donoghue, Brendan and Candes, Emmanuel},
  journal={Foundations of computational mathematics},
  volume={15},
  number={3},
  pages={715--732},
  year={2015},
  publisher={Springer},
  url={https://arxiv.org/abs/1204.3982},
  doi={10.1007/s10208-013-9150-3}
}

@article{flammarion2015averaging,
  title={From averaging to acceleration, there is only a step-size},
  author={Flammarion, Nicolas and Bach, Francis},
  booktitle={Proceedings of the International Conference on Learning Theory (COLT)},
  year={2015},
  url={https://arxiv.org/abs/1504.01577}
}

@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015},
  url={https://arxiv.org/abs/1502.03167}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011},
  url={http://jmlr.org/papers/v12/duchi11a.html}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014},
  url={https://arxiv.org/abs/1412.6980}
}

@book{briggs2000multigrid,
  title={A multigrid tutorial},
  author={Briggs, William L and Henson, Van Emden and McCormick, Steve F},
  year={2000},
  publisher={SIAM},
  doi={10.1137/1.9780898719505}
}

@article{su2014differential,
  title={A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights},
  author={Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2510--2518},
  year={2014},
  url={https://arxiv.org/abs/1503.01243}
}

@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier},
  url={https://www.researchgate.net/profile/Boris_Polyak2/publication/243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods/links/5666fa3808ae34c89a01fda1.pdf},
  doi={10.1016/0041-5553(64)90137-5}
}

@article{flammarion2015averaging,
  title={From Averaging to Acceleration, There is Only a Step-size.},
  author={Flammarion, Nicolas and Bach, Francis R},
  booktitle={COLT},
  pages={658--695},
  year={2015},
  url={https://arxiv.org/abs/1504.01577}
}

@article{williamsnthpower,
  title={The Nth Power of a 2x2 Matrix.},
  author={Williams, Kenneth},
  journal={Mathematics Magazine},
  volume={65},
  number={5},
  pages={336},
  year={1992},
  publisher={MAA},
  url={http://people.math.carleton.ca/~williams/papers/pdf/175.pdf},
  doi={10.2307/2691246}
}

@article{hardtzen,
  title={The Zen of Gradient Descent},
  author={Hardt, Moritz},
  year={2013},
  url={http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html}
}

@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media},
  doi={10.1007/978-1-4419-8853-9}
}

@article{rutishauser1959theory,
  title={Theory of gradient methods},
  author={Rutishauser, Heinz},
  booktitle={Refined iterative methods for computation of the solution and the eigenvalues of self-adjoint boundary value problems},
  pages={24--49},
  year={1959},
  publisher={Springer},
  doi={10.1007/978-3-0348-7224-9_2}
}

@article{bubeck2015geometric,
  title={A geometric alternative to Nesterov's accelerated gradient descent},
  author={Bubeck, S{\'e}bastien and Lee, Yin Tat and Singh, Mohit},
  journal={arXiv preprint arXiv:1506.08187},
  year={2015},
  url={https://arxiv.org/pdf/1506.08187.pdf}
}

@article{drusvyatskiy2016optimal,
  title={An optimal first order method based on optimal quadratic averaging},
  author={Drusvyatskiy, Dmitriy and Fazel, Maryam and Roy, Scott},
  journal={arXiv preprint arXiv:1604.06543},
  year={2016},
  url={https://arxiv.org/pdf/1604.06543.pdf}
}

@article{allen2014linear,
  title={Linear coupling: An ultimate unification of gradient and mirror descent},
  author={Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
  journal={arXiv preprint arXiv:1407.1537},
  year={2014},
  url={https://arxiv.org/pdf/1407.1537.pdf}
}

@article{nesterov2008accelerating,
  title={Accelerating the cubic regularization of Newton’s method on convex problems},
  author={Nesterov, Yu},
  journal={Mathematical Programming},
  volume={112},
  number={1},
  pages={159--181},
  year={2008},
  publisher={Springer},
  doi={10.1007/s10107-006-0089-x},
  url={http://folk.uib.no/ssu029/Pdf_file/Nesterov08.pdf}
}

@article{qian1999momentum,
  title={On the momentum term in gradient descent learning algorithms},
  author={Qian, Ning},
  journal={Neural networks},
  volume={12},
  number={1},
  pages={145--151},
  year={1999},
  publisher={Elsevier},
  doi={10.1016/s0893-6080(98)00116-6},
  url={https://pdfs.semanticscholar.org/735d/4220d5579cc6afe956d9f6ea501a96ae99e2.pdf}
}

@article{amari1998natural,
  title={Natural gradient works efficiently in learning},
  author={Amari, Shun-Ichi},
  journal={Neural computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998},
  publisher={MIT Press},
  doi={10.1162/089976698300017746},
  url={http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf}
}

@inproceedings{wiesler2011convergence,
  title={A convergence analysis of log-linear training},
  author={Wiesler, Simon and Ney, Hermann},
  booktitle={Advances in Neural Information Processing Systems},
  pages={657--665},
  year={2011},
  url={http://papers.nips.cc/paper/4421-a-convergence-analysis-of-log-linear-training.pdf}
}

@article{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning.},
  author={Sutskever, Ilya and Martens, James and Dahl, George E and Hinton, Geoffrey E},
  journal={ICML (3)},
  volume={28},
  pages={1139--1147},
  year={2013},
  url={http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf}
}


@article{hintonNIPS,
  title={Deep Learning, NIPS'2015 Tutorial},
  author={Hinton, Geoff and Bengio, Yoshua and LeCun, Yann},
  year={2015},
  url={http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf}
}



</script>

<script>
    renderMathInElement(
        document.body,
        {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
            ]
        }
    );  

</script>

<!--   <figure id = "eigensum2"></figure>

  <script>



  var eigensum = d3.select("#eigensum2")
  var mathdiv = eigensum.append("div")

  for (var i =0; i < 7; i++) {

   if (i != 6) {
    var html = katex.renderToString("w_"+(i+1)+" p_"+(i+1))
   }else{
    var html = katex.renderToString("\\text{model}")  
   }
   mathdiv.append("span").style("text-align","center")
    .style("display","inline-block")
    .style("width", "110px")
    .style("height", "50px")
    .style("font-size", "16px")
    .style("cursor", "ew-resize")
    .html(html)
    .call( d3.drag()
        .on("drag", (function(i) { return function() { 
          var m = d3.mouse(this) 
          var html = katex.renderToString(m[0]/10 + " p_"+(i+1))
          d3.select(this).html(html)
          var w = zeros(6)
          w[i] = m[0]/10
          updates[i](w)

        }} )(i) ))

   if (i < 5) {
    var html = katex.renderToString("+")
    mathdiv.append("span").style("text-align","center")
    .style("display","inline-block")
    .style("width", "25px")
    .style("height", "50px")
    .style("font-size", "16px")
    .html(html)  
   } else{
    if (i == 5) {
    var html = katex.renderToString("=")
       mathdiv.append("span").style("text-align","center")
    .style("display","inline-block")
    .style("width", "26px")
    .style("height", "50px")
    .style("font-size", "16px")
    .html(html)  
    }
  }
  
  }

  var div = eigensum
    .style("display", "block")
    .style("margin-left","auto")
    .style("margin-right","auto")
    .style("width", 940+"px")
    .style("height", (260) + "px")
    .style("position", "relative")
    .append("div")

  var x = [-0.6, -0.5,-0.4,0.4,0.5,0.6]
  var D = vandermonde(x, 5)
  var Eig = numeric.eig(numeric.dot(numeric.transpose(D),D))

  var lambda = Eig.lambda.x
  var U = numeric.transpose(Eig.E.x)

  var updates = []
  for (var i = 0; i < 6; i++ ){
    var w = zeros(6)
    w[i] = 2

    var scal = lambda[i] < 1e-10 ? -100 : -1.5/Math.sqrt(lambda[i]) 
    var update = renderEigenSum(div.append("svg"),x,numeric.mul(U[i],scal))
    updates.push(update.poly)
    if (i != 5) {
      div.append("span").html("+").style("position", "relative").style("top", "-51px")
    }
  }
  
  div.append("span").html("=").style("position", "relative").style("top", "-51px")  
  renderEigenSum(div.append("svg"),x, [-1,-2,2,-2,2,7])

  var annotate = eigensum

  annotate.append("figcaption")
  .style("width", 200 + "px")
  .style("height", 150 + "px")
  .style("left", "30px")
  .style("position", "absolute")
  .style("border-top", "1px solid black")  
    .style("padding", "10px")    
  .html("The first 2 eigenfeatures, the largest components, captures variations between the clusters. ")

  annotate.append("figcaption")
  .style("width", 200 + "px")
  .style("height", 150 + "px")
  .style("left", "300px")
  .style("position", "absolute")
  .style("border-top", "1px solid black")  
  .style("padding", "10px")  
  .html("Next there are smooth variations within clusters, peaks within clusters,")

  annotate.append("figcaption")
  .style("width", 200 + "px")
  .style("height", 150 + "px")
  .style("left", 300+270+"px")
  .style("position", "absolute")
  .style("border-top", "1px solid black")
  .style("padding", "10px")    
  .html("Finally, jagged polynomials which differ wildly on neighboring points. ")

  </script> -->

  <!-- 
  <figure id = "eigensum"></figure>

  <script>

  // var sliderdiv = d3.select("#eigensum").append("div")

  // var s = sliderGen([135, 10]).cRadius(5).margins(30,20).shifty(0).ticks([-1,1]).showticks(false)

  // for (var i =0; i < 6; i++) {
  //  s(sliderdiv)
  // }
 
  var mathdiv = d3.select("#eigensum").append("div")


  for (var i =0; i < 7; i++) {

   if (i != 6) {
    var html = katex.renderToString([-1,-2,2,-2,2,7][i]+" p_"+(i+1))
   }else{
    var html = katex.renderToString("\\text{model}")  
   }
   mathdiv.append("span").style("text-align","center")
    .style("display","inline-block")
    .style("width", "110px")
    .style("height", "50px")
    .style("font-size", "16px")
    .html(html)

   if (i < 5) {
      var html = katex.renderToString("+")
      mathdiv.append("span").style("text-align","center")
      .style("display","inline-block")
      .style("width", "25px")
      .style("height", "50px")
      .style("font-size", "16px")
      .html(html)  
   } else{
    if (i == 5) {
      var html = katex.renderToString("=")
         mathdiv.append("span").style("text-align","center")
      .style("display","inline-block")
      .style("width", "26px")
      .style("height", "50px")
      .style("font-size", "16px")
      .html(html)  
    }
  }
  
  }

  var div = d3.select("#eigensum")
    .style("display", "block")
    .style("margin-left","auto")
    .style("margin-right","auto")
    .style("width", 940+"px")
    .style("height", (200) + "px")
    .style("position", "relative")
    .append("div")

  var x = [-0.6, -0.5,-0.4,0.4,0.5, 0.6]
  var D = vandermonde(x, 5)
  var Eig = numeric.eig(numeric.dot(numeric.transpose(D),D))

  var lambda = Eig.lambda.x
  var U = numeric.transpose(Eig.E.x)

  for (var i = 0; i < 6; i++ ){
    var scal = lambda[i] < 1e-10 ? -100 : -1.5/Math.sqrt(lambda[i]) 
    var w = zeros(6)
    w[i] = [-1,-2,2,-2,2,7][i]
    renderEigenSum(div.append("svg"),x,w)
    if (i != 5) {
    div.append("span").html("+").style("position", "relative").style("top", "-51px")
  }
  }
  
  div.append("span").html("=").style("position", "relative").style("top", "-51px")  
  renderEigenSum(div.append("svg"),x, [-1,-2,2,-2,2,7])


  </script> -->
