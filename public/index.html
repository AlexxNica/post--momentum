<!doctype html>
<meta charset="utf-8">
<script src="assets/lib/template.js"></script>
<script type="text/front-matter">
  title: Why Momentum Works.
  authors:
    - Gabriel Goh: http://gabgoh.github.io
  affiliations:
    - UC Davis: http://math.ucdavis.edu
</script>

<!-- Katex -->
<script src="assets/lib/auto-render.min.js"></script>
<script src="assets/lib/katex.min.js"></script>
<link rel="stylesheet" href="assets/lib/katex.min.css">
<link rel="stylesheet" type="text/css" href="assets/widgets.css">

<!-- Required -->
<script src="assets/lib/lib.js"></script>
<script src="assets/utils.js"></script>
<script>
  var renderQueue = [];
  function renderMath(elem) {
  renderMathInElement(
      elem,
      {
          delimiters: [
              {left: "$$", right: "$$", display: true},
              {left: "$", right: "$", display: false},
          ]
      }
  );
  }

  var deleteQueue = [];
  function renderLoading(figure) {
    var loadingScreen = figure.append("svg")
    .style("width", figure.style("width"))
    .style("height", figure.style("height"))
    .style("position","absolute")
    .style("top", "0px")
    .style("left","0px")
    .style("background","white")
    .style("border", "1px dashed #DDD")
    .style("opacity", 1)

    return function(callback) { loadingScreen
                                  .remove() }
  }

</script>
<svg style="display: none;">
  <g id="pointerThingy">
    <circle fill="none" stroke="#FF6C00" stroke-linecap="round" cx="0" cy="0" r="14"/>
    <circle fill="#FF6C00" cx="0" cy="0" r="11"/>
    <path id="XMLID_173_" fill="#FFFFFF" d="M-3.2-1.3c0-0.1,0-0.2,0-0.3c0-0.1,0-0.2,0-0.3c-0.6,0-1.2,0-1.8,0c0,0.6,0,1.2,0,1.8
      c0.2,0,0.4,0,0.6,0c0-0.4,0-0.8,0-1.2c0,0,0.1,0,0.1,0c0.3,0,0.5,0,0.8,0C-3.4-1.3-3.3-1.3-3.2-1.3c0,0.2,0,0.4,0,0.6
      c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0,0,0,0-0.1c0-1.6,0-3.2,0-4.8c0-0.6,0-1.2,0-1.8c0,0,0,0,0.1,0
      c0.3,0,0.7,0,1,0c0.1,0,0.1,0,0.2,0c0-0.2,0-0.4,0-0.6c-0.4,0-0.8,0-1.2,0C-2-7.2-2-7-2-6.8c0,0,0,0-0.1,0c-0.2,0-0.3,0-0.5,0
      c0,0,0,0-0.1,0c0,1.8,0,3.6,0,5.5c-0.2,0-0.3,0-0.4,0C-3.1-1.3-3.2-1.3-3.2-1.3z M1.1-3.7C1-3.8,1-3.8,1.1-3.7C1-4,1-4.1,1-4.3
      c0,0,0,0,0-0.1c-0.4,0-0.8,0-1.2,0c0-0.8,0-1.6,0-2.4c-0.2,0-0.4,0-0.6,0c0,1.8,0,3.6,0,5.5c0.2,0,0.4,0,0.6,0c0-0.8,0-1.6,0-2.4
      c0,0,0.1,0,0.1,0C0.3-3.7,0.6-3.7,1.1-3.7C1-3.7,1-3.7,1.1-3.7C1.1-3.7,1-3.7,1.1-3.7c0,0.8,0,1.6,0,2.3c0,0,0,0.1,0,0.1
      c0.2,0,0.4,0,0.6,0c0-0.6,0-1.2,0-1.8c0.4,0,0.8,0,1.2,0c0,0.8,0,1.6,0,2.4c0.2,0,0.4,0,0.6,0c0-0.6,0-1.2,0-1.8c0.2,0,0.4,0,0.6,0
      c0,0,0,0,0,0.1c0,0.1,0,0.3,0,0.4c0,0,0,0.1,0,0.1c0.2,0,0.4,0,0.5,0c0,0,0.1,0,0.1,0.1c0,0.2,0,0.5,0,0.7c0,1.1,0,2.3,0,3.4
      c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0c0,0,0,0,0,0c0,0.6,0,1.1,0,1.7c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0c0,0.4,0,0.8,0,1.2
      c-1.6,0-3.2,0-4.9,0c0-0.4,0-0.8,0-1.2c-0.2,0-0.4,0-0.6,0C-2,3.8-2,3.4-2,3c-0.2,0-0.4,0-0.6,0c0,0.4,0,0.8,0,1.2
      c0.2,0,0.4,0,0.6,0C-2,4.8-2,5.4-2,6c2,0,4.1,0,6.1,0c0-0.1,0-0.2,0-0.3c0-0.5,0-0.9,0-1.4c0-0.1,0-0.1,0-0.2c0.2,0,0.4,0,0.5,0
      c0.1,0,0.1,0,0.1-0.1c0-0.4,0-0.9,0-1.3c0-0.1,0-0.3,0-0.4c0.1,0,0.2,0,0.3,0c0.1,0,0.2,0,0.3,0c0-1.4,0-2.8,0-4.3
      c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c-0.4,0-0.8,0-1.2,0c0-0.2,0-0.4,0-0.6
      c-0.1,0-0.2,0-0.3,0c-0.4,0-0.9,0-1.3,0C1.2-3.7,1.1-3.7,1.1-3.7z M-3.2,1.8c0,0.4,0,0.8,0,1.2c0.2,0,0.4,0,0.5,0
      c0.1,0,0.1,0,0.1-0.1c0-0.3,0-0.6,0-1c0-0.1,0-0.1,0-0.2C-2.8,1.8-3,1.8-3.2,1.8c0-0.4,0-0.8,0-1.2c-0.2,0-0.4,0-0.6,0
      c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0,0,0,0,0.1c0,0.1,0,0.3,0,0.4c0,0.2,0,0.5,0,0.7
      c0,0,0,0.1,0.1,0.1c0.1,0,0.2,0,0.3,0C-3.4,1.8-3.3,1.8-3.2,1.8z"/>
    <path id="XMLID_172_" fill="#FFFFFF" d="M4.1,4.2C4.1,4.2,4.1,4.2,4.1,4.2c0-0.6,0-1.2,0-1.8c0,0,0,0,0,0c0.2,0,0.4,0,0.6,0
      c0,0,0-0.1,0-0.1c0-1.1,0-2.3,0-3.4c0-0.2,0-0.5,0-0.7c0,0,0-0.1-0.1-0.1c-0.2,0-0.4,0-0.5,0c0,0,0-0.1,0-0.1c0-0.1,0-0.3,0-0.4
      c0,0,0-0.1,0-0.1c-0.2,0-0.4,0-0.6,0c0,0.6,0,1.2,0,1.8c-0.2,0-0.4,0-0.6,0c0-0.8,0-1.6,0-2.4c-0.4,0-0.8,0-1.2,0
      c0,0.6,0,1.2,0,1.8c-0.2,0-0.4,0-0.6,0c0,0,0-0.1,0-0.1c0-0.7,0-1.5,0-2.2c0,0,0-0.1,0-0.1l0,0c0.1,0,0.2,0,0.2,0
      c0.4,0,0.9,0,1.3,0c0.1,0,0.2,0,0.3,0c0,0.2,0,0.4,0,0.6c0.4,0,0.8,0,1.2,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6
      c0.2,0,0.4,0,0.6,0c0,1.4,0,2.8,0,4.3c-0.1,0-0.2,0-0.3,0c-0.1,0-0.2,0-0.3,0c0,0.1,0,0.3,0,0.4c0,0.4,0,0.9,0,1.3
      c0,0.1,0,0.1-0.1,0.1C4.5,4.2,4.3,4.2,4.1,4.2L4.1,4.2z"/>
    <path id="XMLID_171_" fill="#FFFFFF" d="M4.1,4.2c0,0.1,0,0.1,0,0.2c0,0.5,0,0.9,0,1.4c0,0.1,0,0.2,0,0.3C2.1,6,0,6-2,6
      c0-0.6,0-1.2,0-1.8c-0.2,0-0.4,0-0.6,0c0-0.4,0-0.8,0-1.2C-2.4,3-2.2,3-2,3c0,0.4,0,0.8,0,1.2c0.2,0,0.4,0,0.6,0c0,0.4,0,0.8,0,1.2
      c1.6,0,3.2,0,4.9,0c0-0.4,0-0.8,0-1.2C3.7,4.2,3.9,4.2,4.1,4.2L4.1,4.2z"/>
    <path id="XMLID_170_" fill="#FFFFFF" d="M-2-6.8c0,0.6,0,1.2,0,1.8c0,1.6,0,3.2,0,4.8c0,0,0,0,0,0.1c-0.2,0-0.4,0-0.6,0
      c0-0.2,0-0.4,0-0.6c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6l0,0c0.1,0,0.1,0,0.2,0c0.1,0,0.3,0,0.4,0c0-1.8,0-3.6,0-5.5
      c0,0,0.1,0,0.1,0C-2.4-6.8-2.2-6.8-2-6.8C-2.1-6.8-2-6.8-2-6.8L-2-6.8z"/>
    <path id="XMLID_169_" fill="#FFFFFF" d="M1.1-3.7C1-3.7,1-3.7,1.1-3.7c-0.4,0-0.8,0-1.2,0c0,0,0,0-0.1,0c0,0.8,0,1.6,0,2.4
      c-0.2,0-0.4,0-0.6,0c0-1.8,0-3.6,0-5.5c0.2,0,0.4,0,0.6,0c0,0.8,0,1.6,0,2.4c0.4,0,0.8,0,1.2,0c0,0,0,0.1,0,0.1C1-4.1,1-4,1.1-3.7
      C1-3.8,1-3.8,1.1-3.7L1.1-3.7z"/>
    <path id="XMLID_168_" fill="#FFFFFF" d="M-3.2,1.8c-0.1,0-0.2,0-0.3,0c-0.1,0-0.2,0-0.3,0c0,0-0.1,0-0.1-0.1c0-0.2,0-0.5,0-0.7
      c0-0.1,0-0.3,0-0.4c0,0,0,0,0-0.1c-0.2,0-0.4,0-0.6,0c0-0.2,0-0.4,0-0.6c0.2,0,0.4,0,0.6,0c0,0.2,0,0.4,0,0.6c0.2,0,0.4,0,0.6,0
      C-3.2,0.9-3.2,1.3-3.2,1.8c0.2,0,0.4,0,0.6,0c0,0.1,0,0.1,0,0.2c0,0.3,0,0.6,0,1C-2.6,3-2.7,3-2.7,3c-0.2,0-0.3,0-0.5,0
      C-3.2,2.6-3.2,2.2-3.2,1.8z"/>
    <path id="XMLID_167_" fill="#FFFFFF" d="M-3.2-1.3c-0.1,0-0.2,0-0.3,0c-0.3,0-0.5,0-0.8,0c0,0,0,0-0.1,0c0,0.4,0,0.8,0,1.2
      c-0.2,0-0.4,0-0.6,0c0-0.6,0-1.2,0-1.8c0.6,0,1.2,0,1.8,0c0,0.1,0,0.2,0,0.3C-3.2-1.5-3.2-1.4-3.2-1.3L-3.2-1.3z"/>
    <path id="XMLID_166_" fill="#FFFFFF" d="M-2-6.8C-2-7-2-7.2-2-7.4c0.4,0,0.8,0,1.2,0c0,0.2,0,0.4,0,0.6c-0.1,0-0.1,0-0.2,0
      C-1.3-6.8-1.6-6.8-2-6.8C-2-6.8-2-6.8-2-6.8L-2-6.8z"/>
  </g>
</svg>

<dt-article class="centered">
  <h1>Why Momentum Works [DRAFT]</h1>

  <figure style = "position:relative; width:920px; height:400px;">
    <div id="banana" style="position:relative; border: 1px solid rgba(0, 0, 0, 0.2);"></div>
    <div id="sliderAlpha" style="position:absolute; width:300px; height: 50px; left:0px; top: 320px;">
      <text class="figtext" style="top: -5px; left: 20px; position: relative;">Step size α = 0.02</text>
    </div>
    <div id="sliderBeta" style="position:absolute; width: 300px; height: 50px; left: 240px; top: 320px;;">
      <text class="figtext" style="top: -5px; left: 20px; position: relative;">Momentum β = 0.99</text>
    </div>
    <figcaption id="Bananacaption" style="position:absolute; width: 360px; height: 50px; left: 520px; top: 320px;">
      Find something to put here. Find something to put here. Gradient Descent (β = 0), Momentum (β = 0.99). Find something to put here. Find something to put here.
    </figcaption>
  </figure>

  <dt-byline class="l-page"></dt-byline>

  <script src="assets/lib/contour_plot.js"></script>
  <script src="assets/iterates.js"></script>
  <script>

  // Render Foreground
  var iterControl = genIterDiagram(bananaf, [1,1/3], [[-2,2],[2/3 + 0.4,-2/3 + 0.4]])
                    .alpha(0.003)
                    .beta(0)
                    (d3.select("#banana").style("position","relative"))

  var iterChange = iterControl.control
  var getw0 = iterControl.w0

  var StepRange = d3.scaleLinear().domain([0,100]).range([0,0.0062])
  var MomentumRange = d3.scaleLinear().domain([0,100]).range([0,0.98])

  var update = function (i,j) { iterChange(i, 0, getw0()) }

  var slidera = sliderGen([230, 40])
              .ticks([0,0.0015,0.003])
              .change( function (i) {
                d3.select("#sliderAlpha").selectAll(".figtext").html("Step size α = " + getalpha().toPrecision(2) )
                iterChange(getalpha(), getbeta(), getw0() )
              } )
              .startxval(0.003)
              .cRadius(7)
              .shifty(-12)
              .margins(20,20)

  var sliderb = sliderGen([230, 40])
              .ticks([0,0.5,0.99])
              .change( function (i) {
                d3.select("#sliderBeta").selectAll(".figtext").html("Momentum β = " + getbeta().toPrecision(2) )
                iterChange(getalpha(), getbeta(), getw0() )
              } )
              .cRadius(7)
              .shifty(-12)
              .startxval(0.74)
              .margins(20,20)

  var getalpha = slidera( d3.select("#sliderAlpha")).xval
  var getbeta  = sliderb( d3.select("#sliderBeta")).xval

  iterChange(getalpha(), getbeta(), getw0() )

  </script><!--
  <p>
  If you've spent enough some time optimizing smooth functions, you may have met this optimizers' old nemesis. The condition has many names, but it always borrows the language of pathology: Ill-conditioning, pathological curvature, dead gradients. You're performing gradient descent. Your choice of step-size, seems correct. The gradients don't blow up. There is no division by zero, no NaNs, no square rooting of minus one. In fact, things often begin quite well - with an impressive, almost immediate decrease in the loss. But as the iterations progress, you start to get a nagging feeling you're not making as much progress as you should be. You're iterating hard, but the loss isn't getting smaller. Should you keep iterating, and hope for the best?
  </p>

  <p>
  The problem could be pathological curvature. The landscapes are often described as a valleys, trenches, canals, ravines. In these steep valleys, gradient descent fumbles. All progress along certain directions grind close to a halt. The iterates only approaches the optimum in small, tedious steps, or ossilate between two valleys. These are all classical symptoms of pathological curvature.
  </p> -->
  <p>
  Gradient descent has many virtues, but speed is not one of them. When optimizing a smooth function $f$, gradient descent

  $$w^{k+1} = w^k-\alpha\nabla f(w^k).$$

  Your choice of step-size, $\alpha$ seems correct. The gradients don't blow up. There is no division by zero, no NaNs, no square rooting of minus one. In fact, things begin quite well - with an impressive, almost immediate decrease in the loss. But after a time, you notice something amiss. The improvements begin to slow. The loss isn't getting smaller. And convergence is nowhere in sight. Something is wrong, but what? -->
  </p>

  <p>
  You may have encountered the optimizers old nemesis - pathological curvature. Pathological curvature is, simply put, regions of $f$ which aren't scaled properly. The landscapes are often described as a valleys, trenches, canals, ravines. The iterates either jump between valleys, or approach the optimum in small, timid steps. Progress along certain directions grind to a halt. In these unfortunate regions, gradient descent fumbles.  </p>
  <p>
  But there is a mysterious tweak to gradient descent which improves things significantly. The change is innocent, and costs almost nothing - we give gradient descent a short term memory:

  $$
  \begin{aligned}
  z^{k+1}&=\beta z^{k}+\nabla f(w^{k})\\
  w^{k+1}&=w^{k}-\alpha z^{k+1},
  \end{aligned}
  $$

  Instead of taking taking a step in the gradient, we move in a geometrically decaying superposition all the previous gradients. When $ \beta = 0 $ , we recover gradient descent. But for $ \beta = 0.99 $ (sometimes $ 0.999$, if things are really bad), this appears to be the boost we need. Our iterations regain that speed and boldness it lost, speeding to the optimum with a renewed energy.

  </p>
  <p>
  Optimizers call this minor miracle "acceleration".
  </p>

  <p>
  The new iteration, known best to machine learners as Momentum<dt-fn> also known by it's older names, the Heavy Ball Method or the Chebychev Method </dt-fn> <dt-cite key="sutskever2013importance,polyak1964some,rutishauser1959theory"></dt-cite>, may seem like a cheap hack. A simple trick to get around gradient descent's more aberrant behavior - a smoother for oscillations between steep canyons. But the truth, if anything, is the other way round. It is gradient descent which is the hack. First, momentum gives a guaranteed quadratic speedup on convex functions. This is no small matter - this is the speedup you get from the Fast Fourier Transform, and Grover's Algorithm. When the universe speeds things up for you quadratically, you should start to pay attention.
  </p>

  <p>
  But there's more. A lower bound, courtesy of Nesterov <dt-cite key="nesterov2013introductory"></dt-cite>, states that momentum is in a certain technical sense optimal. Now this doesn't mean it is the best algorithm under any circumstances for all functions. But it does mean it satisfies some curiously beautiful mathematical properties which scratches a very human itch for perfection and closure. But more on that later. Let's say this for now - momentum is an algorithm for the book.
  </p>

<hr>
  <h2>First Steps: Gradient Descent</h2>
  <p>
  We begin by studying gradient descent on simplest model possible which isn't trivial. I choose the convex quadratic,

  $$f(w) = \tfrac{1}{2}w^TAw - b^Tw, \qquad w \in \mathbf{R}^n. $$

  Simple as this model may be, it is rich enough to approximate of many functions (think of $A$ as your favorite model of curvature - the Hessian, Fisher Information Matrix <dt-cite key="amari1998natural"></dt-cite>, etc) and captures all the key features of pathological curvature. And more importantly, we can write a formula for gradient descent iterations here in closed form, with no approximating bounds or inequalities.
  </p>

  <p>
  This is how it goes. Since $\nabla f(w)=Aw - b$, the iterates are

  $$
  w^{k+1}=w^{k}- \alpha (Aw^{k} - b)
  $$

  Here's the trick. There is a very natural space to view gradient descent where the iterates all act independently - the eigenvalues of $A$.
  </p>
  <figure style = "width:750px; height:340px; display:block; margin-left:auto; margin-right:auto; position:relative" id = "change_of_variables">
  <div id = "mom1" style="width:400px; position:absolute; left:0px; top:0px"></div>
  <div id = "mom2" style="width:400px; position:absolute; left:400px; top:0px"></div>
  </svg>
  </figure>
  <script>
  deleteQueue.push(renderLoading(d3.select("#change_of_variables")))
  renderQueue.push(function(callback) {
    var U = givens(Math.PI/4)
    var Ut = numeric.transpose(U)
    // Render Foreground
    var left = d3.select("#mom1").style("border", "1px solid rgba(0, 0, 0, 0.2)")

    var c1 = genIterDiagram(quadf,  [0,0], [[-3,3],[-3,3]])
              .width(340)
              .height(340)
              .iters(300)
              .alpha(0.018)
              .showSolution(false)
              .pathWidth(1)
              .circleRadius(1.5)
              .pointerScale(0.5)
              .showStartingPoint(false)
              .drag(function() {
                c2.control(c1.alpha(),
                          c1.beta(),
                          numeric.dot(U,c1.w0())) })
              (left)

    var right = d3.select("#mom2").style("border", "1px solid rgba(0, 0, 0, 0.2)")
    var c2 = genIterDiagram(eyef,  [0,0], [[-3,3],[-3,3]])
              .width(340)
              .height(340)
              .iters(300)
              .alpha(0.018)
              .showSolution(false)
              .pathWidth(1)
              .circleRadius(1.5)
              .pointerScale(0.5)
              .showStartingPoint(false)
              .drag(function() {
                c1.control(c2.alpha(),
                          c2.beta(),
                          numeric.dot(Ut,c2.w0())) })
              (right)

  // Initalize
  c2.control(0.018,0,[-2.5,1])
  c1.control(0.018,0,numeric.dot(Ut,[-2.5,1]));
  callback(null);
});

</script>
<p>
  Every symmetric matrix $A$ has an eigenvalue decomposition

  $$
  A=Q \text{diag}(\lambda_{1},\ldots,\lambda_{n})Q^{T},\qquad Q = [q_1,\ldots,q_n],
  $$

  and as per convention, we will assume that the $\lambda_i$'s are sorted, from smallest $\lambda_1$ to biggest $\lambda_n$. Perform a change of variables, $x^{k} = Q^T(w^{k} - w^\star)$. And the iterations break apart, becoming

  $$
  x_{i}^{k+1}=x_{i}^{k}-\alpha \lambda_ix_{i}^{k} = (1-\alpha\lambda_i)x^k_i=(1-\alpha \lambda_i)^kx^0_i
  $$

  Moving back to our original space $w$, we can see that

  $$
  w^k - w^\star = Qx^k=\sum_i^n x^0_i(1-\alpha\lambda_i)^k q_i
  $$

  and there we have it - gradient descent in closed form.
  </p>
  </p>
  <h3>Decomposing the Error</h3>
  <p>
  The above equation admits a simple interpretation. $x^0$ is the component of the error in the initial guess in the space of $Q$. There are $n$ such errors, and each of these errors follow their own, solitary path to the minimum, at a rate of $1-\alpha\lambda_i$. The closer that number is to $1$, the slower it converges.
  </p>
  <p>
  For most step-sizes, the eigenvectors with largest eigenvectors converge the fastest. This triggers an explosion of progress in the first few iterations, before things slow down as the smaller eigenvector's struggles are revealed. By writing the contributions of each eigenspaces's sub-optimality to the loss
  $$
  f(w^{k})-f(w^{\star})=\sum(1-\alpha\lambda_{i})^{2k}\lambda_{i}[x_{i}^{0}]^2
  $$
  we can visualize the contributions of each eigenspace to the loss $f(w^k)$.
  </p>
  <figure style="position:relative; width:920px; height:360px" id = "milestones_gd">
  <figcaption style="position:absolute; text-align:left; left:140px; width:350px; height:80px">Optimization can be seen as combination of several component problems, shown here as <svg style="position:relative; top:2px; width:3px; height:14px; background:#fde0dd"></svg> 1 <svg style="position:relative; top:2px; width:3px; height:14px; background:#fa9fb5"></svg> 2 <svg style="position:relative; top:2px; width:3px; height:14px; background:#c51b8a"></svg> 3 with eigenvalues <svg style="position:relative; top:2px; width:3px; height:14px; background:#fde0dd"></svg> $\lambda_1=1$, <svg style="position:relative; top:2px; width:3px; height:14px; background:#fa9fb5"></svg> $\lambda_2=10$, and <svg style="position:relative; top:2px; width:3px; height:14px; background:#c51b8a"></svg> $\lambda_3=100$ respectively. </figcaption>

<!-- ["#fde0dd", "#fa9fb5", "#c51b8a"]
 -->
  <div id = "sliderStep" style="position:absolute; left:550px; width:250px; height:100px">
    <div id="stepSizeMilestones" class="figtext" style="position:absolute; left:15px; top:15px">Step Size </div>
    <div class="figtext2" style="position:absolute; font-size:11px; left:152px; top:18px">Optimal Step Size </div>
    <svg style="position:absolute; font-size:10px; left:224px; top:34px">
<line marker-end="url(#arrowhead)" style="stroke: black; stroke-width: 1.5; visibility: visible;" x2="5" y2="10" x1="5" y1="0"></line>
    </svg>

  </div>
  <div id = "obj"></div>
  </figure>
  <script src="assets/milestones.js"></script>
  <script>
    deleteQueue.push(renderLoading(d3.select("#milestones_gd")))  
    renderQueue.push(function(callback) {
      var graphDiv = d3.select("#obj")
                      .style("width",  920 + "px")
                      .style("height", 300 + "px")
                      .style("top", "90px")
                      .style("position", "relative")
                      .style("margin-left", "auto")
                      .style("margin-right", "auto")
                      .attr("width", 920)
                      .attr("height", 500)

      var svg = graphDiv.append("svg").attr("width", 920).attr("height", 500)

      var updateSliderGD = renderMilestones(svg, function() {})

      var slidera = sliderGen([250, 80])
                  .ticks([0,1,200/(101),2])
                  .change( function (i) {
                    var html = katex.renderToString("\\alpha = " + i.toPrecision(4) )
                    d3.select("#stepSizeMilestones")
                      .html("Stepsize " + html )
                    updateSliderGD(i,0.000)
                  } )
                  .ticktitles(function(d,i) { return [0,1,"",2][i] })
                  .startxval(200/(106))
                  .cRadius(7)
                  .shifty(-12)
                  .shifty(10)
                  .margins(20,20)(d3.select("#sliderStep"))


      renderDraggable(svg, [133.5, 23], [114.5, 90], 2, " ").attr("opacity", 0.4)
      renderDraggable(svg, [133.5, 88], [115.5, 95], 2, " ").attr("opacity", 0.4)
      renderDraggable(svg, [132.5, 154], [114.5, 100], 2, " ").attr("opacity", 0.4)


      svg.append("text")
        .attr("class", "katex morsd mathit")
        .style("font-size", "19px")
        .style("font-family","KaTeX_Math")
        .attr("x", 105)
        .attr("y", 50)
        .attr("text-anchor", "end")
        .attr("fill", "gray")
        .html("f(w<tspan baseline-shift = \"super\" font-size = \"15\">k</tspan>) - f(w<tspan baseline-shift = \"super\" font-size = \"15\">*</tspan>)")



      svg.append("text")
        .style("font-size", "13px")
        .attr("x", 0)
        .attr("y", 80)
        .attr("dy", 0)
        .attr("transform", "translate(110,0)")
        .attr("class", "caption")
        .attr("text-anchor", "end")
        .attr("fill", "gray")
        .text("At the initial point, the error in each component is equal.")

      svg.selectAll(".caption").call(wrap, 100)


      svg.append("text")
        .style("font-size", "13px")
        .attr("x", 420)
        .attr("y", 270)
        .attr("dy", 0)
        .attr("text-anchor", "end")
        .attr("fill", "gray")
        .text("At the initial point, the error in each component is equal.")

      callback(null);
    });
  </script>
  <p>
 <h3>Choosing A Step-size</h3>
  <p>
  The above analysis gives us immediate guidance as to how to set a step-size $\alpha$. In order to converge, each $|1-\alpha \lambda_i|$ must be strictly less than 1. <dt-fn>All workable step-sizes, therefore, fall in the interval $0<\alpha<\frac{2}{\lambda_n}$</dt-fn> The overall convergence rate is determined by the slowest error component, which must be either $\lambda_0$ or $\lambda_n$:
  $$
  \begin{aligned}\text{rate}(\alpha) & ~=~ \max_{i}\left|1-\alpha\lambda_{i}\right|\\[0.9em] & ~=~ \max\left\{|1-\alpha\lambda_{0}|,~ |1-\alpha\lambda_{n}|\right\} \end{aligned}
  $$
  </p>
  <p>
  This overall rate is minimized when the rates for $\lambda_0$ and $\lambda_n$ are the same -- this mirrors our informal observation in the previous section that the optimal step size causes the first and last eigenvector to converge at the same time. If we work this through we get:

  $$
  \begin{aligned}
  \text{optimal }\alpha ~=~{\mathop{\text{argmin}}\limits_\alpha} ~\text{rate}(\alpha) & ~=~\frac{2}{\lambda_{1}+\lambda_{n}}\\[1.4em]
  \text{optimal rate} ~=~{\min_\alpha} ~\text{rate}(\alpha) & ~=~\frac{\lambda_{n}/\lambda_{1}-1}{\lambda_{n}/\lambda_{1}+1}
  \end{aligned}
  $$
  </p>
  <p>
  Notice the ratio $\lambda_n/\lambda_1$ determines the convergence rate of the problem. In fact, this ratio appears often enough that we give it a name, and a symbol - the condition number.
  $$
  \text{condition number} := \kappa :=\frac{\lambda_n}{\lambda_1}
  $$
  The condition number means many things. It is a measure of how singular a matrix is. It is a measure of how robust $A^{-1}b$ is to perturbations in $b$. And in this context - the condition number gives us a measure of how poorly gradient descent will perform. A ratio of $1$ is ideal, giving convergence in one step (of course, the function is trivial). And larger the ratio, the slower gradient descent will be. The condition number is therefore a direct measure pathological curvature.
  </p>

<hr>

  <h3>Example - Linear Regression</h3>
  <p>
  The above analysis reveals an interesting insight - all errors are not made equal. Indeed, there are different kinds of errors, $n$, to be exact, one for each of the eigenvectors of $A$. And gradient descent is better at correcting some kinds of errors better than others. Lets take apply this understanding on a few, simple examples.
  </p>

  <p>
  Linear Regression, conveniently a quadratic objective, provides a cartoon we need for understanding pathological curvature in deep learning. Let's do a quick refresher. In Linear Regression we fit $n$ features (the components of $z_i$) to observations $d_i$. Assume there are $m$ data points, stacked to form matrix $Z$. Then the optimization problem we're solving is

  $$
  \text{minimize}\qquad \frac{1}{2}\sum_i^m (z_i^Tw - d_i)^2 = \tfrac{1}{2}\|Zw-d\|^2
  $$

  which is equivalent to
  $$
  \text{minimize}\qquad\tfrac{1}{2}w^{T}Z^{T}Zw-(Zd)^{T}w
  $$
  a quadratic function. To understand gradient descent on linear regression, we must understand the eigenvectors of $Z^TZ$.
  <h3>Bad Scaling</h3>

  <p>
  One simple culprit of pathological curvature is a bad scaling of the data. Even if the data were perfectly uncorrelated with mean 0, i.e. $Z^TZ$ was diagonal, the condition number of the matrix still depends on the variance of the individual features (the values of the diagonal). If the ratio of the largest and smallest variances are large, the matrix $Z^TZ$ becomes ill conditioned.<dt-fn>This holds even if the matrix is correlated, see <dt-cite key="wiesler2011convergence"></dt-cite></dt-fn>
  </p>

  <p>
  Fortunately, this kind of pathological curvature is relatively easily fixed. It displays a clear signature - the value of the gradients in the "pathological coordinates" are abnormally small. And they can thus be weighted upwards. This is the principle behind Adagrad <dt-cite key="duchi2011adaptive"></dt-cite> and ADAM <dt-cite key="kingma2014adam"></dt-cite>. And in the training of deep neural networks, we can modify our objective so that the data is normalized before moving to the next layer - this is batch normalization <dt-cite key="ioffe2015batch"></dt-cite>, a surprisingly powerful heuristic. </p>

  <h3>Understanding Early Stopping</h3>
  <p>
  The situation where the directions of pathology aren't axis aligned gets more complicated. But by studying this carefully, we can precisely understand and characterize the path the iterates take to the optimum, gain a few insights into a common heuristic in the optimization of neural networks - Early Stopping.
  </p>

  <p>
  It has been noted by many practitioners that there exists a turning point sometime in the middle of optimization where the model's generalization capacity peaks. The observation is as follows. Early in the optimization, when we are far from the optimum, both the training and test error goes down - all is well. But somewhere in the middle, we hit a point of negative returns, so to speak. The test error stops going down, and in fact, starts going up! Each iterate now hurts our model, in a classical case of over-fitting. This is a strikingly counterintuitive fact. Too much optimality, it seems, is a bad thing! But how can this be?
  </p>

  <h4> Example: Polynomial Regression</h4>

  <p>
  Lets understand this phenomena in the quadratic model. Consider polynomial regression. Given 1D data, $\xi_i$, we wish to fit the model

  $$
\text{model}(\xi)=w_{1}p_{1}(\xi)+\cdots+w_{n}p_{n}(\xi)\qquad p_{i}=\xi\mapsto\xi^{i-1}
  $$

  to our data. This model, though nonlinear in the input ($\xi$) is linear in the weights, i.e. we can write the model as a linear combination of monomials, like:
  </p>

  <figure id = "poly0" style="width:940px; height:165px"></figure>
  <script src="assets/eigensum.js"></script>
  <script>
  deleteQueue.push(renderLoading(d3.select("#poly0")))  
  renderQueue.push(function(callback) {

    // Preprocess x, get eigendecomposition, etc
    var x = [-0.6, -0.55,-0.5,-0.45,-0.4,0.4,0.45,0.5,0.55,0.6]
    var D = vandermonde(x, 5)
    var Eigs = eigSym(numeric.dot(numeric.transpose(D),D))
    var U = Eigs.U
    var lambda = Eigs.lambda

    // Preprocess y
    var b = [-3/2,-4/2,-5/2,-3/2,-2/2,1/2,2/2,3/2,2/2,1/2]
    var Dtb = numeric.dot(b,D)
    var sol = numeric.mul(numeric.dot(U, Dtb), lambda.map(inv))

    var step = 1.8/lambda[0]
    var iter = geniter(U, lambda, Dtb, step)

    var eigensum = d3.select("#poly0")

    var wi = [2,2,2,2,2,2]

    function refit(b) {
      var Dtb = numeric.dot(b,D)
      var sol = numeric.mul(numeric.dot(U, Dtb), lambda.map(inv))
      var Utsol = numeric.dot(sol,U)
      eigenControl.updateweights(Utsol)
    }

    var eigenControl = renderEigenPanel(eigensum, numeric.identity(6), x, b, wi, refit);

    callback(null);
  });

  </script>
  <p>

  Because of the linearity, we can fit this model at our observed data $\xi_i$ by using linear regression on the model mismatch

  $$
  \text{minimize}_w \qquad\frac{1}{2}\sum_i (\text{model}(\xi_{i})-d_{i})^{2} = \tfrac{1}{2}\|Zw - d\|^2
  $$
  where
  $$
  Z=\left(\begin{array}{ccccc}
  1 & \xi_{1} & \xi_{1}^{2} & \ldots & \xi_{1}^{n-1}\\
  1 & \xi_{2} & \xi_{2}^{2} & \ldots & \xi_{2}^{n-1}\\
  \vdots & \vdots & \vdots & \ddots & \vdots\\
  1 & \xi_{m} & \xi_{m}^{2} & \ldots & \xi_{m}^{n-1}
  \end{array}\right).
  $$
  </p>

  <p>
  The path of convergence, as we know, is elucidated when we view the iterates in the space of $Q$. So let's recast our regression problem in $Q$ space. First, we do a change of variables, by rotating $w$ into $Qw$, and counter-rotating our feature maps $p$ into eigenspace, $\bar{p}$. We can now conceptualize the same regression as one over a different polynomial basis, with the model

  $$
  \text{model}(\xi)=x_{1}\bar{p}_{1}(\xi)+\cdots+x_{n}\bar{p}_{n}(\xi)\qquad p_{i}=\sum q_{ij}p_j.
  $$

  This model is identical to the old one. But these new features $\bar{p}$ (which I call eigenfeatures) and weights have the pleasing property that each coordinate acts independently of the other. Now our optimization problem breaks down, really, into $n$ small 1D optimization problems. And each coordinate can be optimized greedily and independently, one at a time in any order, to produce the final, global, optimum. These independent features, which I call the eigenfeatures, are also much more informative,
  </p>


  <figure id = "poly1" style="width:940px; height:285px"></figure>

  <script>
  deleteQueue.push(renderLoading(d3.select("#poly1")))    
  renderQueue.push(function(callback) {
    var inv = function(lambda) { return 1/lambda }
    var scal = function(lambda) { return lambda < 1e-10 ? -100 : 1.5/Math.sqrt(lambda) }

    // Preprocess x, get eigendecomposition, etc
    var x = [-0.6, -0.55,-0.5,-0.45,-0.4,0.4,0.45,0.5,0.55,0.6]
    var D = vandermonde(x, 5)
    var Eigs = eigSym(numeric.dot(numeric.transpose(D),D))
    var U = Eigs.U
    var lambda = Eigs.lambda

    // Preprocess y
    var b = [-3/2,-4/2,-5/2,-3/2,-2/2,1/2,2/2,3/2,2/2,1/2]
    var Dtb = numeric.dot(b,D)
    var sol = numeric.mul(numeric.dot(U, Dtb), lambda.map(inv))

    var step = 1.8/lambda[0]
    var iter = geniter(U, lambda, Dtb, step)

    var eigensum = d3.select("#poly1")

    var wi = lambda.slice(0).map(scal)

    function refit(b) {
      var Dtb = numeric.dot(b,D)
      var sol = numeric.mul(numeric.dot(U, Dtb), lambda.map(inv))
      var Utsol = numeric.dot(sol,U)
      eigenControl.updateweights(sol)
    }

    var eigenControl = renderEigenPanel(eigensum, U, x, b, wi, refit, true)

    var annotate = eigensum

    annotate.append("figcaption")
    .style("width", 200 + "px")
    .style("height", 150 + "px")
    .style("left", "30px")
    .style("position", "absolute")
    .style("border-top", "1px solid black")
      .style("padding", "10px")
    .html("The first 2 eigenfeatures, the largest components, captures variations between the clusters. ")

    annotate.append("figcaption")
    .style("width", 200 + "px")
    .style("height", 150 + "px")
    .style("left", "300px")
    .style("position", "absolute")
    .style("border-top", "1px solid black")
    .style("padding", "10px")
    .html("Next there are smooth variations within clusters, peaks within clusters,")

    annotate.append("figcaption")
    .style("width", 200 + "px")
    .style("height", 150 + "px")
    .style("left", 300+270+"px")
    .style("position", "absolute")
    .style("border-top", "1px solid black")
    .style("padding", "10px")
    .html("Finally, jagged polynomials which differ wildly on neighboring points. ");

    var figwidth = d3.select("#poly1").style("width")
    var figheight = d3.select("#poly1").style("height")
    var svg = d3.select("#poly1")
                        .append("svg")
                        .style("width", figwidth)
                        .style("height", figheight)
                        .style("position", "absolute")
                        .style("top","0px")
                        .style("left","0px")
                        .style("pointer-events","none")

    // Swoopy Annotator
    var annotations = [
    {
      "x": 0,
      "y": 0,
      "path": "M 807,198 A 26.661 26.661 0 0 1 838,159",
      "text": "drag points to fit data",
      "textOffset": [
        799,
        214
      ]
    }
    ]

    var swoopy = d3.swoopyDrag()
      .x(function(d){ return (d.x) })
      .y(function(d){ return (d.y) })
        .draggable(false)
        .annotations(annotations)

    var svg = d3.select("#poly1").selectAll("svg")
    var swoopySel = svg.append("g").attr("class", "figtext").call(swoopy)

    svg.append('marker')
        .attr('id', 'arrow')
        .attr('viewBox', '-10 -10 20 20')
        .attr('markerWidth', 20)
        .attr('markerHeight', 20)
        .attr('orient', 'auto')
      .append('path')
        .attr('d', 'M-6.75,-6.75 L 0,0 L -6.75,6.75')
        .attr("transform", "scale(0.5)")

    swoopySel.selectAll('path').attr('marker-end', 'url(#arrow)')

    callback(null);
  });

  </script>
  <p>
  The observations in the above diagram can be justified mathematically. From a statistical point of view, we would like a model which is in some sense, robust to noise. Our model cannot possibly be meaningful if the slightest perturbation to the observations changes the entire model dramatically. And thus, it would be useful if there were a decomposition of our model into the parts which fit the data, and the parts which fit the noise. The eigenfeatures are in a sense are exactly such a decomposition. In the eigenfeature basis, the most robust components appear in the front (with the largest eigenvalues), and the most sensitive components in the back (with the smallest eigenvalues). And with a small perturbation to the observations, it is the smallest eigenvalues which jump around the most, while the largest eigenvalues remain stable.
  </p>

  <p>
  Now this measure of robustness, by a rather convenient coincidence, is also a measure of how easily an eigenspace converges. And thus, we arrive at a surprising result. The "pathological directions" - the eigenspaces which converge the slowest are also those which are most sensitive to noise! This property is easily exploited. Start at a simple initial point like 0 (call this a prior, if you like), and track the iterates till a desired level of complexity is reached. Lets see how this plays out in gradient descent.
  </p>

  <figure id = "poly2" style="width:940px; height:360px"></figure>

  <script>
  deleteQueue.push(renderLoading(d3.select("#poly2")))  
  renderQueue.push(function(callback) {
    var inv = function(lambda) { return 1/lambda }
    var scal = function(lambda) { return lambda < 1e-10 ? -100 : 1.5/Math.sqrt(lambda) }

    // Preprocess x, get eigendecomposition, etc
    var x = [-0.6, -0.55,-0.5,-0.45,-0.4,0.4,0.45,0.5,0.55,0.6]
    var b = [-3/2,-4/2,-5/2,-3/2,-2/2,1/2,2/2,3/2,2/2,1/2]

    var D = vandermonde(x, 5)
    var Eigs = eigSym(numeric.dot(numeric.transpose(D),D))
    var U = Eigs.U
    var lambda = Eigs.lambda

    // Preprocess y
    var Dtb = numeric.dot(b,D)
    var sol = numeric.mul(numeric.dot(U, Dtb), lambda.map(inv))

    var step = 1.8/lambda[0]
    var iter = geniter(U, lambda, Dtb, step)

    var eigensum = d3.select("#poly2")

    var wi = lambda.slice(0).map(scal)

    function refit(b) {
      var Dtb = numeric.dot(b,D)
      iter = geniter(U, lambda, Dtb, step)
      onChange(sliderControl.slidera.xval())
    }

    var eigenControl = renderEigenPanel(eigensum, U, x, b, wi, refit, true)

    var barlengths = getStepsConvergence(lambda, step).map(Math.log)

    var onChange = function(i) {
      eigenControl.updateweights(numeric.dot(U,iter(Math.floor(Math.exp(i-0.1)) )))
    }

    var sliderControl = sliderBarGen(barlengths).update(onChange)(d3.select("#poly2"))

    d3.select("#poly2").append("figcaption")
          .style("width", "120px")
          .style("position", "absolute")
          .style("left", "820px")
          .style("top","200px")
          .html("When an eigenspace has converged to three significant digits, the bar greys out. Drag the observations to change fit.")

    sliderControl.slidera.init()

    var figwidth = d3.select("#poly2").style("width")
    var figheight = d3.select("#poly2").style("height")
    var svgannotate = d3.select("#poly2")
                        .append("svg")
                        .style("width", figwidth)
                        .style("height", figheight)
                        .style("position", "absolute")
                        .style("top","0px")
                        .style("left","0px")
                        .style("pointer-events","none")

    renderDraggable(svgannotate,
                    [139.88888549804688, 243.77951049804688],
                    [121.88888549804688, 200.77951049804688],
                    5,
                    "We begin at x=w=0");

    callback(null);
  });

  </script>
  <p>
  The effect of early stopping is very similar to that of more conventional methods of regularization, such as Tikhonov Regression. Both methods try to suppress the components of the smallest eigenvalues, though both through a employ a different method of spectral decay <dt-fn>In Tikhonov Regression we add a quadratic penalty to the regression, minimizing
$$
\text{minimize}\qquad\tfrac{1}{2}\|Zw-d\|^{2}+\eta\|w\|^{2}=\tfrac{1}{2}w^{T}(Z^{T}Z+\eta I)w-(Zd)^{T}w
$$
Recall that $Z^{T}Z=Q\text{diag}(\Lambda_{1},\ldots,\Lambda_{n})Q^T$. The solution to Tikhonov Regression is therefore
$$
(Z^{T}Z+\eta I)^{-1}(Zd)=Q\text{diag}\left(\frac{1}{\lambda_{1}+\eta},\cdots,\frac{1}{\lambda_{n}+\eta}\right)Q^T(Zd)
$$
Rewrite the eigenvalues of $(Z^{T}Z+\eta I)^{-1}$ as
$$
\frac{1}{\lambda_{i}+\eta}=\frac{1}{\lambda_{i}}\left(1-\left(1+\lambda_{i}/\eta\right)^{-1}\right).
$$
Now the term $1-\left(1+\lambda_{i}/\eta\right)^{-1}$ can be seen as a multiplicative decay which hits the smallest eigenvalues (those closest to $0$) the hardest. Gradient descent can be seen as employing a similar decay, but with the decay rate $1-\left(1-\alpha\lambda_{i}\right)^{k}$ instead. Note that this decay is dependent on the stepsize. For stepsizes
close to the upper limit of convergence, the smallest eigenvalue may converge faster (or at the same rate) as the largest. These stepsizes,
however, occupy a minuscule fraction of the set of acceptable stepsizes, and are almost never encountered in practice.
</dt-fn>. But early stopping has a distinct advantage. Once the stepsize is chosen, there are no regularization parameters to fiddle with. Indeed, in the course of a single optimization, we have the entire family of models, from underfitted to overfitted, at our disposal. This gift, it seems, doesn't seem to come at a price. A beautiful free lunch <dt-cite key="hintonNIPS"></dt-cite> indeed.
  </p>

<hr>
  <h2>The Dynamics of Momentum</h2>

  <p>
  Let's turn our attention back to momentum. Recall the momentum update is

  $$
  \begin{aligned}
  z^{k+1}&=\beta z^{k}+\nabla f(w^{k})\\
  w^{k+1}&=w^{k}-\alpha z^{k+1}.
  \end{aligned}
  $$

  Since $\nabla f(w^k) = Ax - b$, the update on the quadratic is

  $$
  \begin{aligned}
  z^{k+1}&=\beta z^{k}+ (Aw^{k}-b)\\
  w^{k+1}&=w^{k}-\alpha z^{k+1}
  \end{aligned}
  $$

  following <dt-cite key="o2015adaptive"></dt-cite>, we go through the same motions, with the change of variables $
  x^{k} = Q(w^{k} - w^\star)$ and $ y^{k} = Qz^{k}$ to yield the update rule

  $$
  \begin{aligned}
  y_{i}^{k+1}&=\beta y_{i}^{k}+\lambda_{i}x_{i}^{k}\\
  x_{i}^{k+1}&=x_{i}^{k}-\alpha y_{i}^{k+1}.
  \end{aligned}
  $$

  in which each of the components of $x$ and $y$ act independently of the other coordinates (though $x_i$ and $y_i$ are coupled). And therefore, we can rewrite our iterates as

  <dt-fn>
  This is true as we can write updates in matrix form as
  $$
  \left(\!\!\begin{array}{cc}
  1 & 0\\
  \alpha & 1
  \end{array}\!\!\right)\Bigg(\!\!\begin{array}{c}
  y_{i}^{k+1}\\
  x_{i}^{k+1}
  \end{array}\!\!\Bigg)=\left(\!\!\begin{array}{cc}
  \beta & \lambda_{i}\\
  0 & 1
  \end{array}\!\!\right)\left(\!\!\begin{array}{c}
  y_{i}^{k}\\
  x_{i}^{k}
  \end{array}\!\!\right)

  $$
  which implies, by inverting the matrix on the left,
  $$
  \Bigg(\!\!\begin{array}{c}
  y_{i}^{k+1}\\
  x_{i}^{k+1}
  \end{array}\!\!\Bigg)=\left(\!\!\begin{array}{cc}
  \beta & \lambda_{i}\\
  -\alpha\beta & 1-\alpha\lambda_{i}
  \end{array}\!\!\right)\left(\!\!\begin{array}{c}
  y_{i}^{k}\\
  x_{i}^{k}
  \end{array}\!\!\right)=R^{k+1}\left(\!\!\begin{array}{c}
  x_{i}^{0}\\
  y_{i}^{0}
  \end{array}\!\!\right)
  $$

  </dt-fn>
  $$
  \left(\!\!\begin{array}{c}
  y_{i}^{k}\\
  x_{i}^{k}
  \end{array}\!\!\right)=R^k\left(\!\!\begin{array}{c}
  y_{i}^{0}\\
  x_{i}^{0}
  \end{array}\!\!\right)
  \qquad
  R = \left(\!\!\begin{array}{cc}
  \beta & \lambda_{i}\\
  -\alpha\beta & 1-\alpha\lambda_{i}
  \end{array}\!\!\right).
  $$
  We are almost there. We need a second trick. There are many ways of taking a matrix to the $k^{th}$ power. But for the $2 \times 2$ case there is an elegant and little known formula <dt-cite key="williamsnthpower"></dt-cite> in terms of the eigenvectors of $R$, $\sigma_1$ and $\sigma_2$.

  $$
\color{#AAA}{\color{black}{R^{k}}=\begin{cases}
\color{black}{\sigma_{1}^{k}}R_{1}-\color{black}{\sigma_{2}^{k}}R_{2} & \sigma_{1}\neq\sigma_{2}\\
\sigma_{1}^{k}(kR/\sigma_1-(k-1)I) & \sigma_{1}=\sigma_{2}
\end{cases},\qquad R_{j}=\frac{R-\sigma_{j}I}{\sigma_{1}-\sigma_{2}}}
  $$

   This formula may be a lot to take in, but I've highlighted the important bits. This formula plays the exact same role the individual convergence rates, $1-\alpha\lambda_i$ do in gradient descent. But instead of one geometric series, we have two, which may have real or complex values. And since we need both $\sigma_1$ and $\sigma_2$ to converge, our convergence criteria is now

   $$\max \{|\sigma_1|, |\sigma_2|\} < 1.$$

   The exact values of $\alpha$ and $\beta$ needed to achieve this is complicated. But by plotting $\max \{|\sigma_1|, |\sigma_2|\}$ <dt-cite key="flammarion2015averaging"></dt-cite>, we see that there are distinct regions of the parameter space which reveal a rich taxonomy of convergence behavior
  </p>

  <figure id = "momentum2D" style="width:984px; height:540px">
    <div class = "l-body" style="display:block">
      <div id = "momentumCanvas" style="position:absolute; left:45px"></div>
      <div id = "momentumAnnotation" style="position:absolute; width: 204px; height: 80px; left: 630px; top: 20px;"></div>
      <div style="position:absolute; width: 204px; height: 80px; left: 643px; top: 10px;" class ="figtext" >
        Convergence Rate

      </div>

      <figcaption style="position:absolute; width: 204px; height: 80px; left: 645px; top: 86px;">
      A plot of $\max{\sigma_1, \sigma_2}$ reveals distinct regions, each with its own style of convergence.
      </figcaption>

    </div>
    <div id = "taxonomy"></div>
    <svg id="momentumOverlay" style="position:absolute; width:984px; height:540px; z-index:4; pointer-events:none"></svg>
  </figure>
  <script src="assets/momentum.js"></script>
  <script>
  deleteQueue.push(renderLoading(d3.select("#momentum2D")))    
  renderQueue.push(function(callback) {
    var defaults = [[0.0015, 0.9],
                    [0.0015, 0.125],
                    [0.01, 0.00001],
                    [0.02, 0.05   ],
                    [0.025, 0.235 ]]

    coor = render2DSliderGen(
      function(a,b,bold) {
        var xy = coor(a,b)
        updatePaths[0](xy[0], xy[1],bold)
        updateStemGraphs[0](a,b)
      },
      function(a,b,bold) {
        var xy = coor(a,b)
        updatePaths[1](xy[0], xy[1],bold)
        updateStemGraphs[1](a,b)
      },
      function(a,b,bold) {
        var xy = coor(a,b)
        updatePaths[2](xy[0], xy[1],bold)
        updateStemGraphs[2](a,b)
      },
      function(a,b,bold) {
        var xy = coor(a,b)
        updatePaths[3](xy[0], xy[1],bold)
        updateStemGraphs[3](a,b)
      },
      function(a,b,bold) {
        var xy = coor(a,b)
        updatePaths[4](xy[0], xy[1],bold)
        updateStemGraphs[4](a,b)
      }, defaults)(d3.select("#momentumCanvas"))

    var tax = renderTaxonomy(d3.select("#momentum2D"))

    var updatePaths = renderOverlay(d3.select("#momentumOverlay"), tax.div)
    var updateStemGraphs = tax.update

    colorMap(
    d3.select("#momentumAnnotation"),
    180,
    d3.scaleLinear().domain([0,0.3,0.5,0.7,1,1.01]).range(colorbrewer.Spectral[5].concat(["black"])),
    d3.scaleLinear().domain([0,1.2001]).range([0, 180])
    )

    var up = function (i, alpha, beta) {
              var xy = coor(alpha, beta)
              updatePaths[i](xy[0], xy[1], true)
              updateStemGraphs[i](alpha,beta)
            }

    for (var i = 0; i<5; i++) {
      up(i,defaults[i][0], defaults[i][1])
    }

    callback(null);
  });
  </script>
  <p>

  Notice an immediate boon we get. Momentum allows us to use a crank up the step-size up by a factor of 2 before diverging. But the true magic happens when we find the sweet spot of $\alpha$ and $\beta$.
  </p>
  <h3>The Critical Damping Coefficient</h3>

  <p>
  Let us try to first optimize over $\beta$. When $\alpha$ is small (e.g. in problems with pathological curvature), Momentum admits an interesting physical interpretation <dt-cite key="qian1999momentum"></dt-cite>. Consider a physical simulation operating, like in a video game, in discrete time.
  </p>

  <figure id = "momentum_annotations" style="width:530px; height:150px; display:block; margin-left:auto; margin-right:auto; position:relative">

  <div style="position:relative; top:-35px">

  <span style="position:absolute; left:0px; top:0px">
  $$y_{i}^{k+1}$$
  </span>

  <span style="position:absolute; left:160px; top:0px">
  $$=$$
  </span>


  <span style="position:absolute;left: 360px;top:0px;">
  $$+$$
  </span>

  <span style="position:absolute; left:410px; top:0px">
  $$\lambda_{i}x_{i}^{k}$$
  </span>

  <figcaption style="position:absolute;left:410px;top: 60px;width:150px;">
  and perturbed by an external force field
  </figcaption>

  <figcaption style="position:absolute;left:0px;top: 60px;width:100px;">
  We can think of $y_i^k$ as <b>velocity</b>
  </figcaption>

  <span style="position:absolute; left:200px; top:0px">
  $$\beta y_{i}^{k}$$
  </span>

  <figcaption style="position:absolute;left:200px;top: 60px;width:150px;">
  which is dampened at each step
  </figcaption>


  <span style="position:absolute;left: 0px;top: 100px;">
  $$x_i^{k+1}$$
  </span>

  <span style="position:absolute;left:160px;top: 100px;">
  $$=$$
  </span>

  <span style="position:absolute;left:200px;top: 100px;">
  $$x_i^k - \alpha y_i^{k+1}$$
  </span>

  <figcaption style="position:absolute;left:0px;top:155px;width:120px;">
  And $x$ is our particle's <b>position</b>
  </figcaption>

  <figcaption style="position:absolute;left:200px;top: 155px;width:300px;">
  which is moved at each step by a small amount in the direction of the velocity $y^{k+1}_i$.
  </figcaption>


  </div>
  </figure>

  <script>
  renderMath(document.getElementById("momentum_annotations"))
  </script>
  <p>
  Lets break apart what this equation means. For $150$ iterates, we plot the particle's velocity (the horizontal axis) against its position (the vertical axis), in a phase diagram.
  </p>

  <script src = "assets/phasediagram_description.js"></script>
  <figure id = "phasediagram0" style="position:relative; width:550px; height:720px;margin-left:auto; margin-right:auto">
  <div id = "phasediagram0div"; style="position:absolute; left:80px; top:10px">
  <figcaption style="width:200px; top:190px; position:absolute">
  $$
  \begin{aligned}
  z_i^{k+1}&=z_i^{k}\\
  w_i^{k+1}&=w_i^{k}- 0.01 z_i^{k+1},
  \end{aligned}
  $$
  When $\lambda_i = 0$ and $\beta=1$, this is Newton's first law. An object in motion stays in motion. The object moves at constant speed.
  </figcaption>

  <figcaption style="width:200px; left:260px; top:190px; position:absolute">
  $$
  \begin{aligned}
  z^{k+1}&= 0.95 z^{k}\\
  w^{k+1}&=w^{k}- 0.01 z^{k+1},
  \end{aligned}
  $$
  With a damping coefficient $\beta$ less than 1, the particle decelerates. $\beta$ is the amount of kinetic energy the particle loses at each tick.   </figcaption>


  <figcaption style="width:200px; top:530px; position:absolute">
  $$
  \begin{aligned}
  z_i^{k+1}&=z_i^{k}+ x_i^k\\
  w_i^{k+1}&=w_i^{k}- 0.01 z_i^{k+1},
  \end{aligned}
  $$

  Here the velocity is modiﬁed by an external force field. This force field varies in proportion to the particle’s distance from $0$. The object moves in a periodic trajectory. </figcaption>


  <figcaption style="width:200px; left:260px; top:530px; position:absolute">
  $$
  \begin{aligned}
  z_i^{k+1}&= 0.95 z_i^{k}+ x_i^k\\
  w_i^{k+1}&=w_i^{k}- 0.01 z_i^{k+1},
  \end{aligned}
  $$

  Combining damping and the force field, the particle behaves like a damped harmonic oscillator, returning lazily to equlibrium. </figcaption>
  <figcaption style="width:220px; left:80px; top:-13px; position:absolute">
  $\beta = 1$
  </figcaption>

  <figcaption style="width:220px; left:320px; top:-13px; position:absolute">
  $\beta = 0.95$
  </figcaption>

  <figcaption style="width:220px; left:-70px; top:88px; position:absolute">
  $\lambda_i = 0$
  </figcaption>

  <figcaption style="width:220px; left:-70px; top:440px; position:absolute">
  $\lambda_i = 1$
  </figcaption>
  </div>
  </figure>
  <script>
    deleteQueue.push(renderLoading(d3.select("#phasediagram0")))
    renderQueue.push(function(callback) {
      phaseDiagram_dec(d3.select("#phasediagram0div"))
      callback(null);
    });
  </script>
  <p>
  The momentum iteration, therefore, can be seen as a discretization of a damped harmonic oscilator. This system is best imagined as a weight suspended on a spring. We pull the weight down by one unit, and we study the path it goes as it returns to equilibrium. In the analogy, the spring is the source of our external force $\lambda_ix^k_i$, and equilibrium is the state when both $x^k_i$ and $y^k_i$ are 0. The choice of $\beta$ crucially affects the rate of return to equlibrium.
  </p>
  <figure id = "phasediagram1" style="position:relative; width:648px; height:490px; margin-left: auto; margin-right: auto"></figure>
  </div>
  <script src="assets/phasediagram.js"></script>
  <script>
    deleteQueue.push(renderLoading(d3.select("#phasediagram1")))  
    renderQueue.push(function(callback) {
      phaseDiagram(d3.select("#phasediagram1"));
      callback(null);
    });
  </script>
  <p>

  </p>
  <p>
  The critical $\beta = (1 - \sqrt{\alpha \lambda_i})^2$ gives us a convergence rate (in eigenspace $i$) of $1 - \sqrt{\alpha\lambda_i}.$ A square root improvement over gradient descent, $1-\alpha\lambda_i$! This is our first quadratic speedup. Alas, this only applies to the error in the $i^{th}$ eigenspace.
  </p>

  <h3>Optimal parameters</h3>
  <p>
  To get a global convergence rate, we must optimize over both $\alpha$ and $\beta$. This is a more complicated affair, <dt-fn> and involves an optimization problem over
  $$
  \min_{\alpha,\beta}\max\left\{ \bigg\| \! \left(\begin{array}{cc}
  \beta & \lambda_{i}\\
  -\alpha\beta & 1-\lambda_{i}
  \end{array}\right) \! \bigg\|,\ldots,\bigg\| \! \left(\begin{array}{cc}
  \beta & \lambda_{n}\\
  -\alpha\beta & 1-\lambda_{n}
  \end{array}\right)\! \bigg\|\right\}.
  $$
  ($\|\cdot \|$ here denotes the magnitude of the maximum eigenvalue). The optimum can be found with nothing more than high school algebra - and it occurs when the roots of the characteristic polynomial are repeated for the matrices corresponding to the extremal eigenvalues. </dt-fn>, so let's take it as a given that the critical point is
  $$
  \alpha = \left(\frac{2}{\sqrt{\lambda_{1}}+\sqrt{\lambda_{n}}}\right)^{2}  \quad \beta = \left(\frac{\sqrt{\lambda_{n}}-\sqrt{\lambda_{1}}}{\sqrt{\lambda_{n}}+\sqrt{\lambda_{1}}}\right)^{2}
  $$
  Plug this in, and you get
  </p>
  <figure id = "conv_rate_comparisons" style="width:530px; height:19px; display:block; margin-left:auto; margin-right:auto; position:relative">
  <div style="position:relative; top:-40px">

  <span style="position:absolute; left:16px">
  $$\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}$$
  </span>

  <figcaption style="position:absolute;left: 116px;top: 29px;width:130px;">
  Convergence rate, <b>Gradient Descent</b>
  </figcaption>

  <span style="position:absolute; left:313px">
  $$ \frac{\kappa-1}{\kappa+1}$$
  </span>


  <figcaption style="position:absolute;left: 393px;top: 29px;width:150px;">
  Convergence rate, <b>Momentum</b>
  </figcaption>
  </div>
  </figure>
  <script>
  renderMath(document.getElementById("conv_rate_comparisons"))
  </script>
  <p>
  With barely a modicum of extra effort, we have essentially square rooted the condition number! These gains, in principle, require explicit knowledge of $\lambda_1$ and $\lambda_n$. But the formulas reveal a simple guideline. For ill conditioned problems, set $\beta$ as close to $1$ as you can, and make the $\alpha$ as high as possible. Being at the knife's edge of divergence, like in gradient descent, is a good place to be.
  </p>

  <figure id="milestonesMomentumFig" style="position:relative; width:920px; height:460px">
  <figcaption style="position:absolute; text-align:left; top:30px; left:140px; width:280px; height:120px">We can do the same decomposition here with momentum, with eigenvalues <svg style="position:relative; top:2px; width:3px; height:14px; background:#fde0dd"></svg> $\lambda_1=1$, <svg style="position:relative; top:2px; width:3px; height:14px; background:#fa9fb5"></svg> $\lambda_2=10$, and <svg style="position:relative; top:2px; width:3px; height:14px; background:#c51b8a"></svg>. Though the decrease is no longer monotonic, but significantly faster. </figcaption>
  <figcaption style="position:absolute; text-align:left; top:210px; left:5px; width:280px; height:120px; font-size:15px"> $f(w) - f(w^\star)$</figcaption>
  <figcaption style="position:absolute; text-align:left; top:430px; left:120px; width:780px; height:120px">The time to convergence is not proportional to the convergence rate due to a small multiplicative constant.</figcaption>

  <div class="figtext" style="position:absolute; left:705px; top:11px">Step size α = </div>
  <div class="figtext" style="position:absolute; left:464px; top:30px">Momentum β = </div>

  <div id = "sliderStep2D" style="position:absolute; left:550px; width:250px; height:100px; top:10px"></div>
  <div id = "milestonesMomentum"></div>
  </figure>
  <script>
    deleteQueue.push(renderLoading(d3.select("#milestonesMomentumFig")))  
    renderQueue.push(function(callback) {
      var graphDiv = d3.select("#milestonesMomentum")
                   .style("width",  920 + "px")
                   .style("height", 300 + "px")
                   .style("top", "170px")
                   .style("position", "relative")
                   .style("margin-left", "auto")
                   .style("margin-right", "auto")
                   .attr("width", 920)
                   .attr("height", 500)

    var svg = graphDiv.append("svg").attr("width", 940).attr("height", 500)

    var update = renderMilestones(svg, function() {})
    var slidera = slider2D(d3.select("#sliderStep2D"), function(x,y) { update(x,y) }, 1, 100)

    // Swoopy Annotator
    var annotations = [
      {
        "x": 0,
        "y": 0,
        "path": "M 250,96 A 32.227 32.227 0 0 0 206,62",
        "text": "Optimal parameters",
        "textOffset": [
          188,
          109
        ]
      }
    ]

    var swoopy = d3.swoopyDrag()
      .x(function(d){ return (d.x) })
      .y(function(d){ return (d.y) })
        .draggable(false)
        .annotations(annotations)

    var svg = d3.select("#sliderStep2D").selectAll("svg")
    var swoopySel = svg.append("g").attr("class", "figtext").call(swoopy)

    svg.append('marker')
        .attr('id', 'arrow')
        .attr('viewBox', '-10 -10 20 20')
        .attr('markerWidth', 20)
        .attr('markerHeight', 20)
        .attr('orient', 'auto')
      .append('path')
        .attr('d', 'M-6.75,-6.75 L 0,0 L -6.75,6.75')
        .attr("transform", "scale(0.5)")

    swoopySel.selectAll('path').attr('marker-end', 'url(#arrow)');

    callback(null);
  });

  </script>
  <p>
  While loss function of gradient descent had a graceful, monotonic curve, optimization with momentum displays clear oscillations. These ripples are not restricted to quadratics, and occur in all kinds of functions in practice. They are not cause for alarm, but are an indication that extra tuning of the hyperparameters are required.
  </p>

  <h3>Example: The Colorization Problem</h3>

  <p>
  Lets look at how momentum accelerates convergence with a concrete example. On a grid of pixels let $G$ be the graph with vertices as pixels, $E$ be the set of edges connecting each pixel to its four neighboring pixels, and $D$ be a small set of a few distinguished vertices. Consider the problem of minimizing
  </p>
  <figure id = "colorizer_equation" style="width:530px; height:95px; display:block; margin-left:auto; margin-right:auto; position:relative">
  <div style="position:relative; top:-40px">
  <span style="position:absolute; top:10px">
  $$\text{minimize} $$
  </span>

  <span style="position:absolute; left:90px">
  $$\qquad  \frac{1}{2} \sum_{i\in D} (w_i - 1)^2 $$
  </span>

  <figcaption style="position:absolute; left:140px; top:100px; width:130px">
  The <b>colorizer</b> pulls distinguished pixels towards 1
  </figcaption>

  <span style="position:absolute; left:310px; top:10px">
  $$+$$
  </span>

  <span style="position:absolute; left:350px">
  $$\frac{1}{2} \sum_{i,j\in E} (w_i - w_j)^2.$$
  </span>


  <figcaption style="position:absolute; left:353px; top:100px; width:150px">
  The <b>smoother</b> spreads out the color
  </figcaption>
  </div>
  </figure>
  <script>
  renderMath(document.getElementById("colorizer_equation"))
  </script>
  <p>
  The above optimization problem is bounded from below by $0$, and thus the optimal solution is the vector of all $1$'s. A simple inspection of the gradient iteration reveals we take a long time to get there. The gradient step, for each component, is some form of weighted average of the current value and its neighbors:

  $$
w_{i}^{k+1}=w_{i}^{k}-\alpha\sum_{j\in N}(w_{i}^{k}-w_{j}^{k})-\begin{cases}
\alpha(w_{i}^{k}-1) & i\in D\\
0 & i\notin D
\end{cases}
  $$

  This kind of local averaging is effective at smoothing out local variations in the function value, but poor at taking advantage of global structure. The updates are akin to a drop of ink, diffusing through water. Movement through equilibrium is made only through local corrections - and hence left undisturbed, its march towards equilibrium is slow and laborious.
  </p>
  <figure id = "flow" style="position: relative;display: block;margin-left: auto;margin-right: auto;width: 920px;height: 705px;">

  <figcaption style="left:790px; top:430px; position:absolute; width:130px">
      The eigenvectors of the colorization problem form a generalized Fourier basis for $R^n$. The smallest eigenvalues have low frequencies, hence gradient descent corrects high frequency errors well but not low frequency ones.
  </figcaption>

  </figure>
  <script src="assets/flow.js"></script>
  <script>
    deleteQueue.push(renderLoading(d3.select("#flow")))    
    renderQueue.push(function(callback) {
      d3.queue()
        .defer(d3.json, "assets/data/Sigma.json")
        .defer(d3.json, "assets/data/matrix.json")
        .defer(d3.json, "assets/data/Uval.json")
        .await(function(error, FlowSigma, M, FlowU) {
          if (error) {
            console.error("Error loading data files");
          }
          else {
            renderFlowWidget(d3.select("#flow"), FlowSigma, M, FlowU)
          }
        });
      callback(null);
    });
  </script>
  <p>
  The acceleration that momentum provides is clear, creating a massive wavefront in the sequence, allowing the system to converge must faster.
  </p>
  <h3>Optimization on Graph Laplacians</h3>
  <p>
  In the colorization problem, the smoother, which defines the couplings between vertices, dominates the behavior of the function. We can define a slightly more general smoother on a $G=(V,E)$,
  $$
  w \mapsto \frac{1}{2} \sum_{i,j\in E} (w_i - w_j)^2 = \frac{1}{2}w^T L_G w.
  $$
  The matrix $L_G$ is the Laplacian matrix
  <dt-fn>This can be written explicitly as
  $$
  [L_{G}]_{ij}=\begin{cases} \text{degree of vertex }i & i=j\\ -1 & i\neq j,(i,j)\text{ or }(j,i)\in E\\ 0 & \text{otherwise} \end{cases}
  $$
  </dt-fn>.
  The study of Laplacians and their quadratic forms form a rich field of mathematics, which relate the linear algebraic properties of the matrix $L_G$ and the graph $G$.
  </p>

  <p>
  One fact is pertinent to our discussion here. The conditioning of $L_G$, here defined as the ratio of the second eigenvector to the last (the first eigenvector is always 0), is directly connected to the connectivity of the graph.
  </p>
  <figure id="laplacianConditioning" style="width:900px; height:265px">
  <svg width="900" height="300" id="graph"></svg>
  <figcaption id = "expander" style="position:absolute; left:70px; top:210px; width:200px">Small world graphs, like expanders and dense graphs, have excellent conditioning</figcaption>

  <figcaption id = "expander" style="position:absolute; left:370px; top:210px; width:200px">The conditioning of grids improves with its dimensionality.</figcaption>

  <figcaption id = "expander" style="position:absolute; left:670px; top:210px; width:200px">And long, wiry graphs, like paths, condition poorly. </figcaption>

  </figure>
  <script src="assets/graph.js"></script>
  <script>
    deleteQueue.push(renderLoading(d3.select("#laplacianConditioning")))    
    renderQueue.push(function(callback) {
      var g1 = d3.select("#graph").append("g")
      genExpander(g1)

      var g2 = d3.select("#graph").append("g").attr("transform", "translate(300,0)")
      genGrid(g2)

      var g3 = d3.select("#graph").append("g").attr("transform", "translate(600,0)")
      genPath(g3);
      callback(null);
    });
  </script>
  <p>

  The intuition behind this should be clear. Well connected graphs allow rapid diffusion of information through the edges, while graphs with poor connectivity do not. And this principle, taken to the extreme, furnish a class of functions so hard to optimize they reveal the limits of the first order optimization.
  </p>
<hr>
  <h2>
  The Resisting Oracle
  </h2>
  <p>
  If a single auxiliary sequence provides a quadratic speedup, what would two sequences give? Could one perhaps choose the $\alpha$'s and $\beta$'s intelligently and adaptively to do even better? It is tempting to ride this wave of optimism - to the cube root and beyond! Now, improvements to the momentum algorithm do exist - but they all run up to a certain, critical, almost inescapable lower bound.
  </p>
  <h3>Adventures in Algorthmic Space</h3>
  <p>
  To understand the limits of what we can do, we must first formally define the algorithmic space in which we are searching. Here's one possible definition. The observation we will make is that both Gradient Descent and momentum can be "unrolled". Indeed, since
  $$
  \begin{array}{lll}
    w^{1} & \!= & \!w^{0} ~-~ \alpha\nabla f(w^{0})\\[0.35em]
    w^{2} & \!= & \!w^{1} ~-~ \alpha\nabla f(w^{1})\\[0.35em]
          & \!= & \!w^{0} ~-~ \alpha\nabla f(w^{0}) ~-~ \alpha\nabla f(w^{1})\\[0.35em]
     & ~ \!\vdots \\

   w^{k} & \!= & \!w^{0} ~-~ \alpha\nabla f(w^{0}) ~-~~~~ \cdots\cdots ~~~~-~ \alpha\nabla f(w^{k})
  \end{array}
  $$
  we can write gradient descent as:

  $$
  w^{k} ~~=~~ w^{0} ~-~ \alpha\sum\nabla f(w^{i})
  $$

  A similar trick can be done with momentum,

  $$
  w^{k} ~~=~~ w^{0} ~+~ \alpha\sum\frac{(1-\beta^{k})}{1-\beta}\nabla f(w^{k})
  $$

  In fact, all manner of first order algorithms, including the Conjugate Gradient algorithm, AdaMax, Averaged Gradient and more can be written in (though not quite so neatly) this unrolled form. Therefore the class of algorithms for which

  $$
  w^{k} ~~=~~ w^{0} ~+~ \sum_{i}^{k}\gamma_{i}^{k}\nabla f(w^{i}) \qquad \text{ for some } \gamma_{i}^{k}
  $$

  contains momentum, gradient descent and a whole bunch of other iterations you might dream up. This is what is assumed in Assumption 2.1.4 <dt-cite key="nesterov2013introductory"></dt-cite> of Nesterov. But let's push this even further, and expand this class to allow different step sizes for different directions.

  $$
  w^{k} ~~=~~ w^{0} ~+~ \sum_{i}^{k}\Gamma_{i}^{k}\nabla f(w^{i}) \quad \text{ for some diagonal matrix } \Gamma_{i}^{k} .
  $$

  This class of methods has now covers most of the popular algorithms for training neural networks, including ADAM and Adagrad. We shall refer to this class of methods as "Linear First Order Methods", and we will show a single function all these methods must ultimately fail on.

  </p>
  <h3>The Convex Rosenbrock</h3>
  <p>
  Earlier, when we talked about the colorizer problem, we observed that wirey graphs cause bad conditioning in our optimization problem. Taking this to its extreme, we can look at a graph consisting of a single path -- a function so badly conditioned that Nesterov called a variant of it "the worst function in the world". The function follows the same structure as the colorizer problem,

  $$
  f^n(w) = \frac{\kappa-1}{8}\left(w_{1}-1\right)^{2}+\frac{\kappa-1}{8}\sum_{i=1}^{n}(w_{i}-w_{i+1})^{2}+\frac{1}{2}\|w\|^{2}
  $$

  except with a small regularization term at the end. It contains strong couplings between the variables (proportional to $\kappa$), and a long path from $w_1$ to $w_n$ - as discussed earlier, a sign of a trench of bad curvature. Let's write out some facts about this function.
  </p>

  <p>
  The optimal solution of this problem is

  $$
  w_{i}^{\star}=\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^{i}
  $$

  and the condtion number of the problem $f^n$ approaches $\kappa$ as $n$ goes to infinity. Now observe the behavior of the momentum algorithm on this function, starting from $w^0 = 0$.
  </p>

  <figure id="rosenViz" style="width:950px; height:780px">
    <div id = "sliderz" style="position:absolute;top: 14px;left:575px;"></div>
    <div class="figtext" style="position:absolute; pointer-events:none; top:14px; width:300px; left:729px; height:100px">Step size α = </div>
    <div class="figtext" style="position:absolute; pointer-events:none; top:35px; width:488px; left:488px; height:100px">Momentum β = </div>

    <figcaption style="width:270px; position:absolute; left:180px; top:50px">Here we see the first 70 iterates of momentum on the Convex Rosenbrock for $n=25$. The behavior here is similar to that of any Linear First Order Algorithm. </figcaption>

    <div style="position:absolute; top:150px; left:-80px">
    <figcaption style="width:120px; position:absolute; left:670px; top:90px">Notice that this triangle is a "dead zone" of our iterates. The iterates are always 0, no matter what the parameters. </figcaption>
    <figcaption style="width:120px; position:absolute; left:670px; top:360px">The remaining expanding space is the "light cone" of our iterate's influence. Momentum does very well here with the optimal parameters. </figcaption>
    <div id = "iterates" style="position:absolute; top:60px; left:0"></div>
    <div class="figtext" style="position:absolute; top:5px; width:300px; left:460px; height:100px">Error</div>
    <div id = "rosen_colorbar1" style="position:absolute; top:5px; width:300px; left:448px; height:100px"></div>
    <div class="figtext" style="position:absolute; top:5px; width:300px; left:240px; height:100px">Weights</div>
    <div id = "rosen_colorbar2" style="position:absolute; top:5px; width:300px; left:228px; height:100px"></div>
    </div>
  </figure>
  <script>
  deleteQueue.push(renderLoading(d3.select("#rosenViz")))    
  renderQueue.push(function(callback) {
    var RQ = [[0.033430859446525574,0.06637421995401382,0.09834969788789749,0.1288910210132599,0.1575528085231781,0.18391713500022888,0.20759953558444977,0.22825466096401215,0.24558131396770477,0.25932684540748596,0.2692908048629761,0.2753278911113739,0.2773500978946686,0.2753278911113739,0.2692908048629761,0.25932684540748596,0.24558131396770477,0.22825466096401215,0.20759953558444977,0.18391713500022888,0.1575528085231781,0.1288910210132599,0.09834969788789749,0.06637421995401382,0.033430859446525574],[-0.06637421995401382,-0.1288910210132599,-0.18391713500022888,-0.22825466096401215,-0.25932684540748596,-0.2753278911113739,-0.2753278911113739,-0.25932684540748596,-0.22825466096401215,-0.18391713500022888,-0.1288910210132599,-0.06637421995401382,-1.917815576756455e-14,0.06637421995401382,0.1288910210132599,0.18391713500022888,0.22825466096401215,0.25932684540748596,0.2753278911113739,0.2753278911113739,0.25932684540748596,0.22825466096401215,0.18391713500022888,0.1288910210132599,0.06637421995401382],[0.09834969788789749,0.18391713500022888,0.24558131396770477,0.2753278911113739,0.2692908048629761,0.22825466096401215,0.1575528085231781,0.06637421995401382,-0.033430859446525574,-0.1288910210132599,-0.20759953558444977,-0.25932684540748596,-0.2773500978946686,-0.25932684540748596,-0.20759953558444977,-0.1288910210132599,-0.033430859446525574,0.06637421995401382,0.1575528085231781,0.22825466096401215,0.2692908048629761,0.2753278911113739,0.24558131396770477,0.18391713500022888,0.09834969788789749],[0.1288910210132599,0.22825466096401215,0.2753278911113739,0.25932684540748596,0.18391713500022888,0.06637421995401382,-0.06637421995401382,-0.18391713500022888,-0.25932684540748596,-0.2753278911113739,-0.22825466096401215,-0.1288910210132599,-1.2834394835694717e-14,0.1288910210132599,0.22825466096401215,0.2753278911113739,0.25932684540748596,0.18391713500022888,0.06637421995401382,-0.06637421995401382,-0.18391713500022888,-0.25932684540748596,-0.2753278911113739,-0.22825466096401215,-0.1288910210132599],[-0.1575528085231781,-0.25932684540748596,-0.2692908048629761,-0.18391713500022888,-0.033430859446525574,0.1288910210132599,0.24558131396770477,0.2753278911113739,0.20759953558444977,0.06637421995401382,-0.09834969788789749,-0.22825466096401215,-0.2773500978946686,-0.22825466096401215,-0.09834969788789749,0.06637421995401382,0.20759953558444977,0.2753278911113739,0.24558131396770477,0.1288910210132599,-0.033430859446525574,-0.18391713500022888,-0.2692908048629761,-0.25932684540748596,-0.1575528085231781],[-0.18391713500022888,-0.2753278911113739,-0.22825466096401215,-0.06637421995401382,0.1288910210132599,0.25932684540748596,0.25932684540748596,0.1288910210132599,-0.06637421995401382,-0.22825466096401215,-0.2753278911113739,-0.18391713500022888,-4.583645525716709e-15,0.18391713500022888,0.2753278911113739,0.22825466096401215,0.06637421995401382,-0.1288910210132599,-0.25932684540748596,-0.25932684540748596,-0.1288910210132599,0.06637421995401382,0.22825466096401215,0.2753278911113739,0.18391713500022888],[-0.20759953558444977,-0.2753278911113739,-0.1575528085231781,0.06637421995401382,0.24558131396770477,0.25932684540748596,0.09834969788789749,-0.1288910210132599,-0.2692908048629761,-0.22825466096401215,-0.033430859446525574,0.18391713500022888,0.2773500978946686,0.18391713500022888,-0.033430859446525574,-0.22825466096401215,-0.2692908048629761,-0.1288910210132599,0.09834969788789749,0.25932684540748596,0.24558131396770477,0.06637421995401382,-0.1575528085231781,-0.2753278911113739,-0.20759953558444977],[-0.22825466096401215,-0.25932684540748596,-0.06637421995401382,0.18391713500022888,0.2753278911113739,0.1288910210132599,-0.1288910210132599,-0.2753278911113739,-0.18391713500022888,0.06637421995401382,0.25932684540748596,0.22825466096401215,4.337912797388764e-15,-0.22825466096401215,-0.25932684540748596,-0.06637421995401382,0.18391713500022888,0.2753278911113739,0.1288910210132599,-0.1288910210132599,-0.2753278911113739,-0.18391713500022888,0.06637421995401382,0.25932684540748596,0.22825466096401215],[-0.24558131396770477,-0.22825466096401215,0.033430859446525574,0.25932684540748596,0.20759953558444977,-0.06637421995401382,-0.2692908048629761,-0.18391713500022888,0.09834969788789749,0.2753278911113739,0.1575528085231781,-0.1288910210132599,-0.2773500978946686,-0.1288910210132599,0.1575528085231781,0.2753278911113739,0.09834969788789749,-0.18391713500022888,-0.2692908048629761,-0.06637421995401382,0.20759953558444977,0.25932684540748596,0.033430859446525574,-0.22825466096401215,-0.24558131396770477],[-0.25932684540748596,-0.18391713500022888,0.1288910210132599,0.2753278911113739,0.06637421995401382,-0.22825466096401215,-0.22825466096401215,0.06637421995401382,0.2753278911113739,0.1288910210132599,-0.18391713500022888,-0.25932684540748596,-2.6265288907570752e-15,0.25932684540748596,0.18391713500022888,-0.1288910210132599,-0.2753278911113739,-0.06637421995401382,0.22825466096401215,0.22825466096401215,-0.06637421995401382,-0.2753278911113739,-0.1288910210132599,0.18391713500022888,0.25932684540748596],[0.2692908048629761,0.1288910210132599,-0.20759953558444977,-0.22825466096401215,0.09834969788789749,0.2753278911113739,0.033430859446525574,-0.25932684540748596,-0.1575528085231781,0.18391713500022888,0.24558131396770477,-0.06637421995401382,-0.2773500978946686,-0.06637421995401382,0.24558131396770477,0.18391713500022888,-0.1575528085231781,-0.25932684540748596,0.033430859446525574,0.2753278911113739,0.09834969788789749,-0.22825466096401215,-0.20759953558444977,0.1288910210132599,0.2692908048629761],[0.2753278911113739,0.06637421995401382,-0.25932684540748596,-0.1288910210132599,0.22825466096401215,0.18391713500022888,-0.18391713500022888,-0.22825466096401215,0.1288910210132599,0.25932684540748596,-0.06637421995401382,-0.2753278911113739,-2.224606886845463e-15,0.2753278911113739,0.06637421995401382,-0.25932684540748596,-0.1288910210132599,0.22825466096401215,0.18391713500022888,-0.18391713500022888,-0.22825466096401215,0.1288910210132599,0.25932684540748596,-0.06637421995401382,-0.2753278911113739],[0.2773500978946686,1.2625047254649028e-16,-0.2773500978946686,-6.312523627324514e-16,0.2773500978946686,8.83753307825432e-16,-0.2773500978946686,-8.83753307825432e-16,0.2773500978946686,1.5781309068311285e-15,-0.2773500978946686,-4.734392720493385e-16,0.2773500978946686,2.0200075607438445e-15,-0.2773500978946686,-1.7990691808479273e-15,0.2773500978946686,1.6728188141805554e-15,-0.2773500978946686,-1.4518804342846382e-15,0.2773500978946686,-3.4718879950284827e-16,-0.2773500978946686,1.2625047254649028e-15,0.2773500978946686],[-0.2753278911113739,0.06637421995401382,0.25932684540748596,-0.1288910210132599,-0.22825466096401215,0.18391713500022888,0.18391713500022888,-0.22825466096401215,-0.1288910210132599,0.25932684540748596,0.06637421995401382,-0.2753278911113739,-1.0026396759284933e-15,0.2753278911113739,-0.06637421995401382,-0.25932684540748596,0.1288910210132599,0.22825466096401215,-0.18391713500022888,-0.18391713500022888,0.22825466096401215,0.1288910210132599,-0.25932684540748596,-0.06637421995401382,0.2753278911113739],[0.2692908048629761,-0.1288910210132599,-0.20759953558444977,0.22825466096401215,0.09834969788789749,-0.2753278911113739,0.033430859446525574,0.25932684540748596,-0.1575528085231781,-0.18391713500022888,0.24558131396770477,0.06637421995401382,-0.2773500978946686,0.06637421995401382,0.24558131396770477,-0.18391713500022888,-0.1575528085231781,0.25932684540748596,0.033430859446525574,-0.2753278911113739,0.09834969788789749,0.22825466096401215,-0.20759953558444977,-0.1288910210132599,0.2692908048629761],[-0.25932684540748596,0.18391713500022888,0.1288910210132599,-0.2753278911113739,0.06637421995401382,0.22825466096401215,-0.22825466096401215,-0.06637421995401382,0.2753278911113739,-0.1288910210132599,-0.18391713500022888,0.25932684540748596,1.770693656524247e-16,-0.25932684540748596,0.18391713500022888,0.1288910210132599,-0.2753278911113739,0.06637421995401382,0.22825466096401215,-0.22825466096401215,-0.06637421995401382,0.2753278911113739,-0.1288910210132599,-0.18391713500022888,0.25932684540748596],[0.24558131396770477,-0.22825466096401215,-0.033430859446525574,0.25932684540748596,-0.20759953558444977,-0.06637421995401382,0.2692908048629761,-0.18391713500022888,-0.09834969788789749,0.2753278911113739,-0.1575528085231781,-0.1288910210132599,0.2773500978946686,-0.1288910210132599,-0.1575528085231781,0.2753278911113739,-0.09834969788789749,-0.18391713500022888,0.2692908048629761,-0.06637421995401382,-0.20759953558444977,0.25932684540748596,-0.033430859446525574,-0.22825466096401215,0.24558131396770477],[0.22825466096401215,-0.25932684540748596,0.06637421995401382,0.18391713500022888,-0.2753278911113739,0.1288910210132599,0.1288910210132599,-0.2753278911113739,0.18391713500022888,0.06637421995401382,-0.25932684540748596,0.22825466096401215,9.870699398085384e-16,-0.22825466096401215,0.25932684540748596,-0.06637421995401382,-0.18391713500022888,0.2753278911113739,-0.1288910210132599,-0.1288910210132599,0.2753278911113739,-0.18391713500022888,-0.06637421995401382,0.25932684540748596,-0.22825466096401215],[-0.20759953558444977,0.2753278911113739,-0.1575528085231781,-0.06637421995401382,0.24558131396770477,-0.25932684540748596,0.09834969788789749,0.1288910210132599,-0.2692908048629761,0.22825466096401215,-0.033430859446525574,-0.18391713500022888,0.2773500978946686,-0.18391713500022888,-0.033430859446525574,0.22825466096401215,-0.2692908048629761,0.1288910210132599,0.09834969788789749,-0.25932684540748596,0.24558131396770477,-0.06637421995401382,-0.1575528085231781,0.2753278911113739,-0.20759953558444977],[-0.18391713500022888,0.2753278911113739,-0.22825466096401215,0.06637421995401382,0.1288910210132599,-0.25932684540748596,0.25932684540748596,-0.1288910210132599,-0.06637421995401382,0.22825466096401215,-0.2753278911113739,0.18391713500022888,8.371955066048889e-16,-0.18391713500022888,0.2753278911113739,-0.22825466096401215,0.06637421995401382,0.1288910210132599,-0.25932684540748596,0.25932684540748596,-0.1288910210132599,-0.06637421995401382,0.22825466096401215,-0.2753278911113739,0.18391713500022888],[-0.1575528085231781,0.25932684540748596,-0.2692908048629761,0.18391713500022888,-0.033430859446525574,-0.1288910210132599,0.24558131396770477,-0.2753278911113739,0.20759953558444977,-0.06637421995401382,-0.09834969788789749,0.22825466096401215,-0.2773500978946686,0.22825466096401215,-0.09834969788789749,-0.06637421995401382,0.20759953558444977,-0.2753278911113739,0.24558131396770477,-0.1288910210132599,-0.033430859446525574,0.18391713500022888,-0.2692908048629761,0.25932684540748596,-0.1575528085231781],[-0.1288910210132599,0.22825466096401215,-0.2753278911113739,0.25932684540748596,-0.18391713500022888,0.06637421995401382,0.06637421995401382,-0.18391713500022888,0.25932684540748596,-0.2753278911113739,0.22825466096401215,-0.1288910210132599,-3.0802549003271684e-16,0.1288910210132599,-0.22825466096401215,0.2753278911113739,-0.25932684540748596,0.18391713500022888,-0.06637421995401382,-0.06637421995401382,0.18391713500022888,-0.25932684540748596,0.2753278911113739,-0.22825466096401215,0.1288910210132599],[-0.09834969788789749,0.18391713500022888,-0.24558131396770477,0.2753278911113739,-0.2692908048629761,0.22825466096401215,-0.1575528085231781,0.06637421995401382,0.033430859446525574,-0.1288910210132599,0.20759953558444977,-0.25932684540748596,0.2773500978946686,-0.25932684540748596,0.20759953558444977,-0.1288910210132599,0.033430859446525574,0.06637421995401382,-0.1575528085231781,0.22825466096401215,-0.2692908048629761,0.2753278911113739,-0.24558131396770477,0.18391713500022888,-0.09834969788789749],[-0.06637421995401382,0.1288910210132599,-0.18391713500022888,0.22825466096401215,-0.25932684540748596,0.2753278911113739,-0.2753278911113739,0.25932684540748596,-0.22825466096401215,0.18391713500022888,-0.1288910210132599,0.06637421995401382,6.798085926082532e-17,-0.06637421995401382,0.1288910210132599,-0.18391713500022888,0.22825466096401215,-0.25932684540748596,0.2753278911113739,-0.2753278911113739,0.25932684540748596,-0.22825466096401215,0.18391713500022888,-0.1288910210132599,0.06637421995401382],[0.033430859446525574,-0.06637421995401382,0.09834969788789749,-0.1288910210132599,0.1575528085231781,-0.18391713500022888,0.20759953558444977,-0.22825466096401215,0.24558131396770477,-0.25932684540748596,0.2692908048629761,-0.2753278911113739,0.2773500978946686,-0.2753278911113739,0.2692908048629761,-0.25932684540748596,0.24558131396770477,-0.22825466096401215,0.20759953558444977,-0.18391713500022888,0.1575528085231781,-0.1288910210132599,0.09834969788789749,-0.06637421995401382,0.033430859446525574]]

  RLambda = [4.6419172286987305,15.514562606811523,33.45938491821289,58.21471405029297,89.4195556640625,126.61888122558594,169.27023315429688,216.7516632080078,268.37078857421875,323.3748474121094,380.9618225097656,440.29193115234375,500.5,560.7080688476562,620.0381469726562,677.6251220703125,732.6292114257812,784.2483520507812,831.7297973632812,874.381103515625,911.5804443359375,942.7852783203125,967.5405883789062,985.4854125976562,996.3580932617188]

  var alpha = 0.003
  var beta = 0.8
  var b = zeros(25); b[0] =  249.75
  var iter = geniterMomentum(RQ, RLambda, b, alpha, beta).iter
  var res = function(i) { return iter(i)[0] }

  var sampleSVG = d3.select("#iterates")
    .style("display", "block")
      .append("svg")
      .attr("width", 770)
      .attr("height", 700)

  var rosen = d3.scaleLinear().domain([0,0.2,0.5,1.4,5]).range(colorbrewer.RdPu[5]);
  var jet = d3.scaleLinear().domain([-0.5,0,0.2,0.5,1.0,5]).range(colorbrewer.Spectral[6]);
  var contrast = d3.scaleLinear().domain([-12,0,12]).range(colorbrewer.RdBu[3]);

  var xstar = iter(10000000)
  var numIters = 72


  var Disps3 = []
  var errorPlot = sampleSVG.append("g").attr("transform", "translate(" + 440 + ",0)")
  for (var j = 0; j < numIters; j++) {
    var disp = errorPlot.append("g")

    var denter = disp.selectAll("rect")
        .data(iter(j)[1])
        .enter()

    denter.append("rect")
        .style("fill", function(d,i) { return rosen(d) })
        .attr("height", 7)
        .attr("width", 7.7)
        .attr("x", function(d, i){return i*8+ 10})
        .attr("y", function(d, i){return j*8 })
    Disps3.push(disp)
  }



  var Disps4 = []
  var errorPlot = sampleSVG.append("g").attr("transform", "translate(" + 220 + ",0)")
  for (var j = 0; j < numIters; j++) {
    var disp = errorPlot.append("g")

    var denter = disp.selectAll("rect")
        .data(iter(j)[1])
        .enter()

    denter.append("rect")
        .style("fill", function(d,i) { return jet(d) })
        .attr("height", 7)
        .attr("width", 7.7)
        .attr("x", function(d, i){return i*8+ 10})
        .attr("y", function(d, i){return j*8 })
        // .attr("stroke", "black")
        // .attr("stroke-width", function(d,i) { return (i == j) ? 1.5 : 0 })
        // .attr("stroke-dasharray", "14.8, 14")

    if ((j % 10 == 0) || j == 0) {
    disp.append("text")
        .attr("class", "figtext")
        .attr("text-anchor", "end")
        .attr("x", 0)
        .attr("y", function(d, i){return (j*8 + 10) })
        .html( (j == 0) ? "Iteration 0" : "" + j)
    }

    Disps4.push(disp)
  }
  function update(alpha, beta) {
    var iter = geniterMomentum(RQ, RLambda, b, alpha, beta).iter
    for (var j = 0; j < numIters; j++) {
      var iterj = iter(j)
      Disps3[j].selectAll("rect").data(iterj[1]).merge(Disps3[j]).style("fill", function(d,i) { return rosen(Math.abs(d - xstar[1][i]) ) })
      Disps4[j].selectAll("rect").data(iterj[1]).merge(Disps4[j]).style("fill", function(d,i) { return jet(d) })

    }
  }

  var slidera = slider2D(d3.select("#sliderz"), function(x,y) { update(x/RLambda[RLambda.length - 1],y) }, 4.6, 996.35)

  update(0.0035,0.95)

  colorMap( d3.select("#rosen_colorbar1"),
            180,
            rosen,
            d3.scaleLinear().domain([0,1.5]).range([0, 180]) )

  colorMap( d3.select("#rosen_colorbar2"),
            180,
            jet,
            d3.scaleLinear().domain([0.2,1.5]).range([0, 180]) );


  callback(null);
  });

  </script>
  <p>
  The observations made in the above diagram are true for any Linear First Order algorithm. Let us prove this. First observe that each component of the gradient depends only on the values directly before and after it:

  $$
\nabla f(x)_{i}=\frac{\kappa-1}{4}(2w_{i}-w_{i-1}-w_{i+1})+w_{i}, \qquad i \neq 1.
  $$

  Therefore the fact we start at 0 guarantees that that component must remain stoically there till an element either before and after it turns nonzero. And therefore, by induction, for any linear first order algorithm,
  </p>
  <div style="position:relative; height:178px">
  $$
  \begin{array}{lllllllll}
    w^{0} & = & [~~0, & 0, & 0, & \ldots & 0, & 0, & \ldots & 0~]\\[0.35em]
    w^{1} & = & [~w_{1}^{1}, & 0, & 0, & \ldots & 0, & 0, & \ldots & 0~]\\[0.35em]
    w^{2} & = & [~w_{1}^{2}, & w_{2}^{2}, & 0, & \ldots & 0, & 0, & \ldots & 0~]\\[0.35em]
     & ~ \vdots \\
   w^{k} & = & [~w_{1}^{k}, & w_{2}^{k}, & w_{3}^{k}, & \ldots &  w_{k}^{k}, & 0, & \ldots & 0~]\\
  \end{array}
  $$
  </div>
  <p>
  Think of this restriction as a "speed of light" of information transfer. Error signals will take at least $k$ steps to move from $w_0$ can propagate to $w_k$. We can therefore sum up the errors which can not have changed yet

  $$
\begin{aligned}
\|w^{k}-w^{\star}\| & \geq\sum_{i=k+1}^{n}\!\!w_{i}^{\star}\\[1.7em]
 & =\sum_{i=k+1}^{n}\!\!\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^{i}\\[1.7em]
 & =\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^{k}\|w^{k}-w^{0}\|+\left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^{n}
\end{aligned}
  $$

  As $n$ gets large, the final term going to 0, and the condition number of $f^n$ approaches $\kappa$. And the gap therefore closes to yield the lower bound, which no linear first order algorithm, not ADAM or AdaGrad or Conjugate Gradients, can do better than.
  </p>
  <p>
  Like many such lower bounds, results like this must not be taken literally, but spiritually. Some of our favourite methods, including BFGS, and more, do not fall into the class of linear first order methods. But it is a surprising and satisfying coincidence that this lower bound is exactly that achieved by gradient descent with momentum.
  </p>

<hr>
  <h2>Onwards and Downwards</h2>
  <p>
  The study of acceleration is seeing a small revival within the optimization community. If the ideas in this article excite you, you may wish to read <dt-cite key="su2014differential"></dt-cite>, which fully explores the idea of momentum as the discretization of a certain differential equation. But other, less physical interpretations exist. There an algebraic interpretation of momentum in terms of approximating polynomials <dt-cite key="rutishauser1959theory,hardtzen"></dt-cite>. Geometric interpretations are emerging <dt-cite key="bubeck2015geometric,drusvyatskiy2016optimal"></dt-cite>, connecting momentum to older methods, like the Ellipsoid method. And finally, there are interpretations relating momentum to duality <dt-cite key="allen2014linear"></dt-cite>, perhaps providing a clue as how to accelerate second order methods and Quasi Newton (for a first step, see <dt-cite key="nesterov2008accelerating"></dt-cite>). But Like the proverbial blind men feeling elephant, momentum seems like something bigger than the sum of its parts. One day, hopefully soon, the many perspectives will unify into a satisfying whole.
  </p>
</dt-article>

<p>

  </p>


<dt-appendix class="centered"></dt-appendix>
<script type="text/bibliography">
@article{o2015adaptive,
  title={Adaptive restart for accelerated gradient schemes},
  author={O’Donoghue, Brendan and Candes, Emmanuel},
  journal={Foundations of computational mathematics},
  volume={15},
  number={3},
  pages={715--732},
  year={2015},
  publisher={Springer},
  url={https://arxiv.org/abs/1204.3982},
  doi={10.1007/s10208-013-9150-3}
}

@article{flammarion2015averaging,
  title={From averaging to acceleration, there is only a step-size},
  author={Flammarion, Nicolas and Bach, Francis},
  booktitle={Proceedings of the International Conference on Learning Theory (COLT)},
  year={2015},
  url={https://arxiv.org/abs/1504.01577}
}

@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015},
  url={https://arxiv.org/abs/1502.03167}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011},
  url={http://jmlr.org/papers/v12/duchi11a.html}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014},
  url={https://arxiv.org/abs/1412.6980}
}

@book{briggs2000multigrid,
  title={A multigrid tutorial},
  author={Briggs, William L and Henson, Van Emden and McCormick, Steve F},
  year={2000},
  publisher={SIAM},
  doi={10.1137/1.9780898719505}
}

@article{su2014differential,
  title={A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights},
  author={Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2510--2518},
  year={2014},
  url={https://arxiv.org/abs/1503.01243}
}

@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier},
  url={https://www.researchgate.net/profile/Boris_Polyak2/publication/243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods/links/5666fa3808ae34c89a01fda1.pdf},
  doi={10.1016/0041-5553(64)90137-5}
}

@article{flammarion2015averaging,
  title={From Averaging to Acceleration, There is Only a Step-size.},
  author={Flammarion, Nicolas and Bach, Francis R},
  booktitle={COLT},
  pages={658--695},
  year={2015},
  url={https://arxiv.org/abs/1504.01577}
}

@article{williamsnthpower,
  title={The Nth Power of a 2x2 Matrix.},
  author={Williams, Kenneth},
  journal={Mathematics Magazine},
  volume={65},
  number={5},
  pages={336},
  year={1992},
  publisher={MAA},
  url={http://people.math.carleton.ca/~williams/papers/pdf/175.pdf},
  doi={10.2307/2691246}
}

@article{hardtzen,
  title={The Zen of Gradient Descent},
  author={Hardt, Moritz},
  year={2013},
  url={http://blog.mrtz.org/2013/09/07/the-zen-of-gradient-descent.html}
}

@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media},
  doi={10.1007/978-1-4419-8853-9}
}

@article{rutishauser1959theory,
  title={Theory of gradient methods},
  author={Rutishauser, Heinz},
  booktitle={Refined iterative methods for computation of the solution and the eigenvalues of self-adjoint boundary value problems},
  pages={24--49},
  year={1959},
  publisher={Springer},
  doi={10.1007/978-3-0348-7224-9_2}
}

@article{bubeck2015geometric,
  title={A geometric alternative to Nesterov's accelerated gradient descent},
  author={Bubeck, S{\'e}bastien and Lee, Yin Tat and Singh, Mohit},
  journal={arXiv preprint arXiv:1506.08187},
  year={2015},
  url={https://arxiv.org/pdf/1506.08187.pdf}
}

@article{drusvyatskiy2016optimal,
  title={An optimal first order method based on optimal quadratic averaging},
  author={Drusvyatskiy, Dmitriy and Fazel, Maryam and Roy, Scott},
  journal={arXiv preprint arXiv:1604.06543},
  year={2016},
  url={https://arxiv.org/pdf/1604.06543.pdf}
}

@article{allen2014linear,
  title={Linear coupling: An ultimate unification of gradient and mirror descent},
  author={Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
  journal={arXiv preprint arXiv:1407.1537},
  year={2014},
  url={https://arxiv.org/pdf/1407.1537.pdf}
}

@article{nesterov2008accelerating,
  title={Accelerating the cubic regularization of Newton’s method on convex problems},
  author={Nesterov, Yu},
  journal={Mathematical Programming},
  volume={112},
  number={1},
  pages={159--181},
  year={2008},
  publisher={Springer},
  doi={10.1007/s10107-006-0089-x},
  url={http://folk.uib.no/ssu029/Pdf_file/Nesterov08.pdf}
}

@article{qian1999momentum,
  title={On the momentum term in gradient descent learning algorithms},
  author={Qian, Ning},
  journal={Neural networks},
  volume={12},
  number={1},
  pages={145--151},
  year={1999},
  publisher={Elsevier},
  doi={10.1016/s0893-6080(98)00116-6},
  url={https://pdfs.semanticscholar.org/735d/4220d5579cc6afe956d9f6ea501a96ae99e2.pdf}
}

@article{amari1998natural,
  title={Natural gradient works efficiently in learning},
  author={Amari, Shun-Ichi},
  journal={Neural computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998},
  publisher={MIT Press},
  doi={10.1162/089976698300017746},
  url={http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf}
}

@inproceedings{wiesler2011convergence,
  title={A convergence analysis of log-linear training},
  author={Wiesler, Simon and Ney, Hermann},
  booktitle={Advances in Neural Information Processing Systems},
  pages={657--665},
  year={2011},
  url={http://papers.nips.cc/paper/4421-a-convergence-analysis-of-log-linear-training.pdf}
}

@article{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning.},
  author={Sutskever, Ilya and Martens, James and Dahl, George E and Hinton, Geoffrey E},
  journal={ICML (3)},
  volume={28},
  pages={1139--1147},
  year={2013},
  url={http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf}
}


@article{hintonNIPS,
  title={Deep Learning, NIPS'2015 Tutorial},
  author={Hinton, Geoff and Bengio, Yoshua and LeCun, Yann},
  year={2015},
  url={http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015.pdf}
}

</script>

<!-- Katex -->
<script>
renderMath(document.body)
</script>

<!-- Figure render queue -->
<script>
  setTimeout(function() {
    var q = d3.queue(1);

    d3.zip(deleteQueue,renderQueue).forEach(function(fn) {
      q.defer(function(callback) { fn[1](callback); fn[0](callback) });
      q.defer(function(callback) {
        setTimeout(function() {
          callback(null);
        }, 50);
      });
    });
    q.await(function(error) {
      if (error) {
        console.error("Render error.")
      } else {
        console.log("Render done.")
      }
    });
  }, 50);
  
// DEBUG  
// renderQueue.forEach( function (fn) { fn( function() {}) } )


</script>
